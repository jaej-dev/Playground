{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 3. How to minimize cost\n",
    "## 1. Cost function graph (convex function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Model parameters\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model input and output\n",
    "x = [1, 2, 3]\n",
    "y = [1, 2, 3]\n",
    "\n",
    "# Our hypothesis\n",
    "hypothesis = x * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - y))  # sum of the squares\n",
    "\n",
    "# launch the graph in a session\n",
    "init = tf.global_variables_initializer()    # over rev 1.0 api\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "\n",
    "# Variables for flotting cost function\n",
    "W_val = []\n",
    "cost_val = []\n",
    "\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1 # swing -3 ~ 5\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "cost(W)=\\frac{1}{m}\\sum_{i=1}^{m}(H({x})^i - y^i)^2\n",
    "\\end{equation*}\n",
    "- 위 그래프는 x 축은 W, y 축은 cost 값이며, W가 1일때 cost가 0으로 최소화 되고 있다.\n",
    "- Machine learning은 위와 같은 convex function 그래프를 만들어야하고, 최소화를 시키는 W(weight), b(bias) 값을 찾는데 목적이 있다.\n",
    "\n",
    "## 2. Gradient Descent Algorithm (GDA)\n",
    "- Minimize cost function\n",
    "- Gradient descent is used many minimization problems\n",
    "- For a given cost function, cost(W, b), it will find W, b to minimize cost\n",
    "- It can be applied to more general function:\n",
    "    - cost (w1, w2, ...)\n",
    "- 어떤 위치에서 시작하더라도 최소 비용에 수렴한다.\n",
    "- 그러나 learning rate가 너무 클 경우 발산할수 있다.\n",
    "\n",
    "## 3. How it works\n",
    "- Start with initial guesses\n",
    "  - Start at 0,0 (or any other value)\n",
    "  - Keeping changing W and b a little bit to try and reduce cost(W, b)\n",
    "- Each time you change the parameters, you select the gradient which reduces cost(W, b) the most possible\n",
    "- Repeat\n",
    "- Do so until you converge to a local minimum\n",
    "- Has an interesting property\n",
    "    - Where you start can determine which minimum you end up\n",
    "- learning rate가 경사도를 구하는 두점의 거리이며, 경사도란 두점의 기울기이다. 즉 미분을 하면된다.\n",
    "\n",
    "\n",
    "## 4. Formal definition\n",
    "\\begin{equation*}\n",
    "cost(W)=\\frac{1}{m}\\sum_{i=1}^{m}(H({x})^i - y^i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "이와같은 cost function을 미분하면 아래와 같은 결과식을 얻게된다.\n",
    "\n",
    "\\begin{equation*}\n",
    "W := W - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(W({x})^i - y^i)x^i\n",
    "\\end{equation*}\n",
    "- 미분을 한다는것은 결국 W의 다음번 위치를 판단하기위한 위한 것이다. 현재의 위치에서 변화량을 계산하여 현재의 W에서 빼면 다음 W의 위치가 나오게 되는것이다.\n",
    "- 위 convex function 그래프에서 한점이 왼쪽 경사에 있을 때는 기울기(\"\\\")가 음수가 되어서 W의 값이 증가 하게 된다.\n",
    "\\begin{equation*}\n",
    "W := W - \\alpha -\\nabla = W + \\alpha \\nabla\n",
    "\\end{equation*}\n",
    "- 위 convex function 그래프에서 한점이 오른쪽 경사에 있을 때는 기울기(\"/\")가 양수가 되어서 W의 값이 감소하게 된다.\n",
    "\\begin{equation*}\n",
    "W := W - \\alpha x -\\nabla = W - \\alpha \\nabla\n",
    "\\end{equation*}\n",
    "- 결국 어느 위치에 있건 중앙에 있는 cost가 가장 적게 발생하는 최소 비용에 수렴하게 된다.\n",
    "- 알파는 learning rate를 나타내는 상수로서 경사의 두점의 간격을 정의하는 상수로서 개발자가 테스트를 통해 최적의 값을 찾아야 한다. 보통 0.1 ~ 0.001 사이의 값을 사용한다.\n",
    "- 이것을 Tensorflow에서 구현을 하면 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\\nlearning_rate = 0.1\\ngradient = tf.reduce_mean((W * X - Y) * X)\\ndescent = W - learning_rate * gradient\\nupdate = W.assign(descent)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convex function\n",
    "- 아래와 같이 cost function을 설계할 경우 시작점에 따라 W, b의 값이 달라 질수 있다.\n",
    "<img src=\"http://blog.datumbox.com/wp-content/uploads/2013/10/gradient-descent.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- cost function을 설계할때는 반드시 아래와 같이 convex function의 형태가 되도록 해야 Gradient Descent Algorithm을 정상적으로 적용할수 있다. 만약 그렇지 않다면 global optimum을 찾지 못하고 local optimum을 찾게되어 학습은 실패하게 된다.\n",
    "- convex function의 경우 어떤 점에서 시작을 하더라도 반드시 최소의 기울기를 찾을수 있다.\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-e682b0c1029917edb4dafa63fb6d6c2c\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "## 6. Lab1: Gradient Descent Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tcost: 16.763687 \tW: [-0.09426057]\n",
      "1 \tcost: 4.7683377 \tW: [0.41639435]\n",
      "2 \tcost: 1.356327 \tW: [0.68874365]\n",
      "3 \tcost: 0.38579977 \tW: [0.8339966]\n",
      "4 \tcost: 0.1097386 \tW: [0.91146487]\n",
      "5 \tcost: 0.031214515 \tW: [0.95278126]\n",
      "6 \tcost: 0.008878812 \tW: [0.9748167]\n",
      "7 \tcost: 0.002525512 \tW: [0.9865689]\n",
      "8 \tcost: 0.00071836286 \tW: [0.9928368]\n",
      "9 \tcost: 0.00020433844 \tW: [0.9961796]\n",
      "10 \tcost: 5.8125253e-05 \tW: [0.9979624]\n",
      "11 \tcost: 1.6533199e-05 \tW: [0.9989133]\n",
      "12 \tcost: 4.7030344e-06 \tW: [0.9994204]\n",
      "13 \tcost: 1.3379042e-06 \tW: [0.9996909]\n",
      "14 \tcost: 3.8041532e-07 \tW: [0.99983513]\n",
      "15 \tcost: 1.08179776e-07 \tW: [0.9999121]\n",
      "16 \tcost: 3.0789398e-08 \tW: [0.9999531]\n",
      "17 \tcost: 8.773782e-09 \tW: [0.99997497]\n",
      "18 \tcost: 2.4956535e-09 \tW: [0.99998665]\n",
      "19 \tcost: 7.018066e-10 \tW: [0.9999929]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab3-2 : Linear Regression Minizing cost\n",
    "#          Gradient Descent Algorithm\n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis\n",
    "linear_model = x * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_sum(tf.square(linear_model - y))  # sum of the squares\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * deriavtive\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * x - y) * x)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# launch the graph in a session\n",
    "init = tf.global_variables_initializer()    # over rev 1.0 api\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "\n",
    "for step in range(20):\n",
    "    sess.run(update, feed_dict={x: x_data, y: y_data})\n",
    "    print(step, \"\\tcost:\", sess.run(cost, feed_dict={x: x_data, y: y_data}), \"\\tW:\", sess.run(W))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 소스코드는 cost 함수가 간단하여 미분식(gradient)을 간단하게 직접 구현하였으나,\n",
    "cost 함수가 복잡하여질경우 구현이 어렵다. 그래서 아래와 같이 Tensorflow에서 구현한 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Minimize: Gradient Descent Magic\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\\ntrain = optimizer.minimize(cost)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lab2: Gradient Descent Algorithm Test1 (W = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tcost: 74.666664 \tW: 5.0\n",
      "1 \tcost: 0.3318512 \tW: 1.2666664\n",
      "2 \tcost: 0.0014748968 \tW: 1.0177778\n",
      "3 \tcost: 6.555027e-06 \tW: 1.0011852\n",
      "4 \tcost: 2.91322e-08 \tW: 1.000079\n",
      "5 \tcost: 1.2839034e-10 \tW: 1.0000052\n",
      "6 \tcost: 5.163277e-13 \tW: 1.0000004\n",
      "7 \tcost: 0.0 \tW: 1.0\n",
      "8 \tcost: 0.0 \tW: 1.0\n",
      "9 \tcost: 0.0 \tW: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(5.0)\n",
    "\n",
    "# Model input and output\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))  # sum of the squares\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in a session\n",
    "init = tf.global_variables_initializer()    # over rev 1.0 api\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "\n",
    "for step in range(10):\n",
    "    print(step, \"\\tcost:\", sess.run(cost, feed_dict={X: x_data, Y: y_data}), \"\\tW:\", sess.run(W))\n",
    "    sess.run(train, {X: x_data, Y: y_data})\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W가 최초 5.0이면 Covex curve 오른쪽 경사면 제일 위쪽에 위치 했는데, 학습을 거듭할수록 W가 1.0에 수렴하는 것을 알수 확인할수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lab3: Gradient Descent Algorithm Test2 (W = -3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tcost: 74.666664 \tW: -3.0\n",
      "1 \tcost: 0.3318512 \tW: 0.7333336\n",
      "2 \tcost: 0.0014748932 \tW: 0.98222226\n",
      "3 \tcost: 6.555027e-06 \tW: 0.9988148\n",
      "4 \tcost: 2.91322e-08 \tW: 0.99992096\n",
      "5 \tcost: 1.3195844e-10 \tW: 0.9999947\n",
      "6 \tcost: 5.163277e-13 \tW: 0.99999964\n",
      "7 \tcost: 2.4868996e-14 \tW: 0.99999994\n",
      "8 \tcost: 0.0 \tW: 1.0\n",
      "9 \tcost: 0.0 \tW: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(-3.0)\n",
    "\n",
    "# Model input and output\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))  # sum of the squares\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in a session\n",
    "init = tf.global_variables_initializer()    # over rev 1.0 api\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "\n",
    "for step in range(10):\n",
    "    print(step, \"\\tcost:\", sess.run(cost, feed_dict={X: x_data, Y: y_data}), \"\\tW:\", sess.run(W))\n",
    "    sess.run(train, {X: x_data, Y: y_data})\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W가 최초 -3.0이면 Covex curve 왼쪽 경사면 제일 위쪽에 위치 했는데, 학습을 거듭할수록 W가 1.0에 수렴하는 것을 알수 확인할수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compute gradient and Apply Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서 GradientDescentOptimizer()로 계산된 gradient값과 W(weight)값을 돌려준다\n",
    "  - gvs = optimizer.compute_gradients(cost) \n",
    "\n",
    "- 이곳에서 gradient값을 변경할수 있다. 현재는 그대로 사용\n",
    "  - gvs = gvs \n",
    "\n",
    "- 여기서 수정된 gradient 값을 적용 시켜준다\n",
    "  - apply_gradients = optimizer.apply_gradients(gvs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
