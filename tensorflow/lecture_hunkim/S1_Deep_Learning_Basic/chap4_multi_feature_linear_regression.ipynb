{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 4. Multi feature linear regression\n",
    "\n",
    "## 1. Hypothesis\n",
    "### 1.1. Theory\n",
    "\\begin{equation*}\n",
    "H({x})=W({x}) + b\n",
    "\\end{equation*}\n",
    " \n",
    "\\begin{equation*}\n",
    "H({x_1, x_2, x_3, ..., x_n})=w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b\n",
    "\\end{equation*}\n",
    "\n",
    "### 1.2. Implementation by Matrix multiplication\n",
    "\\begin{equation*}\n",
    "[x_1  x_2  x_3  x_n]\n",
    "\\begin{bmatrix}\n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "w_3\\\\\n",
    "w_n\\\\\n",
    "\\end{bmatrix} = x_1w_1 + x_2w_2 + x_3w_3 + x_nw_n\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "H(X) = XW\n",
    "\\end{equation*}\n",
    "- 입력 데이터가 많아질 경우 수식이 너무 길어지기 때문에 matrix의 곱셈 형태로 수식화가 가능하며, 머신 러닝에 사용하는 샘플 데이터를 모두 matrix에 집어 넣어 쉽게 연산해낼 수 있는 장점이 있다.\n",
    "\n",
    "### 1.3. 3 Feature(variables x1, x2, x3) * 5 instance(sample number) matrix\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "x_{31} & x_{32} & x_{33} \\\\\n",
    "x_{41} & x_{42} & x_{43} \\\\\n",
    "x_{51} & x_{52} & x_{53} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "w_3\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x_{11}w_1 + x_{12}w_2 + x_{13}w_3 \\\\\n",
    "x_{21}w_1 + x_{22}w_2 + x_{23}w_3 \\\\\n",
    "x_{31}w_1 + x_{32}w_2 + x_{33}w_3 \\\\\n",
    "x_{41}w_1 + x_{42}w_2 + x_{43}w_3 \\\\\n",
    "x_{51}w_1 + x_{52}w_2 + x_{53}w_3\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "[5, 3] [3, 1] = [5, 1]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "X * W = H(X) ==> [n, f] [f, y] = [n, y]\n",
    "\\end{equation*}\n",
    "\n",
    "- 일반적으로 X는 주어지기 때문에 이미 알고 있다. X는 f개의 feature(variable) 종류와 n개의 intance(샘플 데이터)를 의미 한다.\n",
    "- 또한 출력값 H(X)도 주어진다. linear regression의 경우 y가 1개이고, logistic regression의 경우 y가 여러개가 될수 있다.\n",
    "- 그러므로 W(weight)의 크기를 설계할때 X에서 f를 H(X)에서 y를 가져와 적용한다.\n",
    "\n",
    "\n",
    "## 2. Cost function\n",
    "\\begin{equation*}\n",
    "cost(W)=\\frac{1}{m}\\sum_{i=1}^{m}(H({x})^i - y^i)^2\n",
    "\\end{equation*}\n",
    " \n",
    "\\begin{equation*}\n",
    "cost(W)=\\frac{1}{m}\\sum_{i=1}^{m}(H({x_1}^i,{x_2}^i,{x_3}^i, {x_n}^i) - y^i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "- 예측하려는 값 H(x1, x2, x3, ..., xn)을 그대로 대입하면 된다.\n",
    "- 이것을 다시 정리 하면 아래와 같다.\n",
    "\n",
    "\\begin{equation*}\n",
    "cost(W)=\\frac{1}{m}\\sum_{i=1}^{m}(WX-y)^2\n",
    "\\end{equation*}\n",
    "\n",
    "## 3. Minimize Cost (Gradient descent algorithm)\n",
    "- Feature 수에 상관없이 동일하며, 아래식과 같이 Weight은 현재 Weight에서 cost function을 미분한값에 learning rate를 곱한값을 뺀것과 같다.\n",
    "\\begin{equation*}\n",
    "W := W - \\alpha \\frac{\\partial}{\\partial W}cost(W)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "## 4. Lab1: Matrix multiplication을 이용한 Multi feature linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost: 80516.38 \n",
      "Prediction:\n",
      " [[ -93.077995]\n",
      " [-122.902214]\n",
      " [-115.32393 ]\n",
      " [-126.26742 ]\n",
      " [ -95.99668 ]]\n",
      "10000 Cost: 0.40201458 \n",
      "Prediction:\n",
      " [[151.72481]\n",
      " [184.42906]\n",
      " [180.69597]\n",
      " [196.68288]\n",
      " [141.18903]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab4-1 : Multi Variable Linear Regression\n",
    "# \n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "x_data = [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]          \n",
    "\n",
    "# Placeholder for a tensor that will be always fed\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Our hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in a session\n",
    "init = tf.global_variables_initializer()    # over rev 1.0 api\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "\n",
    "# training\n",
    "for step in range(10001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "            feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 10000 == 0:\n",
    "       print(step, \"Cost:\", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10000회의 학습을 통해 Cost값이 80516에서 0에 가깝게 최소화 되어가는 것을 확인할 수 있다.\n",
    "- 최종적인 예측 값이, 최초 주어진 y_data와 비슷한것을 확인할 수 있다.\n",
    "\n",
    "## 5. Lab2: File input multi linear regression with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost: 32313.89 \n",
      "Prediction:\n",
      " [[-11.626006  ]\n",
      " [-22.355602  ]\n",
      " [-17.772346  ]\n",
      " [-17.851078  ]\n",
      " [-20.505215  ]\n",
      " [-14.032096  ]\n",
      " [ -8.969039  ]\n",
      " [ -0.19958979]\n",
      " [-21.132484  ]\n",
      " [-17.17147   ]\n",
      " [-12.689815  ]\n",
      " [-15.717196  ]\n",
      " [-19.987759  ]\n",
      " [-19.37931   ]\n",
      " [-10.117206  ]\n",
      " [-20.504623  ]\n",
      " [-22.891539  ]\n",
      " [ -7.7509565 ]\n",
      " [-18.005363  ]\n",
      " [-14.989589  ]\n",
      " [-12.150096  ]\n",
      " [-19.044708  ]\n",
      " [-11.306457  ]\n",
      " [-15.474425  ]\n",
      " [-22.502056  ]]\n",
      "10000 Cost: 6.0806427 \n",
      "Prediction:\n",
      " [[153.13841]\n",
      " [184.39146]\n",
      " [181.45366]\n",
      " [199.06929]\n",
      " [139.37843]\n",
      " [104.77288]\n",
      " [150.87257]\n",
      " [114.50329]\n",
      " [174.04066]\n",
      " [164.26408]\n",
      " [143.9617 ]\n",
      " [142.56738]\n",
      " [186.05742]\n",
      " [152.57445]\n",
      " [151.75441]\n",
      " [188.43651]\n",
      " [143.64365]\n",
      " [182.02278]\n",
      " [177.09578]\n",
      " [158.62404]\n",
      " [176.56282]\n",
      " [174.29689]\n",
      " [167.89342]\n",
      " [151.08755]\n",
      " [190.43071]]\n",
      "\n",
      "Your score will be  [[185.46397]]\n",
      "\n",
      "Other scores will be  [[132.06215]\n",
      " [175.28055]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab4-2 : Filie Input Linear Regression\n",
    "# \n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "#print(x_data.shape, x_data, len(x_data))\n",
    "#print(y_data.shape, y_data, len(y_data))\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Placeholder for a tensor that will be always fed\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# Our hypothesis\n",
    "linear_model = tf.matmul(x, W) + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(linear_model - y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in a session\n",
    "init = tf.global_variables_initializer()    # over rev 1.0 api\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "\n",
    "# training\n",
    "for step in range(10001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, linear_model, train],\n",
    "            feed_dict={x: x_data, y: y_data})\n",
    "    if step % 10000 == 0:\n",
    "       print(step, \"Cost:\", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "# Ask my score\n",
    "print(\"\\nYour score will be \", sess.run(linear_model,\n",
    "            feed_dict={x: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"\\nOther scores will be \", sess.run(linear_model,\n",
    "            feed_dict={x: [[60, 70, 65], [90, 100, 80]]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lab3: File input multi linear regression with tensorflow reader\n",
    "- numpy를 사용할 경우 파일 사이즈에 따라 메모리를 비효율적으로 사용하게 되어 메모리 에러가 발생할수 있다.\n",
    "- Tensorflow의 queue runners는 이러한 단점을 보완하기 위해 만들어진 library로서 여러개의 파일에서 데이터를 batch size만큼 읽어서 queue에 쌓았다가 꺼내면서 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  18588.691 \n",
      "Prediction:  [[18.754368]\n",
      " [24.329311]\n",
      " [22.763397]\n",
      " [26.297508]\n",
      " [18.118395]\n",
      " [16.608494]\n",
      " [21.977345]\n",
      " [19.277817]\n",
      " [25.36718 ]\n",
      " [27.474846]]\n",
      "10000 Cost:  4.766433 \n",
      "Prediction:  [[153.57343 ]\n",
      " [184.87018 ]\n",
      " [181.7545  ]\n",
      " [199.05394 ]\n",
      " [140.50648 ]\n",
      " [106.041916]\n",
      " [151.11375 ]\n",
      " [114.809906]\n",
      " [174.56572 ]\n",
      " [164.65022 ]]\n",
      "\n",
      "Your score will be  [[185.9972]]\n",
      "\n",
      "Other scores will be  [[132.17192]\n",
      " [175.46928]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab4-3 : Filie Input Linear Regression via tensorflow reader\n",
    "# \n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    ['data-01-test-score.csv'], shuffle=False, name='filename_queue')\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Default values, in case of empty columns.\n",
    "# Also specifies the type of the decoded result\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "# Collect batches of csv in\n",
    "train_x_batch, train_y_batch = \\\n",
    "    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "\n",
    "# Placeholders for a tensor that will be always fed.\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])    \n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1], name='weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
    "\n",
    "# Hypothesis\n",
    "linear_model = tf.matmul(x, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(linear_model - y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Start populating the filename queue\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# Machine learning\n",
    "for step in range(10001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "            [cost, linear_model, train], feed_dict={x: x_batch, y: y_batch})\n",
    "    if step % 10000 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction: \", hy_val)\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "# ask my score\n",
    "print(\"\\nYour score will be \", sess.run(linear_model,\n",
    "    feed_dict={x: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"\\nOther scores will be \", sess.run(linear_model,\n",
    "    feed_dict={x: [[60, 70, 65], [90, 100, 80]]}))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
