{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 9. Backpropagation\n",
    "\n",
    "## 1. Backpropagation Idea\n",
    "<img src=\"https://devblogs.nvidia.com/wp-content/uploads/2015/08/training_inference1.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- Chap 8. 에서 언급한 XOR 문제를 logistic regression 3개와 forward propagation, back propagation 알고리즘을 이용하여 해결할수 있다.\n",
    "\n",
    "- Forward propagation을 통과 시킨후 에러가 발생되면 backpropagation을 통하여 Weight과 Bias값 학습시키게 된다.\n",
    "\n",
    "- 결국 Tensorflow는 모든 Tensor(노드를 이루는 Operation, 선을 이루는 Data)를 graph로 다루게되며, Gradient Descent Optimizer API를 통해 backward 방향으로 미분을 구하여 Backpropagation을 실현하게 되며 Machine learning을 이루어지게 한다.\n",
    "\n",
    "\n",
    "## 2. Forward Propagation\n",
    "\n",
    "- Logistic regression의 Hypothesis는 아래와 같았다.\n",
    "  - logits = tf.matmul(X, W) + b\n",
    "  - hypothesis = tf.sigmoid(logits)\n",
    "\n",
    "- 하기 블록 다이어 그램에서 Neuron1, 2, 3은 각각의 logistic regression 이며 Hypothesis 설계에 필요한 값들은 아래와 같다.\n",
    "  - input1은 x1, input2는 x2\n",
    "  - W11, W12는 logistic regression1의 Weight\n",
    "  - W21, W22는 logistic regression2의 Weight\n",
    "  - W31, W32는 logistic regression3의 Weight\n",
    "  - Bias1, 2, 3은 각 logistic regession의 bias 이다.\n",
    "  - Neuron3의 입력은 기호가 표시되어 있지 않았으나 y1, y2라고 명한다.\n",
    "  - Output은 Y hat으로서 XOR 진리표의 값과 확인할 것이다.\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Medhat_Moussa/publication/228939274/figure/fig1/AS:393876184551431@1470918808455/Topology-of-ANN-used-to-solve-logic-XOR-problem.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- 진리표는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "| x1 | x2 | y1 | y2 | y hat | XOR | \n",
    "|----|----|----|----|-------|-----| \n",
    "| 0  | 0  | 0  | 1  |   0   | 0   | \n",
    "| 0  | 1  | 0  | 0  |   1   | 1   | \n",
    "| 1  | 0  | 0  | 0  |   1   | 1   | \n",
    "| 1  | 1  | 1  | 0  |   0   | 0   | \n",
    "| 1  | 1  | 1  | 0  |   0   | 0   |\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 진리표와 같은 결과가 나오는 과정은 아래와 같다.\n",
    " - x1과 x2는 진리표와 같이 각각 (0, 0), (0, 1), (1, 0), (1, 1)으로 주어 진다.\n",
    " - logistic regession1의 Weight은 (5, 5)이며  bias는 -8로 설정 하였다.\n",
    " - logistic regession2의 Weight은 (-7, -7) bias는 3으로 설정 하였다.\n",
    " - logistic regession3의 Weight은 (-11, -11) bias는 6으로 설정 하였다.\n",
    "\n",
    "- Logistic regression1의 계산 과정은 아래와 같다. W = (5, 5),   b = -8 이다.\n",
    "  - (0＊5 + 0＊5) + -8 =   0 - 8 = -8 이후 sigmoid(-8) = 0\n",
    "  - (0＊5 + 1＊5) + -8 =   5 - 8 = -3 이후 sigmoid(-3) = 0\n",
    "  - (1＊5 + 0＊5) + -8 =   5 - 8 = -3 이후 sigmoid(-3) = 0\n",
    "  - (1＊5 + 1＊5) + -8 = 10 - 8 =   2 이후 sigmoid(  2) = 1\n",
    "\n",
    "- Logistic regression1의 계산 과정은 아래와 같다. W = (-7, -7),   b = 3 이다.\n",
    "  - (0＊-7 + 0＊-7) + 3 =   0 + 3 =  3  이후 sigmoid(3) = 1\n",
    "  - (0＊-7 + 1＊-7) + 3 =  -7 + 3 = -4  이후 sigmoid(3) = 0\n",
    "  - (1＊-7 + 0*＊7) + 3 =  -7 + 3 = -4  이후 sigmoid(3) = 0\n",
    "  - (1＊-7 + 1＊-7) + 3 = -14 + 3 = -11 이후 sigmoid(3) = 0\n",
    "\n",
    "- Logistic regression3의 계산 과정은 아래와 같다. W = (-11, -11),   b = 6 이다.\n",
    "  - x1과 x2는 이전 결과의 조합이므로 (0, 1), (0, 0), (0, 0), (1, 0)으로 주어진다.\n",
    "  - (0＊-11 + 1＊-11) + 6 = -11 + 6 = -5 이후 sigmoid(-5) = 0\n",
    "  - (0＊-11 + 0＊-11) + 6 =   0 + 6 =  6 이후 sigmoid(  6) = 1\n",
    "  - (0＊-11 + 0＊-11) + 6 =   0 + 6 =  6 이후 sigmoid(  6) = 1\n",
    "  - (1＊-11 + 0＊-11) + 6 = -11 + 6 = -5 이후 sigmoid(-5) = 0\n",
    "  \n",
    "- 참고로 sigmoid function graph는 아래와 같다.\n",
    "\\begin{equation*}\n",
    "W := W - \\alpha \\frac{\\partial}{\\partial W}cost(W)\n",
    "\\end{equation*}\n",
    "<img src=\"https://ml4a.github.io/images/figures/sigmoid.png\" alt=\"\" title=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network\n",
    "- 위 블록다이어그램에서 Logistic regression 1과 Logistic regression 2는 아래 그림의 Hidden 영역과 같이 1개의 Multinomial logistic regression 으로 만들수 있다.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/7g8Bj.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "- Hidden 영역 Multinomial logistic regression의 Weight 값과 bias 값을 아래와 같이 2차원 벡터로 가지면 된다.\n",
    "\n",
    "\\begin{equation*}\n",
    "W_1 = \n",
    "\\begin{bmatrix}\n",
    "5, -7 \\\\\n",
    "5, -7 \\\\\n",
    "\\end{bmatrix}, b_1 =\n",
    "\\begin{bmatrix}\n",
    "8, 3 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "- Output 영역의 Logistic regression의 Weight 값과 bias 값은 아래와 같다.\n",
    "\\begin{equation*}\n",
    "W_2 =\n",
    "\\begin{bmatrix}\n",
    "-11, -11 \\\\\n",
    "\\end{bmatrix}, b_2 = 6\n",
    "\\end{equation*}\n",
    "\n",
    "- Input을 X라 하고, Multinomial logistic regression의 hypothesis k(x)를 설계한다면 아래와 같다.\n",
    "\\begin{equation*}\n",
    "K(x) = sigmoid(XW_1 + b_1)\n",
    "\\end{equation*}\n",
    "\n",
    "- 최종적으로 Output 영역의 hypothesis H(x)를 설계한다면 아래와 같다.\n",
    "\\begin{equation*}\n",
    "H(x) = sigmoid((K(x))W_2 + b_2)\n",
    "\\end{equation*}\n",
    "\n",
    "- 최종적으로 설계된 hypothesis를 Tensorflow 코드로 구현하면 아래와 같다.\n",
    "  - K = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "  - hypothesis = tf.sigmoid(tf.matmul(K, W2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backpropagation (How can we learn w1, w2, b1, b2 from traning data?)\n",
    "- Forwardpropagation으로 traning data를 입력시키고, Backpropagation (derivative, chain rule)을 사용하여 학습을 시킬수 있다.\n",
    "\n",
    "- 먼저 Forwardpropagation을 시킬때 아래와 같이 logits을 만들수 있다.\n",
    "  - f = wx + b\n",
    "  - wx 를 g라고 한다면 g = wx\n",
    "  - f = g + b가 된다.\n",
    "  - 위 식과 traning data를 입력하여 Tensor graph로 표현하면 아래와 같다.\n",
    "    - g = w ＊ x = -2 ＊ 10 = -10\n",
    "    - f = g + b = -7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "w = -2\n",
    "-------------\\\n",
    "∂f/∂w = 5     \\\n",
    "               \\  g = -10   \n",
    "x = 5          (*)------------\\\n",
    "---------------/ ∂f/∂g = 1     \\ \n",
    "∂f/∂x = -2                      \\\n",
    "                                 \\\n",
    "b = 3                           (+)------- f = -7\n",
    "--------------------------------/\n",
    "∂f/∂b = 1\n",
    "'''                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Backpropagation을 하는 목적은 w가 f에 미치는 영향, x가 f에 미치는 영향, b가 f에 미치는 영향을 알고 싶은 것이다. 이것을 알기 위해 역순으로 미분을 해나가는 것이다. 이 과정을 통하여 Weight과 Bias의 값을 조절하게 되며 결국 이것이 학습이 일어나는 것이다. neural network의 hidden layer가 아무리 많더라도 operation을 알기 때문에 역순으로 미분을 해나갈수 있다.\n",
    "\n",
    "\n",
    "- 위 그래프에서 미분을 간단히 하기위해 아래 편미분을 먼저 수행한다.\n",
    "\n",
    "\n",
    "- g = wx를 편미분하면 아래와 같다. (Chap 0의 2.2.1. 참조)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial g}{\\partial_ w} = x,\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial g}{\\partial_ x}=w \n",
    "\\end{equation*}\n",
    "\n",
    "- f = g + b를 편미분하면 아래와 같다. (Chap 0의 2.2.2. 참조)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial_ g} = 1,\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial_ b} = 1 \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "- Backpropagation을 위해 최종적으로 미분식을 정리하면 아래와 같다.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial_ w} = \\frac{\\partial f}{\\partial_ g} \\text{＊} \\frac{\\partial g}{\\partial_ w} = 1 \\text{＊} x = 1 \\text{＊} 5 = 5\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial_ x} = \\frac{\\partial f}{\\partial_ g} \\text{＊} \\frac{\\partial g}{\\partial_ x} = 1 \\text{＊} w = 1 \\text{＊} -2 = -2 \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial_ b} = 1\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial_ g}  = 1\n",
    "\\end{equation*}\n",
    "\n",
    "- 각 미분식의 의미는 아래와 같다.\n",
    "  - ∂f/∂w = 5 는 w의 값이 최종 출력에 5배로 영향을 준다는 의미이다.\n",
    "  - ∂f/∂x = -2 는 x의 값이 최종 출력에 -2배로 영향을 준다는 의미이다.\n",
    "  - ∂f/∂b = 1 은 bias의 값이 최종 출력 f에 1:1로 비례한다는 의미이다.\n",
    "\n",
    "\n",
    "- Sigmoid의 미분\n",
    "  - sigmoid를 그래프로 펼치면 아래와 같고 역순으로 미분을 해 나간다.  \n",
    "\n",
    "\\begin{equation*}\n",
    "g(z) = \\frac{1}{(1+e^{-z})}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                                 x\n",
    "z --- (* -1) --- (exp) --- (+1) --- (1/x) --- g\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural network은 위와 같은 그래프들이 길게 늘어선것 이다. 끝부터 처음까지 역순으로 미분을 해 나가면서 backpropagation 구하고 학습을 가능하게 하는 것이다.\n",
    "\n",
    "- Tensorflow가 모든 tensor를 graph로 다루는 이유도 미분을 하여 backpropagation을 구하기 위한것이다. 실제로 미분은 GradientDescentOptimizer 함수가 수행한다.\n",
    "\n",
    "- Tensorflow에서는 이런 graph를 다루는 것을 시각화 하기 위해 Tensor Borad라는 매우 유용한 툴을 제공 한다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*aOYUa3hHKi9HlYnnFAipUQ.gif\" alt=\"\" title=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lab1: Solving XOR problem with one logistic regression\n",
    "- Logistic regression을 사용하는 이유는 XOR의 output이 0, 1과 같이 binary 형태이기 때문이다.\n",
    "- 1개의 network(Logistic regression) 으로는 XOR 문제를 해결할 수 없음을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.87597954 [[0.7863567]\n",
      " [0.6628261]]\n",
      "5000 0.6931472 [[1.3183484e-07]\n",
      " [1.3247467e-07]]\n",
      "10000 0.6931472 [[1.3183484e-07]\n",
      " [1.3247467e-07]]\n",
      "\n",
      "Hypothesis:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab9-1 : xor by logistic regression\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "learning_rate = 0.1    \n",
    "\n",
    "# XOR True Table input\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "# XOR True Table output\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2]) # XOR input X1, X2\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) # XOR output Y\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1], name='weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
    "\n",
    "# Hyphthesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cost / loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "    tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                sess.run(W))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={\n",
    "        X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lab2: Solving XOR problem with neural network\n",
    "- 2개의 Network(Logistic regression)을 이용하여 XOR 문제를 해결하고 있음을 보여준다.\n",
    "\n",
    "- 위 설명에서는 XOR 문제를 해결하기 위해 3개의 Logistic regression을 이용한다고 설명하였다.  Neuron3의 입력을 Neuron1의 출력과 Neuron2의 출력에서 받았으나, 하기 Tensorflow 소스코드에서는 첫번째 Logistic regression의 W1, B1을 2개씩 출력하여 두번째 Logistic regssion의 입력으로 보내어 처리하였다. 즉 3개의 Logistic regression을 사용한것과 동일한 결과를 얻는다.\n",
    "\n",
    "- 이와 같이 Tensorflow에서는 매우 쉽게 Weight과 Bias의 출력을 wide하고, Network의 깊이를 deep하게 처리할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2737582 [array([[ 2.1355143 , -0.0782264 ],\n",
      "       [ 0.04496152, -0.48487958]], dtype=float32), array([[-0.6957488],\n",
      "       [-1.3244522]], dtype=float32)]\n",
      "5000 0.07993939 [array([[ 3.6978915, -5.4047523],\n",
      "       [ 3.6858444, -5.338445 ]], dtype=float32), array([[-6.841885 ],\n",
      "       [-7.2329164]], dtype=float32)]\n",
      "10000 0.016905494 [array([[ 4.8416247, -6.316066 ],\n",
      "       [ 4.8371215, -6.2909093]], dtype=float32), array([[-9.921189],\n",
      "       [-9.878948]], dtype=float32)]\n",
      "\n",
      "Hypothesis:\n",
      " [[0.01368036]\n",
      " [0.98169047]\n",
      " [0.9817343 ]\n",
      " [0.01679078]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab9-2 : xor by neural nerwork\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "learning_rate = 0.1    \n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "'''\n",
    "W = tf.Variable(tf.random_normal([2, 1], name='weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
    "\n",
    "# Hyphthesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "'''\n",
    "W1 = tf.Variable(tf.random_normal([2, 2], name='weight1'))\n",
    "b1 = tf.Variable(tf.random_normal([2], name='bias1'))\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1], name='weight2'))\n",
    "b2 = tf.Variable(tf.random_normal([1], name='bias2'))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "\n",
    "# Cost / loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "    tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                sess.run([W1, W2]))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={\n",
    "        X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lab3: Solving XOR problem with wide and deep neural network\n",
    "- Lab2에 비하여 학습이 잘되는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Lab2 Result\n",
    "Hypothesis:\n",
    " [[0.01368036]\n",
    " [0.98169047]\n",
    " [0.9817343 ]\n",
    " [0.01679078]]\n",
    " \n",
    "# Lab3 Result\n",
    "Hypothesis:\n",
    " [[9.6710393e-04]\n",
    " [9.9877375e-01]\n",
    " [9.9876839e-01]\n",
    " [1.8678077e-03]]  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5568961 [array([[-0.60164016,  0.8919202 ,  0.35191226,  0.56761146, -0.99739665,\n",
      "         0.32676136, -0.0275667 , -0.7428496 , -0.95138216, -2.952086  ],\n",
      "       [ 2.1419542 ,  1.1751897 ,  0.3820754 , -1.32208   , -0.01911528,\n",
      "         0.16187686, -3.0404184 ,  0.2068328 ,  0.3681465 , -1.5922024 ]],\n",
      "      dtype=float32), array([[-0.8270249 , -0.5908381 , -0.9152915 , -1.6387377 , -0.39875317,\n",
      "        -0.07010718, -0.62081033, -1.494441  ,  0.23517573, -0.9144238 ],\n",
      "       [-0.7995736 ,  1.6431977 ,  0.7156355 ,  1.8664869 , -0.38515177,\n",
      "         0.40816587, -0.6172208 , -0.09925716,  0.2734332 , -1.2857459 ],\n",
      "       [-0.08891804,  0.20978071, -0.9512092 ,  0.33139288, -2.0785854 ,\n",
      "        -0.48373953,  1.2817999 ,  0.48855504,  0.47712448,  0.98934746],\n",
      "       [ 1.07698   ,  1.2265451 , -0.7964994 , -0.08373559, -0.36422762,\n",
      "        -0.32580382, -0.06039998,  2.4420319 ,  0.18790258,  0.5140296 ],\n",
      "       [-1.050017  ,  1.1016577 , -0.998483  , -0.6537007 , -0.05316174,\n",
      "        -0.30850646, -0.4078329 ,  0.8703138 ,  0.5311842 , -1.5634769 ],\n",
      "       [-0.80293334,  1.2371863 , -0.94726264,  1.5831666 ,  0.50350946,\n",
      "         0.5476687 ,  0.02769241,  0.7434187 , -2.0529249 , -0.4246273 ],\n",
      "       [-0.8053039 ,  1.2467974 ,  1.5002865 ,  0.6664527 ,  1.641914  ,\n",
      "         0.11184189, -0.652233  , -0.38696525,  0.6784816 ,  0.97930974],\n",
      "       [ 0.4667148 ,  0.10491195, -0.3211533 , -0.09212124, -0.1874165 ,\n",
      "         1.5469565 ,  0.6847207 ,  0.6443898 , -0.4678866 ,  0.53416663],\n",
      "       [-0.7637375 , -0.5976923 ,  0.35266188, -0.60202813, -2.195065  ,\n",
      "        -2.1569178 , -0.44849858, -1.1209635 , -0.25319758, -0.6977408 ],\n",
      "       [-1.3866205 ,  0.03587005, -0.7622232 , -0.548433  , -0.98209435,\n",
      "        -0.7834198 , -1.1772585 ,  0.31850815,  0.255569  ,  1.4671814 ]],\n",
      "      dtype=float32), array([[ 0.5507815 ,  0.66937625, -0.5209393 ,  1.5659636 ,  1.0079393 ,\n",
      "        -0.22740944,  0.3899571 , -0.5259616 , -0.9373757 ,  0.0439489 ],\n",
      "       [ 1.4494996 , -0.21960275,  0.32625502,  0.8459835 ,  0.05941822,\n",
      "        -1.2975945 ,  0.8201999 ,  1.429978  , -1.0298345 ,  1.113551  ],\n",
      "       [ 0.15112938, -0.78403187,  0.32924318, -0.6608658 ,  0.38337976,\n",
      "         0.3600089 ,  1.5161812 , -0.7266012 ,  1.0384514 , -1.2873834 ],\n",
      "       [ 1.4316593 , -0.41271874, -0.17760175,  0.23817658,  0.35126725,\n",
      "        -0.65760183,  0.11382692, -2.0171087 , -2.4601886 ,  0.15219013],\n",
      "       [ 0.3046254 ,  0.3685863 , -0.15401658,  0.7556832 ,  1.0505038 ,\n",
      "         0.19995883,  0.4046586 ,  0.5405631 , -1.190176  ,  0.9428921 ],\n",
      "       [ 0.77658284,  0.21097937, -1.050298  ,  1.8273757 , -0.37859052,\n",
      "        -1.906339  ,  0.9381284 ,  0.3876163 ,  1.2714026 , -0.4508804 ],\n",
      "       [ 0.28120115,  0.7530719 ,  0.8204954 , -0.1418878 ,  0.06524419,\n",
      "        -1.6492594 ,  0.41782692, -0.35013673, -0.3989025 ,  0.40584624],\n",
      "       [ 0.2887937 , -0.58733535, -1.661883  , -0.34457022, -1.865016  ,\n",
      "        -1.4115736 , -0.0188571 , -0.8804624 , -0.6584951 , -0.5870544 ],\n",
      "       [-1.653982  , -0.32442123,  0.9352118 , -1.2446266 , -0.5998034 ,\n",
      "        -0.26928294,  0.14971878,  1.4802777 , -0.6574659 , -0.02064397],\n",
      "       [ 0.08672855,  0.27821863,  0.8569067 , -0.01011001,  0.08352266,\n",
      "        -0.00363863, -0.66424775, -0.38947973, -1.5466491 ,  0.9137281 ]],\n",
      "      dtype=float32), array([[ 1.957308  ],\n",
      "       [-1.6550303 ],\n",
      "       [-0.14441182],\n",
      "       [ 1.1807388 ],\n",
      "       [-0.19372019],\n",
      "       [ 0.8096321 ],\n",
      "       [-0.5193319 ],\n",
      "       [-0.50393873],\n",
      "       [ 0.2193661 ],\n",
      "       [ 1.4016349 ]], dtype=float32)]\n",
      "5000 0.004114406 [array([[-2.1392157 ,  0.38450712,  1.1914288 ,  2.9709527 , -1.4390224 ,\n",
      "         0.8570586 , -0.42365938, -2.1464033 , -0.8707169 , -4.54306   ],\n",
      "       [ 4.1819234 ,  2.1758423 ,  0.84332025, -2.213423  ,  0.0253891 ,\n",
      "         1.1469904 , -3.0294483 ,  0.26470324,  0.12419076, -4.342548  ]],\n",
      "      dtype=float32), array([[-2.4343774e+00, -6.3951319e-01, -8.9406461e-01, -2.0375762e+00,\n",
      "        -8.5875070e-01, -1.0868026e+00, -9.3247396e-01, -1.6885284e+00,\n",
      "         2.4193048e+00, -2.5677630e-01],\n",
      "       [-1.2430965e+00,  1.6603539e+00,  7.1006304e-01,  1.8928311e+00,\n",
      "        -3.7771195e-01, -6.9307834e-03, -7.9348326e-01, -2.2083887e-01,\n",
      "         1.0026004e+00, -8.3304787e-01],\n",
      "       [-4.7143083e-02,  1.5766487e-01, -9.5722961e-01, -1.0536930e-01,\n",
      "        -2.0493898e+00, -9.5168698e-01,  1.1780791e+00,  4.9121672e-01,\n",
      "         1.2952938e+00,  1.4777504e+00],\n",
      "       [ 1.9634472e+00,  1.0441352e+00, -8.0832446e-01, -1.4969331e+00,\n",
      "        -3.0744448e-01, -1.0757439e+00, -9.8461352e-02,  2.6446304e+00,\n",
      "         1.6287415e+00,  1.2297853e+00],\n",
      "       [-6.7162746e-01,  1.0773308e+00, -9.7354418e-01, -6.0810411e-01,\n",
      "         4.1969687e-01,  1.2633553e-01, -1.8644607e-01,  6.0490412e-01,\n",
      "        -9.5480138e-01, -1.9847611e+00],\n",
      "       [-4.4519168e-01,  1.1971124e+00, -9.3975943e-01,  1.3677338e+00,\n",
      "         8.6716121e-01,  5.0690305e-01,  8.1999406e-02,  5.9171617e-01,\n",
      "        -2.2097175e+00, -2.2366141e-01],\n",
      "       [-5.9341073e-01,  1.2198884e+00,  1.5002466e+00,  5.2037370e-01,\n",
      "         1.7120899e+00,  1.4566223e-01, -6.0548109e-01, -3.8375884e-01,\n",
      "         5.4701626e-01,  9.3769366e-01],\n",
      "       [ 6.6950142e-01,  1.1450268e-01, -2.9745167e-01,  2.0425227e-01,\n",
      "         2.3570609e-01,  2.1148205e+00,  9.0926403e-01,  3.5495442e-01,\n",
      "        -2.0432551e+00, -2.8360283e-03],\n",
      "       [-6.2882292e-01, -5.8288062e-01,  3.6436063e-01, -4.7978416e-01,\n",
      "        -2.0014193e+00, -1.9122226e+00, -3.6619624e-01, -1.2288232e+00,\n",
      "        -1.0812132e+00, -9.2777336e-01],\n",
      "       [-1.9206141e+00, -2.4837641e-01, -7.3414004e-01, -2.2206249e+00,\n",
      "        -1.4617896e+00, -1.8280839e+00, -1.1881973e+00,  3.8686895e-01,\n",
      "         2.8473716e+00,  1.9248203e+00]], dtype=float32), array([[ 1.64678371e+00,  2.43532449e-01, -9.26392198e-01,\n",
      "         2.45989251e+00,  1.17903411e+00, -2.14974195e-01,\n",
      "         3.66554797e-01, -1.36036325e+00, -9.37133312e-01,\n",
      "         6.67372882e-01],\n",
      "       [ 1.01023805e+00,  7.33008459e-02,  5.49860179e-01,\n",
      "         4.67033088e-01,  3.76792229e-03, -1.42947805e+00,\n",
      "         9.52704966e-01,  1.36554992e+00, -1.02550614e+00,\n",
      "         7.89081395e-01],\n",
      "       [ 3.31975937e-01, -8.84491265e-01,  2.67082572e-01,\n",
      "        -5.09051144e-01,  4.01606739e-01,  3.63104135e-01,\n",
      "         1.51322675e+00, -8.87838185e-01,  1.03880441e+00,\n",
      "        -1.16957295e+00],\n",
      "       [ 1.95029449e+00, -5.07406712e-01, -3.81390899e-01,\n",
      "         7.17479348e-01,  4.81686562e-01, -7.00340629e-01,\n",
      "         1.79986462e-01, -2.63999653e+00, -2.45430303e+00,\n",
      "         3.76136571e-01],\n",
      "       [ 1.07037318e+00,  3.09946872e-02, -4.28267628e-01,\n",
      "         1.37808537e+00,  1.17335880e+00,  2.00093493e-01,\n",
      "         4.01885837e-01, -1.22357704e-01, -1.18901563e+00,\n",
      "         1.35025477e+00],\n",
      "       [ 1.71452117e+00, -9.50957537e-02, -1.38053715e+00,\n",
      "         2.58947444e+00, -1.83666572e-01, -1.90352881e+00,\n",
      "         9.63255405e-01, -4.52270389e-01,  1.27653539e+00,\n",
      "        -4.55082804e-02],\n",
      "       [ 8.38617682e-01,  5.14235198e-01,  6.41742170e-01,\n",
      "         3.14204693e-01,  1.25855342e-01, -1.66248131e+00,\n",
      "         4.34367508e-01, -8.87424469e-01, -3.96851301e-01,\n",
      "         7.23179340e-01],\n",
      "       [-6.33085847e-01,  6.36616256e-03, -1.24750519e+00,\n",
      "        -1.15514052e+00, -2.00179243e+00, -1.60204732e+00,\n",
      "         1.45575568e-01, -6.27354026e-01, -6.56669557e-01,\n",
      "        -1.16337287e+00],\n",
      "       [-3.83152246e+00,  4.94560510e-01,  1.82011724e+00,\n",
      "        -3.04338646e+00, -1.03594255e+00, -4.42759603e-01,\n",
      "         2.96521664e-01,  2.73262358e+00, -6.57858431e-01,\n",
      "        -1.11462963e+00],\n",
      "       [-1.31386316e+00,  1.14160144e+00,  1.39071202e+00,\n",
      "        -1.18166721e+00, -7.80519098e-02, -2.39107043e-01,\n",
      "        -4.95996654e-01,  1.81349307e-01, -1.54968238e+00,\n",
      "         1.23308033e-01]], dtype=float32), array([[ 5.280865  ],\n",
      "       [-2.3184    ],\n",
      "       [-2.1890287 ],\n",
      "       [ 4.6485844 ],\n",
      "       [ 1.3365582 ],\n",
      "       [ 0.5758649 ],\n",
      "       [-1.2206599 ],\n",
      "       [-3.5473595 ],\n",
      "       [ 0.24482065],\n",
      "       [ 1.8230222 ]], dtype=float32)]\n",
      "10000 0.0013241249 [array([[-2.31294   ,  0.30232838,  1.2312173 ,  3.1522849 , -1.4695035 ,\n",
      "         0.92723954, -0.42654952, -2.214852  , -0.86673933, -4.641682  ],\n",
      "       [ 4.3514876 ,  2.1871998 ,  0.89914054, -2.3151255 ,  0.02320875,\n",
      "         1.1856971 , -3.0294483 ,  0.2937502 ,  0.13392247, -4.445317  ]],\n",
      "      dtype=float32), array([[-2.5703168 , -0.6502774 , -0.8892237 , -2.153389  , -0.9173725 ,\n",
      "        -1.1796491 , -0.9604997 , -1.698882  ,  2.5918343 , -0.23196773],\n",
      "       [-1.3013102 ,  1.654021  ,  0.7135547 ,  1.8248223 , -0.38384774,\n",
      "        -0.02931208, -0.8052722 , -0.22920755,  1.0321891 , -0.8271695 ],\n",
      "       [-0.05893629,  0.15631711, -0.95594615, -0.17955336, -2.0650249 ,\n",
      "        -0.97346777,  1.1647383 ,  0.5010485 ,  1.3280132 ,  1.5053529 ],\n",
      "       [ 2.0031967 ,  1.04638   , -0.8071401 , -1.605697  , -0.35375902,\n",
      "        -1.117481  , -0.11575719,  2.681567  ,  1.7203141 ,  1.2978511 ],\n",
      "       [-0.66809964,  1.0710084 , -0.968103  , -0.5496721 ,  0.45938322,\n",
      "         0.16957061, -0.16356644,  0.5819332 , -1.0184064 , -2.0289168 ],\n",
      "       [-0.4161691 ,  1.1916472 , -0.9335296 ,  1.3650923 ,  0.9066693 ,\n",
      "         0.5522604 ,  0.09718937,  0.58334416, -2.2861862 , -0.24143362],\n",
      "       [-0.5852791 ,  1.2198884 ,  1.5003692 ,  0.52593833,  1.7134569 ,\n",
      "         0.14917773, -0.60327387, -0.38289487,  0.544076  ,  0.9371026 ],\n",
      "       [ 0.6606849 ,  0.10736964, -0.2920246 ,  0.28054687,  0.27974516,\n",
      "         2.1605003 ,  0.9346046 ,  0.32502058, -2.111828  , -0.05835981],\n",
      "       [-0.6287907 , -0.58521247,  0.36630255, -0.45933354, -1.9866629 ,\n",
      "        -1.8965431 , -0.3581799 , -1.2374814 , -1.1051953 , -0.9440362 ],\n",
      "       [-1.961071  , -0.25012115, -0.73317736, -2.259507  , -1.5312322 ,\n",
      "        -1.9060488 , -1.2012705 ,  0.3987441 ,  3.02509   ,  1.9595313 ]],\n",
      "      dtype=float32), array([[ 1.7365583e+00,  2.0693624e-01, -9.9167335e-01,  2.5355048e+00,\n",
      "         1.2176733e+00, -2.1446660e-01,  3.6541140e-01, -1.4262315e+00,\n",
      "        -9.3713331e-01,  7.2031909e-01],\n",
      "       [ 1.0165406e+00,  3.5826158e-02,  5.5222851e-01,  4.6154249e-01,\n",
      "        -8.5405167e-03, -1.4364082e+00,  9.6001589e-01,  1.3378506e+00,\n",
      "        -1.0255061e+00,  7.8137779e-01],\n",
      "       [ 3.4182253e-01, -8.9168775e-01,  2.5951666e-01, -5.0045025e-01,\n",
      "         4.0576199e-01,  3.6310413e-01,  1.5132267e+00, -8.9866269e-01,\n",
      "         1.0388044e+00, -1.1633581e+00],\n",
      "       [ 2.0268388e+00, -5.7958913e-01, -4.5206073e-01,  7.9518914e-01,\n",
      "         5.1795918e-01, -6.9936150e-01,  1.8100685e-01, -2.7361081e+00,\n",
      "        -2.4541740e+00,  4.3210331e-01],\n",
      "       [ 1.1472121e+00, -2.2599459e-02, -4.8252538e-01,  1.4407963e+00,\n",
      "         1.2041495e+00,  1.9975074e-01,  4.0273470e-01, -2.0763116e-01,\n",
      "        -1.1890156e+00,  1.3961473e+00],\n",
      "       [ 1.8091865e+00, -1.7197137e-01, -1.4538696e+00,  2.6738491e+00,\n",
      "        -1.4051051e-01, -1.9017208e+00,  9.6324444e-01, -5.7482809e-01,\n",
      "         1.2769015e+00,  1.8671779e-02],\n",
      "       [ 8.8373786e-01,  4.7944757e-01,  6.0781765e-01,  3.5222411e-01,\n",
      "         1.4266020e-01, -1.6635588e+00,  4.3578804e-01, -9.3507349e-01,\n",
      "        -3.9671624e-01,  7.4950314e-01],\n",
      "       [-6.5669978e-01,  3.4767492e-03, -1.2123226e+00, -1.1974790e+00,\n",
      "        -2.0346174e+00, -1.6123091e+00,  1.5441529e-01, -6.0110813e-01,\n",
      "        -6.5680766e-01, -1.2009714e+00],\n",
      "       [-3.9834335e+00,  5.6038219e-01,  1.9476800e+00, -3.1939726e+00,\n",
      "        -1.1262099e+00, -4.5529118e-01,  3.0733335e-01,  2.8842297e+00,\n",
      "        -6.5818363e-01, -1.2323973e+00],\n",
      "       [-1.3618821e+00,  1.1609851e+00,  1.4463506e+00, -1.2472674e+00,\n",
      "        -1.2272167e-01, -2.5007838e-01, -4.8709944e-01,  2.4178804e-01,\n",
      "        -1.5498843e+00,  6.7897476e-02]], dtype=float32), array([[ 5.6691165 ],\n",
      "       [-2.4285376 ],\n",
      "       [-2.4614656 ],\n",
      "       [ 5.0241213 ],\n",
      "       [ 1.4955359 ],\n",
      "       [ 0.5599368 ],\n",
      "       [-1.2866285 ],\n",
      "       [-3.9023151 ],\n",
      "       [ 0.24544178],\n",
      "       [ 1.9644575 ]], dtype=float32)]\n",
      "\n",
      "Hypothesis:\n",
      " [[9.6710393e-04]\n",
      " [9.9877375e-01]\n",
      " [9.9876839e-01]\n",
      " [1.8678077e-03]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab9-3 : xor by neural nerwork wide deep\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "learning_rate = 0.1    \n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "'''\n",
    "W = tf.Variable(tf.random_normal([2, 1], name='weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
    "\n",
    "# Hyphthesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "'''\n",
    "'''\n",
    "W1 = tf.Variable(tf.random_normal([2, 2], name='weight1'))\n",
    "b1 = tf.Variable(tf.random_normal([2], name='bias1'))\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1], name='weight2'))\n",
    "b2 = tf.Variable(tf.random_normal([1], name='bias2'))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "'''\n",
    "W1 = tf.Variable(tf.random_normal([2, 10], name='weight1'))\n",
    "b1 = tf.Variable(tf.random_normal([10], name='bias1'))\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10], name='weight2'))\n",
    "b2 = tf.Variable(tf.random_normal([10], name='bias2'))\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10], name='weight1'))\n",
    "b3 = tf.Variable(tf.random_normal([10], name='bias1'))\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1], name='weight2'))\n",
    "b4 = tf.Variable(tf.random_normal([1], name='bias2'))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# Cost / loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "    tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                sess.run([W1, W2, W3, W4]))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={\n",
    "        X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lab4: Solving XOR problem with backpropagation\n",
    "- 이 예제는 Lab2: Solving XOR problem with neural network 예제와 동일한 결과를 갖지만  미분을 통해 backpropagation을 직접 구현하였다.\n",
    "\n",
    "- Lab2는 train = tf.train.GradientDescentOptimizer() 함수를 이용하여 backpropagation을 간단히 구현한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape 4\n",
      "0 [1.047763, array([[0.1790797 , 0.03775062],\n",
      "       [0.15853871, 0.03622694]], dtype=float32)] [array([[-0.22568084, -0.53591216],\n",
      "       [ 0.16982368, -0.41798908]], dtype=float32), array([[-1.0534035],\n",
      "       [-0.2579086]], dtype=float32)]\n",
      "5000 [0.5206581, array([[0.03393272, 0.05396489],\n",
      "       [0.05865538, 0.03833643]], dtype=float32)] [array([[-2.0188885 , -3.525843  ],\n",
      "       [-0.02829779, -2.9756508 ]], dtype=float32), array([[-2.5196912],\n",
      "       [ 3.7735164]], dtype=float32)]\n",
      "10000 [0.024093203, array([[0.00685279, 0.00462249],\n",
      "       [0.00716581, 0.00493447]], dtype=float32)] [array([[-5.5450306, -5.335227 ],\n",
      "       [-5.4966826, -5.3049097]], dtype=float32), array([[-9.491918],\n",
      "       [ 8.734669]], dtype=float32)]\n",
      "\n",
      "Hypothesis:\n",
      " [[0.0209769 ]\n",
      " [0.97834617]\n",
      " [0.97833496]\n",
      " [0.03089069]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab9-8 : xor neural network back propagation\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "l1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "Y_pred = tf.sigmoid(tf.matmul(l1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "# ref: see lecture 5-2 cost function of logistic regression    \n",
    "cost = -tf.reduce_mean(Y * tf.log(Y_pred) + (1 - Y) *\n",
    "                       tf.log(1 - Y_pred))\n",
    "\n",
    "# Network\n",
    "#          p1     a1           l1     p2     a2           l2 (y_pred)\n",
    "# X -> (*) -> (+) -> (sigmoid) -> (*) -> (+) -> (sigmoid) -> (loss)\n",
    "#       ^      ^                   ^      ^\n",
    "#       |      |                   |      |\n",
    "#       W1     b1                  W2     b2\n",
    "\n",
    "# Loss derivative\n",
    "d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
    "\n",
    "# Layer 2\n",
    "d_sigma2 = Y_pred * (1 - Y_pred)\n",
    "d_a2 = d_Y_pred * d_sigma2\n",
    "d_p2 = d_a2\n",
    "d_b2 = d_a2\n",
    "d_W2 = tf.matmul(tf.transpose(l1), d_p2)\n",
    "\n",
    "# Mean\n",
    "d_b2_mean = tf.reduce_mean(d_b2, axis=[0])\n",
    "d_W2_mean = d_W2 / tf.cast(tf.shape(l1)[0], dtype=tf.float32)\n",
    "\n",
    "# Layer 1\n",
    "d_l1 = tf.matmul(d_p2, tf.transpose(W2))\n",
    "d_sigma1 = l1 * (1 - l1)\n",
    "d_a1 = d_l1 * d_sigma1\n",
    "d_b1 = d_a1\n",
    "d_p1 = d_a1\n",
    "d_W1 = tf.matmul(tf.transpose(X), d_a1)\n",
    "\n",
    "# Mean\n",
    "d_W1_mean = d_W1 / tf.cast(tf.shape(X)[0], dtype=tf.float32)\n",
    "d_b1_mean = tf.reduce_mean(d_b1, axis=[0])\n",
    "\n",
    "# Weight update\n",
    "step = [\n",
    "  tf.assign(W2, W2 - learning_rate * d_W2_mean),\n",
    "  tf.assign(b2, b2 - learning_rate * d_b2_mean),\n",
    "  tf.assign(W1, W1 - learning_rate * d_W1_mean),\n",
    "  tf.assign(b1, b1 - learning_rate * d_b1_mean)\n",
    "]\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(Y_pred > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"shape\", sess.run(tf.shape(X)[0], feed_dict={X: x_data}))\n",
    "\n",
    "\n",
    "    for i in range(10001):\n",
    "        sess.run([step, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if i % 5000 == 0:\n",
    "            print(i, sess.run([cost, d_W1], feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([Y_pred, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Lab5: Tensor Board with solving XOR problem example\n",
    "- step 1: From TF graph, decide which tensors you want to log\n",
    "  - 로깅할 값이 한개면 scalar, 여러개면 histogram을 사용한다.\n",
    "  - w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "  - cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "\n",
    "- step 2: Merge all summaries\n",
    "  - merged_summary = tf.summary.merge_all()\n",
    "  \n",
    "  \n",
    "- step 3: Create writer and add graph\n",
    "  - writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "  - writer.add_graph(sess.graph) # show the graph\n",
    "\n",
    "\n",
    "- step 4: Run summary merge and add_summary\n",
    "  - summary, _ = sess.run([merged_summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "  - writer.add_summary(summary, global_step=step)\n",
    "\n",
    "\n",
    "- step 5: Launch the Tensorboard\n",
    "  - $ tensorboard --logdir=./logs/xor_logs_r0_01\n",
    "\n",
    "\n",
    "- step 6: Launch browser with localhost\n",
    "  - http://127.0.0.1:6006\n",
    "  - ssh의 포트 포워딩을 이용하여 리모트 서버에 접속하여 볼수도 있다.\n",
    "    - $ ssh -L local_port:127.0.0.1:remote_port username@server.com\n",
    "      - local> ssh -L 7007:127.0.0.1:6006 xxx@server.com\n",
    "      - server> tensorborad --logdir=./logs/xor_logs_r0_01\n",
    "\n",
    "\n",
    "- Tensor Board에서 그래프를 볼때 계층을 구분해서 보고 싶을 때는 name_scope을 사용한다.\n",
    "    - with tf.name_scope(\"layer1\"):\n",
    "\n",
    "\n",
    "- step3에서 parent 로그 디렉토리(\"logs/\") 하위에 여러개의 child 디렉토리를 만들어 그래프를 저장하면 tensor board에서 그래프를 비교 분석할수 있다. 단, tensorboard 실행시 parent 로그 디렉토리를 지정해 줘야 한다.\n",
    "  - writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "  - writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_02\")\n",
    "  - $ tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.71064335 [array([[ 0.70269835,  0.77858704],\n",
      "       [-1.117285  , -0.20518988]], dtype=float32), array([[1.8076982 ],\n",
      "       [0.44572306]], dtype=float32)]\n",
      "5000 2.5004463e-05 [array([[ -9.919293,  10.363377],\n",
      "       [  9.945174, -10.647296]], dtype=float32), array([[21.51126],\n",
      "       [21.62779]], dtype=float32)]\n",
      "10000 1.7881409e-06 [array([[-10.754227 ,  11.173415 ],\n",
      "       [ 10.777167 , -11.4593525]], dtype=float32), array([[26.690191],\n",
      "       [26.812712]], dtype=float32)]\n",
      "\n",
      "Hypothesis:\n",
      " [[1.9362387e-06]\n",
      " [9.9999821e-01]\n",
      " [9.9999845e-01]\n",
      " [1.9069422e-06]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab9-4 : Tensorboard with xor\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "learning_rate = 0.1    \n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2], name='x-input')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1], name='y-input')\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2], name='weight1'))\n",
    "    b1 = tf.Variable(tf.random_normal([2], name='bias1'))\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"baises1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "with tf.name_scope(\"layer2\"):    \n",
    "    W2 = tf.Variable(tf.random_normal([2, 1], name='weight2'))\n",
    "    b2 = tf.Variable(tf.random_normal([1], name='bias2'))\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"baises2\", b2)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "# Cost / loss function\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "        tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph) # show the graph\n",
    "\n",
    "    # Initialize tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict={\n",
    "            X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 5000 == 0:\n",
    "             print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                sess.run([W1, W2]))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={\n",
    "        X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
