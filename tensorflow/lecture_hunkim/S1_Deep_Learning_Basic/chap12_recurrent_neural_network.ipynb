{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 12 : Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Data\n",
    "- We don't understand one word only\n",
    "- We understand based on the previous words + this word. (time series)\n",
    "- NN/CNN cannot do this\n",
    "- RNN은 시간 개념이 포함되었다. 자연어 처리와 같이 이전 단어가 현재 단어에 영향을 미치기 때문이다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*DItCSHJ-NA2wy4lFfXv-_Q.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "## 2. Vanilla RNN\n",
    "\n",
    "- We can process a sequence of vectors x by applying a recurrent formula at every time step:\n",
    "\n",
    "<img src=\"http://www.yuthon.com/images/RNN_concept.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- Notice: the same function and the same set of parameters are used at every time step.\n",
    "\n",
    "- The state consists of a single \"hidden\" vector h:\n",
    "\n",
    "\\begin{equation*}\n",
    "h_t = f_W(h_{t-1}, x_t)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "h_t = tanh((W_{hh} \\text{＊} h_{t-1}) + (W_{xh} \\text{＊} x_t))\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "y_t = W_{hy}h_t\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- hyperbolic tangent curve\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*f9erByySVjTjohfFdNkJYQ.jpeg\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "## 3. Character level language model example\n",
    "\n",
    "<img src=\"https://byungjun0689.github.io/src/0609/RNN/8.PNG\" alt=\"\" title=\"\" />\n",
    "\n",
    "- vocabulary를 one hot encoring 형태의 벡터로 표현하였다. (h, e, l, o)\n",
    "\n",
    "- 초기 ht-1이 없는 경우 일반적으로 0을 사용한다.\n",
    "\n",
    "- xt는 현재 상태의 입력에 대한 one hot encoding 값이다.\n",
    "\n",
    "- Whh는 이전 hidden layer에서 학습한 weight이다.\n",
    "\n",
    "- Wxh는 현재 상태의 학습시 결정된 weight 값이다. \n",
    "\n",
    "- tanh 함수를 거치면 아래와 같이 -1.0 ~ +1.0사이의 결과를 얻게된다.\n",
    "\n",
    "- 같은 과정을 반복하며 이전의 값들에 영향을 받으며 학습이 진행된다.\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" alt=\"\" title=\"\" />\n",
    "\n",
    "- ht는 위에서 얻은 hidden layer의 state값이다.\n",
    "\n",
    "- Why는 현재 hidden layer값에 따라 학습한 weight값이다.\n",
    "\n",
    "- 최종적인 yt는 ht와 why의 곱이다.\n",
    "\n",
    "- 학습을 위한 cost함수로 softmax를 사용한다.\n",
    "\n",
    "\n",
    "## 4. RNN applications\n",
    "\n",
    "<img src=\"http://i-systems.github.io/HSE545/machine%20learning%20all/16%20Deep%20learning/image_files/rnn_application.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "- Language Modeling\n",
    "\n",
    "- Speech Recognition \n",
    "\n",
    "- Machine Translation ( many to many)\n",
    "\n",
    "- Conversation Modeling / Question Answering\n",
    "\n",
    "- Image (one to many) / Video Captioning (many to many)\n",
    "\n",
    "- Image / Music / Dance Generation\n",
    "\n",
    "\n",
    "## 5. Traning RNNs is challenging\n",
    "\n",
    "- Several advanced models\n",
    "  - Long Short Term Memory (LSTM)\n",
    "  - GRU by Cho et al. 2014\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lab1: RNN basics\n",
    "\n",
    "### 6.1. Tensorflow에서 RNN 구현 방법\n",
    "\n",
    "  - Step 1: cell을 만든다. (BasicLSTMCell, BasicGRUCell, BasicRNNCell 등) 이때 num_units 즉 hidden size를 정해야 한다.\n",
    "  \n",
    "  - Step 2: 입력 x와 cell을 기반으로 구동을 시켜야 하는데 이때 tf.nn.dynamic_rnn함수를 사용한다. \n",
    "\n",
    "  - 두가지 스텝으로 나눈 목적은 cell을 생성하는 형태를 쉽게 변경하기 위해서다.\n",
    "\n",
    "\n",
    "### 6.2. Cell을 만들고 x 데이터를 입력하여 어떤 결과의 ht가 나오는지 살펴보는 예제\n",
    "\n",
    "<img src=\"https://cloud.githubusercontent.com/assets/901975/23348727/cc981856-fce7-11e6-83ea-4b187473466b.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- 입력 x는 one hot encoding 형태로 준다. 이 입력의 형태에 따라 출력의 shape이 결정된다.\n",
    "\n",
    "- 아래 소스코드에서 x는 [[h]]로 주어졌는데 이것 것은 h에 해당하는 one hot encoding 값인  [[[1,0,0,0]]] 로서 shape은 [1,1,4]가 된다.\n",
    "\n",
    "- 세번째의 4는 one hot encoding의 dimension 값이다. 여기서는 4가 된다.\n",
    "\n",
    "- 두번째의 1은 sequence_lengh로에서 one hot encoding된 문자를 몇개를 받을것인지 정한 값이다. RNN에서 입력을 series로 받을수 있는 장점이 있다고 했는데, sequence_length가 입력 데이터를 몇개 받을것인지를 의미한다.\n",
    "\n",
    "- 첫번째 1은 batch_size로서 sequence_lengh의 문자열을 몇개씩 받을것인지 정한 값이다. 대량의 입력 데이터를 효율적으로 받기 위한것이다.\n",
    "\n",
    "- 출력 ht는 hidden 사이즈로서 개발자가 알아서(?) 정한다. 여기서는 2가 된다.\n",
    "\n",
    "- 출력 ht의 shape은 입력 shape의 bach_size, sequence_lengh, 그리고 출력의 hidden_size로 결정되는데 하기소스 에서는 [1,1,2]가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "# http://learningtensorflow.com/index.html\n",
    "# http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/\n",
    "# https://github.com/hunkim/DeepLearningZeroToAll\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "array([[[1., 0., 0., 0.]]], dtype=float32)\n",
      "array([[[0.3312397 , 0.64907897]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('one_cell') as scope:\n",
    "    # Step 1\n",
    "    # One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    print(cell.output_size, cell.state_size)\n",
    "\n",
    "    x_data = np.array([[h]], dtype=np.float32) # x_data = [[[1,0,0,0]]]\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    # Step2\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과적으로 hidden size를 2로 정했기 떄문에 2개의 ht를 출력 했다.\n",
    "- 두 값은 초기 Weight 값이 랜덤하게 정해졌기 때문이다.\n",
    "- 위 그림과 같이 outputs의 shape은 [1,1,2] 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Unfolding to n sequences\n",
    "\n",
    "<img src=\"https://cloud.githubusercontent.com/assets/901975/23383634/649efd0a-fd82-11e6-925d-8041242743b0.png\" alt=\"\" title=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하기 소스에서는 h, e, l, l, o를 입력으로 주었기 때문에 sequence_lenth는 5가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)\n",
      "array([[[ 0.20307572,  0.45051005],\n",
      "        [-0.582679  , -0.6799246 ],\n",
      "        [-0.0949578 ,  0.3987969 ],\n",
      "        [-0.4909559 , -0.15217248],\n",
      "        [-0.35596716,  0.79350567]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('two_sequances') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
    "    print(x_data.shape)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과적으로 hidden size가 2, sequence_length가 5, batch_size가 1인 출력이 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Batching input\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/B5GtZuUvujQ/maxresdefault.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "- 하기 소스에서는 hello, eolll, lleel을 입력으로 주었기 때문에 sequence_length는 5, batch_size는 3이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)\n",
      "array([[[-0.07422438,  0.04761285],\n",
      "        [ 0.01398429, -0.00768795],\n",
      "        [ 0.15866917,  0.002491  ],\n",
      "        [ 0.22025451,  0.00481884],\n",
      "        [ 0.04798251,  0.02212285]],\n",
      "\n",
      "       [[ 0.05469137, -0.03794539],\n",
      "        [-0.03193718,  0.00084602],\n",
      "        [ 0.1291398 ,  0.01384986],\n",
      "        [ 0.20831743,  0.01562873],\n",
      "        [ 0.24378094,  0.01288211]],\n",
      "\n",
      "       [[ 0.15182973,  0.01133412],\n",
      "        [ 0.21781428,  0.01236716],\n",
      "        [ 0.18148851, -0.04054077],\n",
      "        [ 0.18034795, -0.06975642],\n",
      "        [ 0.24427673, -0.07089303]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('3_batches') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(\n",
    "        cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과적으로 hidden size가 2, sequence_length가 5, batch_size가 3인 출력이 나왔다.\n",
    "\n",
    "- 하기 소스에서는 hello, eolll, lleel을 입력으로 주었기 때문에 sequence_length는 5, batch_size는 3이 된다. 위 소스와 다른점은 sequence_length를 직접 파라미터로 주었고 결과는 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)\n",
      "array([[[ 0.08456729,  0.17041391],\n",
      "        [-0.01894175,  0.09074476],\n",
      "        [-0.08715205,  0.09361196],\n",
      "        [-0.13500401,  0.10542631],\n",
      "        [-0.12139936,  0.01657603]],\n",
      "\n",
      "       [[-0.07369197, -0.01015405],\n",
      "        [-0.09755881, -0.02169333],\n",
      "        [-0.17746384,  0.03082916],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.08068489,  0.05343523],\n",
      "        [-0.13173534,  0.08288844],\n",
      "        [-0.16055581,  0.04604651],\n",
      "        [-0.1652531 ,  0.01564497],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('3_batches_dynamic_length') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(\n",
    "        cell, x_data, sequence_length=[5,3,4], dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Initial state\n",
    "- initial_state를  zero_state로 초기화 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)\n",
      "array([[[-0.05778912, -0.10833783],\n",
      "        [ 0.09644508, -0.08903784],\n",
      "        [ 0.01725953, -0.12827596],\n",
      "        [-0.0662363 , -0.17657696],\n",
      "        [ 0.12660974, -0.08763576]],\n",
      "\n",
      "       [[ 0.112213  ,  0.01687984],\n",
      "        [ 0.28726444,  0.0737764 ],\n",
      "        [ 0.09988581, -0.05714034],\n",
      "        [-0.02238259, -0.13308102],\n",
      "        [-0.08896179, -0.1797359 ]],\n",
      "\n",
      "       [[-0.08288808, -0.08829874],\n",
      "        [-0.12647307, -0.1458457 ],\n",
      "        [ 0.07303302, -0.17805964],\n",
      "        [ 0.15202868, -0.14663127],\n",
      "        [ 0.07407042, -0.15284944]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('initial_state') as scope:\n",
    "    batch_size = 3\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                      [e, o, l, l, l],\n",
    "                      [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch: 3\n",
    "    hidden_size=2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data,\n",
    "                                         initial_state=initial_state, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Create input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]],\n",
      "\n",
      "       [[15., 16., 17.],\n",
      "        [18., 19., 20.],\n",
      "        [21., 22., 23.],\n",
      "        [24., 25., 26.],\n",
      "        [27., 28., 29.]],\n",
      "\n",
      "       [[30., 31., 32.],\n",
      "        [33., 34., 35.],\n",
      "        [36., 37., 38.],\n",
      "        [39., 40., 41.],\n",
      "        [42., 43., 44.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create input data\n",
    "batch_size=3\n",
    "sequence_length=5\n",
    "input_dim=3\n",
    "\n",
    "x_data = np.arange(45, dtype=np.float32).reshape(batch_size, sequence_length, input_dim)\n",
    "pp.pprint(x_data)  # batch, sequence_length, input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7. Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 7.48110488e-02,  4.12624627e-02, -9.81451720e-02,\n",
      "         -4.58508693e-02, -6.72385171e-02],\n",
      "        [ 3.95697318e-02,  1.93976834e-02, -8.82038027e-02,\n",
      "         -1.37005150e-01, -5.50371669e-02],\n",
      "        [ 1.36544313e-02,  2.78955442e-03, -3.88649367e-02,\n",
      "         -2.51669079e-01, -1.41604347e-02],\n",
      "        [ 4.43557836e-03,  2.95496691e-04, -9.21832118e-03,\n",
      "         -3.61898363e-01, -1.59202924e-03],\n",
      "        [ 1.41868938e-03,  2.78856842e-05, -1.32527947e-03,\n",
      "         -4.52703238e-01, -7.99781701e-05]],\n",
      "\n",
      "       [[ 3.54791380e-04,  1.27511225e-06, -1.03235609e-04,\n",
      "         -7.45713338e-02, -6.94698315e-07],\n",
      "        [ 1.09441113e-04,  1.87836562e-07, -2.08458423e-05,\n",
      "         -1.33192539e-01, -5.50214772e-08],\n",
      "        [ 3.28911519e-05,  1.98798329e-08, -3.45821059e-06,\n",
      "         -1.76927119e-01, -3.76000342e-09],\n",
      "        [ 9.68641052e-06,  1.87661553e-09, -5.79109837e-07,\n",
      "         -2.08437979e-01, -2.59325256e-10],\n",
      "        [ 2.80797872e-06,  1.69934275e-10, -9.87668685e-08,\n",
      "         -2.30607495e-01, -1.79524295e-11]],\n",
      "\n",
      "       [[ 7.33964896e-07,  7.88068742e-12, -1.69427494e-08,\n",
      "         -1.47982435e-02, -1.19003700e-12],\n",
      "        [ 2.08719086e-07,  1.13775666e-12, -2.95299274e-09,\n",
      "         -2.49744207e-02, -8.20540535e-14],\n",
      "        [ 5.89036162e-08,  1.18308778e-13, -5.10692488e-10,\n",
      "         -3.18976305e-02, -5.63044018e-15],\n",
      "        [ 1.65252967e-08,  1.10765660e-14, -8.83901355e-11,\n",
      "         -3.65706235e-02, -3.85483424e-16],\n",
      "        [ 4.61469130e-09,  1.00137876e-15, -1.53041260e-11,\n",
      "         -3.97061780e-02, -2.63480517e-17]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('generated_data') as scope:\n",
    "    # One cell RNN input_dim (3) -> output_dim (5). sequence: 5, batch: 3\n",
    "    cell = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, initial_state=initial_state, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7. Multi RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic rnn:  Tensor(\"MultiRNNCell/rnn/transpose_1:0\", shape=(3, 5, 2), dtype=float32)\n",
      "array([[[-0.00027085, -0.00084227],\n",
      "        [ 0.00534773,  0.00444331],\n",
      "        [ 0.0189098 ,  0.02099136],\n",
      "        [ 0.03821621,  0.04889377],\n",
      "        [ 0.05934175,  0.08434415]],\n",
      "\n",
      "       [[ 0.00857534,  0.00970436],\n",
      "        [ 0.02859911,  0.03536183],\n",
      "        [ 0.05668376,  0.07609322],\n",
      "        [ 0.08596861,  0.12464718],\n",
      "        [ 0.11093155,  0.17272854]],\n",
      "\n",
      "       [[ 0.00989135,  0.01119583],\n",
      "        [ 0.03240403,  0.04011082],\n",
      "        [ 0.06242602,  0.08408495],\n",
      "        [ 0.09204917,  0.13425173],\n",
      "        [ 0.116112  ,  0.18219046]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 2\n",
    "\n",
    "with tf.variable_scope('MultiRNNCell') as scope:\n",
    "# Make a lstm cell with hidden_size (each unit output vector size)\n",
    "    def lstm_cell():\n",
    "        cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "        return cell\n",
    "\n",
    "    multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(3)],\n",
    "        state_is_tuple=True)\n",
    "    \n",
    "    # rnn in/out\n",
    "    outputs, _states = tf.nn.dynamic_rnn(multi_cells, x_data, dtype=tf.float32)\n",
    "    print(\"dynamic rnn: \", outputs)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())  # batch size, unrolling (time), hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8. Dynamic RNN\n",
    "- 이전 예제에서는 sequence가 길이가 정해져 있었다. 예를 들어 번역기의 경우 사용자에 따라 번역을 요청하는 문자열의 길이가 다 다를것이다. 이에따라 가변 시퀀스 데이터를 처리할수 있어야 한다.\n",
    "\n",
    "- Tensorflow에서는 tf.nn.dynamic_rnn 함수의 파라미터중 sequence_lengh에서 문자열의 크기를 알려줄수 있다. 문자열은 결국 batch이므로 batch size를 아래와 같이 줄수 있다.\n",
    "\n",
    "- 출력 결과를 보면 batch 1에서 1을 주었기 때문에 첫번째 element 5개만 값이 출력되고 나뭐지는 모두 0이 출력되는 것을 확인할수 있다.\n",
    "\n",
    "- 만약 sequence_length를 정의하지 않았을 경우 내부 wight에 따라 모두 어떤 값들을 출력되게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic rnn:  Tensor(\"dynamic_rnn/rnn/transpose_1:0\", shape=(3, 5, 5), dtype=float32)\n",
      "array([[[-2.2194876e-01, -3.5411473e-02, -7.6707162e-02,  1.9601391e-01,\n",
      "         -1.7267908e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]],\n",
      "\n",
      "       [[-5.1824975e-01, -7.1828973e-11, -7.1079111e-01,  7.1213329e-01,\n",
      "         -9.6094620e-04],\n",
      "        [-6.0398436e-01, -1.5767901e-11, -7.4659145e-01,  9.3127543e-01,\n",
      "         -2.4893592e-04],\n",
      "        [-6.5752828e-01, -2.5333351e-12, -7.5408649e-01,  9.7480178e-01,\n",
      "         -6.4929780e-05],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]],\n",
      "\n",
      "       [[-6.5096807e-01, -6.4307270e-20, -7.5915867e-01,  7.5559402e-01,\n",
      "         -1.6757804e-06],\n",
      "        [-7.5529253e-01, -1.4161292e-20, -7.6082230e-01,  9.6011966e-01,\n",
      "         -4.3597126e-07],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('dynamic_rnn') as scope:\n",
    "    cell = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32,\n",
    "                                         sequence_length=[1, 3, 2])\n",
    "    # lentgh 1 for batch 1, lentgh 2 for batch 2\n",
    "    \n",
    "    print(\"dynamic rnn: \", outputs)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())  # batch size, unrolling (time), hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9. Bi-directional dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(   array([[[ 1.0000140e-01, -1.5170319e-01,  1.8074368e-01,  2.5124910e-01,\n",
      "         -1.6122947e-02],\n",
      "        [ 7.5866051e-02, -1.7318512e-01,  1.7943960e-01,  7.0978105e-01,\n",
      "         -4.0421192e-02],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]],\n",
      "\n",
      "       [[ 1.6805212e-04, -2.7985610e-05,  4.1677775e-03,  7.5956744e-01,\n",
      "         -1.5049798e-03],\n",
      "        [ 2.5644591e-05, -5.3059134e-06,  1.3360588e-03,  9.6329087e-01,\n",
      "         -3.8348786e-03],\n",
      "        [ 4.6667906e-06, -6.4679745e-07,  4.5586433e-04,  9.9482572e-01,\n",
      "         -4.3543214e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]],\n",
      "\n",
      "       [[ 5.3603060e-08, -1.2590208e-09,  2.2357472e-05,  7.6158780e-01,\n",
      "         -6.3068823e-05],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]]], dtype=float32),\n",
      "    array([[[-7.6528743e-02, -4.2008436e-01, -2.4419948e-01,  2.8515473e-01,\n",
      "         -1.1489665e-02],\n",
      "        [-1.7413637e-03, -4.6153444e-01, -1.8223388e-01,  5.9102062e-02,\n",
      "         -3.3878633e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]],\n",
      "\n",
      "       [[ 4.1077310e-05, -9.4049042e-01, -3.3250151e-03,  4.6184818e-05,\n",
      "         -5.3875536e-01],\n",
      "        [ 5.0413023e-06, -8.4272778e-01, -1.0015977e-03,  6.8503809e-06,\n",
      "         -5.6688750e-01],\n",
      "        [ 4.6965366e-07, -5.6907982e-01, -3.2426667e-04,  1.0743653e-06,\n",
      "         -5.8032787e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]],\n",
      "\n",
      "       [[ 1.6044814e-09, -5.9440583e-01, -1.1848581e-05,  4.2343675e-09,\n",
      "         -6.2811720e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]]], dtype=float32))\n",
      "(   LSTMStateTuple(c=array([[ 8.2840919e-01, -1.7733762e-01,  6.5099758e-01,  1.4477229e+00,\n",
      "        -1.2345995e-01],\n",
      "       [ 1.0427725e+00, -6.4679750e-07,  8.2304132e-01,  2.9991584e+00,\n",
      "        -1.0565413e-02],\n",
      "       [ 9.9986911e-01, -1.2590209e-09,  8.0292952e-01,  1.0000000e+00,\n",
      "        -2.0609581e-04]], dtype=float32), h=array([[ 7.5866051e-02, -1.7318512e-01,  1.7943960e-01,  7.0978105e-01,\n",
      "        -4.0421192e-02],\n",
      "       [ 4.6667906e-06, -6.4679745e-07,  4.5586433e-04,  9.9482572e-01,\n",
      "        -4.3543214e-03],\n",
      "       [ 5.3603060e-08, -1.2590208e-09,  2.2357472e-05,  7.6158780e-01,\n",
      "        -6.3068823e-05]], dtype=float32)),\n",
      "    LSTMStateTuple(c=array([[-1.8246186e-01, -7.2415590e-01, -3.5535640e-01,  9.5980841e-01,\n",
      "        -2.0004064e-02],\n",
      "       [ 1.5017857e-01, -1.7423317e+00, -3.3250328e-03,  1.0054443e+00,\n",
      "        -1.0021577e+00],\n",
      "       [ 2.1373900e-02, -6.8445170e-01, -1.1848583e-05,  1.0000000e+00,\n",
      "        -9.9997950e-01]], dtype=float32), h=array([[-7.6528743e-02, -4.2008436e-01, -2.4419948e-01,  2.8515473e-01,\n",
      "        -1.1489665e-02],\n",
      "       [ 4.1077310e-05, -9.4049042e-01, -3.3250151e-03,  4.6184818e-05,\n",
      "        -5.3875536e-01],\n",
      "       [ 1.6044814e-09, -5.9440583e-01, -1.1848581e-05,  4.2343675e-09,\n",
      "        -6.2811720e-01]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('bi-directional') as scope:\n",
    "    # bi-directional rnn\n",
    "    cell_fw = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    cell_bw = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_data,\n",
    "                                                      sequence_length=[2, 3, 1],\n",
    "                                                      dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(sess.run(outputs))\n",
    "    pp.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.10. Flattern based softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]],\n",
      "\n",
      "       [[15., 16., 17.],\n",
      "        [18., 19., 20.],\n",
      "        [21., 22., 23.],\n",
      "        [24., 25., 26.],\n",
      "        [27., 28., 29.]],\n",
      "\n",
      "       [[30., 31., 32.],\n",
      "        [33., 34., 35.],\n",
      "        [36., 37., 38.],\n",
      "        [39., 40., 41.],\n",
      "        [42., 43., 44.]]], dtype=float32)\n",
      "array([[ 0.,  1.,  2.],\n",
      "       [ 3.,  4.,  5.],\n",
      "       [ 6.,  7.,  8.],\n",
      "       [ 9., 10., 11.],\n",
      "       [12., 13., 14.],\n",
      "       [15., 16., 17.],\n",
      "       [18., 19., 20.],\n",
      "       [21., 22., 23.],\n",
      "       [24., 25., 26.],\n",
      "       [27., 28., 29.],\n",
      "       [30., 31., 32.],\n",
      "       [33., 34., 35.],\n",
      "       [36., 37., 38.],\n",
      "       [39., 40., 41.],\n",
      "       [42., 43., 44.]], dtype=float32)\n",
      "array([[[  25.,   28.,   31.,   34.,   37.],\n",
      "        [  70.,   82.,   94.,  106.,  118.],\n",
      "        [ 115.,  136.,  157.,  178.,  199.],\n",
      "        [ 160.,  190.,  220.,  250.,  280.],\n",
      "        [ 205.,  244.,  283.,  322.,  361.]],\n",
      "\n",
      "       [[ 250.,  298.,  346.,  394.,  442.],\n",
      "        [ 295.,  352.,  409.,  466.,  523.],\n",
      "        [ 340.,  406.,  472.,  538.,  604.],\n",
      "        [ 385.,  460.,  535.,  610.,  685.],\n",
      "        [ 430.,  514.,  598.,  682.,  766.]],\n",
      "\n",
      "       [[ 475.,  568.,  661.,  754.,  847.],\n",
      "        [ 520.,  622.,  724.,  826.,  928.],\n",
      "        [ 565.,  676.,  787.,  898., 1009.],\n",
      "        [ 610.,  730.,  850.,  970., 1090.],\n",
      "        [ 655.,  784.,  913., 1042., 1171.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# flattern based softmax\n",
    "hidden_size=3\n",
    "sequence_length=5\n",
    "batch_size=3\n",
    "num_classes=5\n",
    "\n",
    "pp.pprint(x_data) # hidden_size=3, sequence_length=4, batch_size=2\n",
    "x_data = x_data.reshape(-1, hidden_size)\n",
    "pp.pprint(x_data)\n",
    "\n",
    "softmax_w = np.arange(15, dtype=np.float32).reshape(hidden_size, num_classes)\n",
    "outputs = np.matmul(x_data, softmax_w)\n",
    "outputs = outputs.reshape(-1, sequence_length, num_classes) # batch, seq, class\n",
    "pp.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.11. Sequence loss example 1\n",
    "- serial data를 입력으로 학습을 할때 RNN에서도 cost 함수가 필요한데 이때 sequence_loss 함수를 이용한다.\n",
    "- 이 함수의 파라미터로는 logits에 예측값을, targets는 label 데이터 사용한다.\n",
    "- prediction의 [0.2, 0.7]이면 1로, [0.6, 0.2]면 0으로, [0.2, 0.9]면 1로 예측했다는 의미이다.\n",
    "- 이에 따라 cost는 약 0.59로 그렇게 좋은 결과는 아니라는 것을 알수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5967595\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim ]\n",
    "prediction = tf.constant([[[0.2, 0.7], [0.6, 0.2], [0.2, 0.9]]], dtype=tf.float32)\n",
    "\n",
    "# [batch_size * sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=prediction, targets=y_data, weights=weights)\n",
    "sess.run(tf.global_variables_initializer())하기소스는\n",
    "print(\"Loss: \", sequence_loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.12. Sequence loss example 2\n",
    "- 6.11.에 비해 예측이 좀더 정확한 데이터를 임의로 만들어 cost를 구해보면 점점더 좋아지는 결과를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1:  0.55435526 Loss2:  0.474077 Loss3:  0.437488 Loss4:  0.3711007\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim ]\n",
    "prediction1 = tf.constant([[[0.3, 0.6], [0.3, 0.6], [0.3, 0.6]]], dtype=tf.float32)\n",
    "prediction2 = tf.constant([[[0.2, 0.7], [0.2, 0.7], [0.2, 0.7]]], dtype=tf.float32)\n",
    "prediction3 = tf.constant([[[0.2, 0.8], [0.2, 0.8], [0.2, 0.8]]], dtype=tf.float32)\n",
    "prediction4 = tf.constant([[[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]]], dtype=tf.float32)\n",
    "\n",
    "# [batch_size * sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "sequence_loss1 = tf.contrib.seq2seq.sequence_loss(prediction1, y_data, weights)\n",
    "sequence_loss2 = tf.contrib.seq2seq.sequence_loss(prediction2, y_data, weights)\n",
    "sequence_loss3 = tf.contrib.seq2seq.sequence_loss(prediction3, y_data, weights)\n",
    "sequence_loss4 = tf.contrib.seq2seq.sequence_loss(prediction4, y_data, weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Loss1: \", sequence_loss1.eval(),\n",
    "      \"Loss2: \", sequence_loss2.eval(),\n",
    "      \"Loss3: \", sequence_loss3.eval(),\n",
    "      \"Loss4: \", sequence_loss4.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lab2: hi hello\n",
    "- 입력 'h'를 주었을때 'ihello'를 한글자씩 예측해 나가도록 학습시키는 예제이다.\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/994FC13359B513A617\" alt=\"\" title=\"\" />\n",
    "\n",
    "### 7.1. Input and hidden layer\n",
    "- input dimention 은 one hot size이므로 5가 된다.\n",
    "- sequence length는 h i h e l l 을 줄것이므로 6이 된다.\n",
    "- 출력이 되는 hidden size는 one hot으로 출력이 되야 하므로 input demention과 마찬가지로 5가 된다.\n",
    "- batch size는 문자열 1개 (sequence length) 이므로 1이 된다.\n",
    "- classifier 는 h i e l o 이렇게 5가지 문자가 되야 하므로 num_classes는 5가 된다.\n",
    "- RNN 구성시 cell size는 출력값이므로 hidden size 였던 5가 된다.\n",
    "- X는 placeholder로 [data type, batch_size, sequence_lenghth, input_dim]으로 구성 한다.\n",
    "- Y는 placeholder로 label의 크기가 되는데 여기서는 sequence length가 된다.\n",
    "- init state는 zero_state로 구성하는데 batch size를 주어야 한다.\n",
    "\n",
    "### 7.2. FC layer and output layer\n",
    "- cost를 구하기 위한 sequence_loss함수의 logits은 dynamic_rnn이 예측할 결과인 output을, targets은 Y값을 준다. 그리고 weights은 tf.ones함수를 사용하여 모두 1로 주어졌다.\n",
    "- loss는 평균은 sequence_loss의 평균값을 가진다.\n",
    "- cost minimize를 위해 AdamOptimizer를 사용한다.\n",
    "- 학습 과정에서 prediction은 index를 출력하고, result_str은 결과를 charter로 변환하여 보여준다.\n",
    "- 학습 진행되며 loss가 낮게 떨어지며 예측을 잘하고 있는것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \tLoss: 1.6078763 \tPrediction: [[3 3 3 3 3 3]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: llllll\n",
      "Step: 1 \tLoss: 1.5102623 \tPrediction: [[3 3 3 3 3 3]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: llllll\n",
      "Step: 2 \tLoss: 1.4327028 \tPrediction: [[3 3 3 3 3 3]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: llllll\n",
      "Step: 3 \tLoss: 1.3489527 \tPrediction: [[3 3 3 3 3 3]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: llllll\n",
      "Step: 4 \tLoss: 1.2551297 \tPrediction: [[1 3 3 3 3 3]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: illlll\n",
      "Step: 5 \tLoss: 1.140437 \tPrediction: [[1 3 3 3 3 3]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: illlll\n",
      "Step: 6 \tLoss: 1.0167552 \tPrediction: [[1 3 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ilello\n",
      "Step: 7 \tLoss: 0.8969265 \tPrediction: [[1 3 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ilello\n",
      "Step: 8 \tLoss: 0.76952547 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 9 \tLoss: 0.655007 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 10 \tLoss: 0.54275775 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 11 \tLoss: 0.4284713 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 12 \tLoss: 0.33451474 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 13 \tLoss: 0.24750167 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 14 \tLoss: 0.1817708 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 15 \tLoss: 0.13267998 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 16 \tLoss: 0.09433412 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 17 \tLoss: 0.066492125 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 18 \tLoss: 0.0477194 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 19 \tLoss: 0.035096254 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 20 \tLoss: 0.026444634 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 21 \tLoss: 0.020456975 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 22 \tLoss: 0.016229078 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 23 \tLoss: 0.01314158 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 24 \tLoss: 0.010803193 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 25 \tLoss: 0.008980002 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 26 \tLoss: 0.0075339056 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 27 \tLoss: 0.006378603 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 28 \tLoss: 0.0054539884 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 29 \tLoss: 0.0047134547 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 30 \tLoss: 0.004119506 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 31 \tLoss: 0.003641067 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 32 \tLoss: 0.0032533386 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 33 \tLoss: 0.0029363036 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 34 \tLoss: 0.0026745955 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 35 \tLoss: 0.0024560248 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 36 \tLoss: 0.0022713589 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 37 \tLoss: 0.002113483 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 38 \tLoss: 0.001977074 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 39 \tLoss: 0.0018580456 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 40 \tLoss: 0.0017534181 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 41 \tLoss: 0.0016607023 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 42 \tLoss: 0.0015781395 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 43 \tLoss: 0.0015044062 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 44 \tLoss: 0.0014382761 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 45 \tLoss: 0.0013789788 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 46 \tLoss: 0.0013257032 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 47 \tLoss: 0.0012777768 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 48 \tLoss: 0.0012345857 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n",
      "Step: 49 \tLoss: 0.0011956159 \tPrediction: [[1 0 2 3 3 4]] true Y: [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str: ihello\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab12-2 : rnn, character level language model, hi hello example\n",
    "# \n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Dictionaray index to chracter\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hello: hihell ->ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],  # h 0\n",
    "              [0, 1, 0, 0, 0],  # i 1\n",
    "              [1, 0, 0, 0, 0],  # h 0\n",
    "              [0, 0, 1, 0, 0],  # e 2\n",
    "              [0, 0, 0, 1, 0],  # l 3\n",
    "              [0, 0, 0, 1, 0]]] # l 3\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]   # ihello\n",
    "\n",
    "# Parameters\n",
    "num_classes = 5 # h, i, e, l, o\n",
    "input_dim = 5 # one-hot size\n",
    "hidden_size = 5 # output from the LSTM, 5 to directly predict one-hot\n",
    "batch_size = 1 # one sentence\n",
    "sequence_length = 6 # ihello\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim]) # X one-hot\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length]) # Y label\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(inputs=X_for_fc,\n",
    "        num_outputs=num_classes, activation_fn=None)\n",
    "\n",
    "# Reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y,\n",
    "        weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(\"Step:\", i, \"\\tLoss:\", l, \"\\tPrediction:\", result, \"true Y:\", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str:\", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lab3: Long sequences\n",
    "\n",
    "### 8.1. Input layer and hidden layer\n",
    "- sample은 string으로 첫자리는 빈자리 이다.\n",
    "- idx2char는 sample 데이터의 유일한 문자를 리스트로 만들어 반환해준다.\n",
    "- char2idx는 idex2char의 문자를 dictionary로 만들어 반환해 준다. dictionary의 key는 문자가 되고 value는 index가 저장 된다.\n",
    "- dic_size는 char2idx 사이즈와 같고 one hot size와 깉다.\n",
    "- rnn_hidden_size는 RNN의 출력사이즈로서 one hot size와 같다.\n",
    "- num_classes는 char2idx와 같다.\n",
    "- batch_size는 한개의 문자열 이므로 1이 된다.\n",
    "- sequen_length는 sample 전체 길에서 1을 뺀만큼이 된다. x_data의 길이와 같기 떄문이다.\n",
    "- sample_idx는 sample에 저장된 string을 index로 저장한다.\n",
    "- x_data는 sample_idx의 처음부터 마지막 이전까지의 데이터가 된다.\n",
    "- y_data는 sample_idx의 두번째부터 마지막 자리까지의 데이터가 된다.\n",
    "- x_data는 one_hot 함수를 이용하여 x_ont_hot 변수가 생성 된다. shape에 주의해야 한다.\n",
    "- cell을 만들고, zero_state로 초기화 하여 dynamic_rnn을 이용하여 outputs을 생성 한다.\n",
    "\n",
    "### 8.2. FC layer 및 output layer\n",
    "- weights은 tf.ones로 모두 1로 한다.\n",
    "- sequence_loss는 sequence_loss 함수를 사용하고 평균을 내어 loss를 산출한다.\n",
    "- optimzer는 AdamOptimizer를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.276697 Prediction: uyyyuuuuuuuuuuu\n",
      "1 loss: 2.0970514 Prediction: oy youu      ou\n",
      "2 loss: 1.9005873 Prediction: yy you    t  ou\n",
      "3 loss: 1.6513932 Prediction: yy you   tt oou\n",
      "4 loss: 1.3725078 Prediction: yy yuu watt you\n",
      "5 loss: 1.1091274 Prediction: yy you want you\n",
      "6 loss: 0.88443244 Prediction: yy you want you\n",
      "7 loss: 0.68676245 Prediction: yf you want you\n",
      "8 loss: 0.5162858 Prediction: if you want you\n",
      "9 loss: 0.38270456 Prediction: if you want you\n",
      "10 loss: 0.28334925 Prediction: if you want you\n",
      "11 loss: 0.20955688 Prediction: if you want you\n",
      "12 loss: 0.15383844 Prediction: if you want you\n",
      "13 loss: 0.110947646 Prediction: if you want you\n",
      "14 loss: 0.07805044 Prediction: if you want you\n",
      "15 loss: 0.05388548 Prediction: if you want you\n",
      "16 loss: 0.03705665 Prediction: if you want you\n",
      "17 loss: 0.025750833 Prediction: if you want you\n",
      "18 loss: 0.018248836 Prediction: if you want you\n",
      "19 loss: 0.013245512 Prediction: if you want you\n",
      "20 loss: 0.009865459 Prediction: if you want you\n",
      "21 loss: 0.007546276 Prediction: if you want you\n",
      "22 loss: 0.0059262826 Prediction: if you want you\n",
      "23 loss: 0.0047707143 Prediction: if you want you\n",
      "24 loss: 0.003926331 Prediction: if you want you\n",
      "25 loss: 0.0032936423 Prediction: if you want you\n",
      "26 loss: 0.00280778 Prediction: if you want you\n",
      "27 loss: 0.0024263663 Prediction: if you want you\n",
      "28 loss: 0.0021211058 Prediction: if you want you\n",
      "29 loss: 0.0018728309 Prediction: if you want you\n",
      "30 loss: 0.001668337 Prediction: if you want you\n",
      "31 loss: 0.001498031 Prediction: if you want you\n",
      "32 loss: 0.0013549926 Prediction: if you want you\n",
      "33 loss: 0.0012339038 Prediction: if you want you\n",
      "34 loss: 0.00113075 Prediction: if you want you\n",
      "35 loss: 0.0010423634 Prediction: if you want you\n",
      "36 loss: 0.0009662476 Prediction: if you want you\n",
      "37 loss: 0.00090035796 Prediction: if you want you\n",
      "38 loss: 0.0008431009 Prediction: if you want you\n",
      "39 loss: 0.0007931525 Prediction: if you want you\n",
      "40 loss: 0.0007494021 Prediction: if you want you\n",
      "41 loss: 0.0007109458 Prediction: if you want you\n",
      "42 loss: 0.0006769584 Prediction: if you want you\n",
      "43 loss: 0.0006468926 Prediction: if you want you\n",
      "44 loss: 0.0006201849 Prediction: if you want you\n",
      "45 loss: 0.0005962956 Prediction: if you want you\n",
      "46 loss: 0.0005748438 Prediction: if you want you\n",
      "47 loss: 0.0005555359 Prediction: if you want you\n",
      "48 loss: 0.0005380543 Prediction: if you want you\n",
      "49 loss: 0.0005222244 Prediction: if you want you\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab12-3 : rnn, character level language model, character sequence\n",
    "# \n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
    "hidden_size = len(char2idx)  # RNN output size\n",
    "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
    "batch_size = 1  # one sample data, one batch\n",
    "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
    "learning_rate = 0.1\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) e.g. hello: hell\n",
    "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) e.g. hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "x_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(\n",
    "    cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes,\n",
    "        activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y,\n",
    "        weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "\n",
    "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Lab4: Long sequences with softmax layer\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/9960873359B7E0E11E\" alt=\"\" title=\"\" />\n",
    "\n",
    "- RNN에서도 CNN에서 사용했던 Softmax layer를 사용할수 있다.\n",
    "- RNN의 CELL은 펼치기 전에 한개의 CELL이므로 reshape을 통해 x_for_softmax를 만들어 softmax 함수로로 처리할수 있다. reshape의 입력은 RNN의 출력이고 크기는 hidden_size가 되어야 한다.\n",
    "- softmax_w 입력의 크기는 hidden_size가 되고, 출력의 크기는 num_classes가 된다.\n",
    "- softmax_b 출력의 크기는 num_classes가 된다.\n",
    "- softmax의 출력은 FC layer의 입력이 되어야 하므로 다시 reshape을 통해 outputs를 만들게 된다. reshape의 입력은 softmax의 출력이고, [batch_size, seq_length, num_classes]의 shape이 되어야 한다. 이 shape은 Softmax layer의 입력이었던 RNN의 출력 크기와 동일하다.\n",
    "- sequnce_loss에서 logits에 outputs을 주어야 한다. 이전 lab4에서는 RNN의 출력을 직접 logits에 주었으나 이것은 activation을 거친 결과이기 때문에 잘못된것이다. softmax layer 출력과 같이 activation을 거치지 않은 결과를 logits으로 주어야 한다. 그래야 학습시 좋은 성능을 낼수 있다.\n",
    "- 이외 부분은 lab3과 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.2742977 Prediction: ow oowioa oyoow\n",
      "500 loss: 0.2786146 Prediction: yf you yant you\n",
      "1000 loss: 0.27772188 Prediction: yf you yant you\n",
      "1500 loss: 0.27749297 Prediction: yf you yant you\n",
      "2000 loss: 0.27739808 Prediction: yf you yant you\n",
      "2500 loss: 0.27734923 Prediction: yf you yant you\n",
      "3000 loss: 0.27732077 Prediction: yf you yant you\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab12-4 : rnn, character level language model, character sequence\n",
    "#           softmax only\n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
    "rnn_hidden_size = len(char2idx)  # RNN output size\n",
    "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
    "batch_size = 1  # one sample data, one batch\n",
    "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
    "learning_rate = 0.1\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
    "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "# flatten the data (ignore batches for now). No effect if the batch size is 1\n",
    "X_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "X_for_softmax = tf.reshape(X_one_hot, [-1, rnn_hidden_size])\n",
    "\n",
    "# softmax layer (rnn_hidden_size -> num_classes)\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [rnn_hidden_size, num_classes])\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [num_classes])\n",
    "outputs = tf.matmul(X_for_softmax, softmax_w) + softmax_b\n",
    "\n",
    "# expend the data (revive the batches)\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "# Compute sequence cost/loss\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)  # mean all sequence loss\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(3001):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "               \n",
    "        if i % 500 == 0:\n",
    "                print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Lab5: Long character sequence with stacked RNN\n",
    "- Sentence 를 장문으로 할때 다음 문자를 예측하는 예제이다.\n",
    "- Sentence 를 seqeunce_length 만큼 잘라서 traning data set 이 될 dataX, dataY를 만든다. 1바이트씩 window를 shift시키면서 만든다.\n",
    "- batch_size는 dataX 만큼이 될것이며 169가 된다.\n",
    "- RNN을 깊게 학습시킬수 있도록 MultiRNNCell 함수를 사용하였다. 이를 적용하지 않을 경우 학습이잘 일어나지 않는다.\n",
    "- 나뭐지 과정은 lab3와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(?, 10, 25), dtype=float32)\n",
      "Step i: 499 j: 169 char_set:   the sea. Loss: 0.22903594\n",
      "---------- Result -------\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab12-5 : rnn, character level language model, long character sequence\n",
    "# \n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {w: i for i, w in enumerate(char_set)}\n",
    "\n",
    "data_dim = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "sequence_length = 10  # Any arbitrary number\n",
    "learning_rate = 0.1\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "#    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x = [char_dic[c] for c in x_str]  # x str to index\n",
    "    y = [char_dic[c] for c in y_str]  # y str to index\n",
    "\n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "\n",
    "batch_size = len(dataX)\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "# One-hot encoding\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "print(X_one_hot)  # check out the shape\n",
    "\n",
    "\n",
    "# Make a lstm cell with hidden_size (each unit output vector size)\n",
    "def lstm_cell():\n",
    "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "    return cell\n",
    "\n",
    "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)],\n",
    "        state_is_tuple=True)\n",
    "\n",
    "# outputs: unfolding size x hidden size, state = hidden size\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes,\n",
    "        activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "# All weights are 1 (equal weights)\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "mean_loss = tf.reduce_mean(sequence_loss)\n",
    "train_op = tf.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate).minimize(mean_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(500):\n",
    "    _, l, results = sess.run(\n",
    "        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n",
    "    for j, result in enumerate(results):\n",
    "        index = np.argmax(result, axis=1)\n",
    "#        print(i, j, ''.join([char_set[t] for t in index]), l)\n",
    "print(\"Step i:\", i, \"j:\", j, \"char_set:\", ''.join([char_set[t] for t in index]), \"Loss:\", l)\n",
    "\n",
    "# Let's print the last char of each result to check it works\n",
    "print(\"---------- Result -------\")\n",
    "results = sess.run(outputs, feed_dict={X: dataX})\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis=1)\n",
    "    if j is 0:  # print all for the first result to make a sentence\n",
    "        print(''.join([char_set[t] for t in index]), end='')\n",
    "    else:\n",
    "        print(char_set[index[-1]], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Lab6: RNN with time seriesdata (stock prediction)\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/odMGK7pwTqY/maxresdefault.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "- 주식의 가격 데이터를 가지고 있고, 7일 단위로 주식값을 읽고 학습하여, 8일차의 Close 가격을 예측하는 예제이다. 이런 형태의 예측 모델을 many to model 이다.\n",
    "- 입력의 dimension 은 Open, High, Low, Volume, Close가 되므로 5가 된다.\n",
    "- sequence length는 7일 단위씩 읽어올 것이므로 7이 된다.\n",
    "- output dimension 은 8일차의 Close값을 예측 하므로 1이 된다.\n",
    "- 파일에서 데이터를 읽어와 시간을 역순으로 데이터를 정리한다.\n",
    "- 읽은 데이터는 MinMaxScaler를 통해 normalize 한다.\n",
    "- x값은 7일간의 데이터 전체를 가진다\n",
    "- y값은 8일차의 데이터에서 close 값만 label로 사용한다.\n",
    "- training set은 dataY의 70%, test set은 dataY의 30%를 사용한다.\n",
    "- layer는 Input - RNN - FC - Output 의 형태가 된다.\n",
    "- rnn layer 설계시 cell의 hideen_dim은 10개로 정했다.\n",
    "- rnn 출력 10개중 마지막 1개만 FC layer의 입력으로 사용한다. 이에따라 output_dim도 1이 된다. 그리고 activation function은 사용하지 않는다. fully_connected의 파라미터로 입력해주게되면 여기서 Y_fred 즉 예측값을 얻게된된다.\n",
    "- cost 함수는 예측값이 1개의 값이므로 간단히 tf.seuare(Y_pred - Y)를 사용하고 모두 더하게 된다. 예측값이 여러개 였을 경우 평균을 내었으므로 주의 한다!\n",
    "- optimizer는 AdamOptimizer를 사용한다.\n",
    "- 학습이 완료되면 Y_pred 예측값과 textX라는 테스트 데이터를 주어 예측한 Y값을 testPredict에 담는다. 이값과 testY값을 비교하기 위해 그래프로 출력해보면 비슷하게 예측하고 있다는것을 알수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 120.23834228515625\n",
      "[step: 100] loss: 0.8781153559684753\n",
      "[step: 200] loss: 0.7470797300338745\n",
      "[step: 300] loss: 0.6202673316001892\n",
      "[step: 400] loss: 0.5380198359489441\n",
      "[step: 500] loss: 0.48963862657546997\n",
      "RMSE: 0.025228282436728477\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab12-6 : rnn, stock prediction\n",
    "# \n",
    "################################################################################\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "    Returns\n",
    "    ----------\n",
    "    data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 501\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
    "xy = xy[::-1]  # reverse order (chronically ordered)\n",
    "xy = MinMaxScaler(xy)\n",
    "x = xy\n",
    "y = xy[:, [-1]]  # Close as label\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "#    print(_x, \"->\", _y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(\n",
    "    dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(\n",
    "    dataY[train_size:len(dataY)])\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        if i % 100 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={\n",
    "                    targets: testY, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(testY)\n",
    "    plt.plot(test_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소스 최상단에 \"%matplotlib inline\" 을 추가하면 jyputer notebook에서 출력을 볼수있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
