{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Improve Neural Network Performance\n",
    "- Neural network의 학습 결과를 좋게 만드는 방법을 정리하면 아래와 같다.\n",
    "  - Input layer 및 hidden layer의 activation fuction은 ReLU를 사용하고, output layer의 activation function은 sigmoid를 사용한다.\n",
    "\n",
    "  - Weights의 초기화는 Xavier initialization을 사용한다.\n",
    "  \n",
    "  - 학습과정에 Weight 설계시 dropout을 사용한다.\n",
    "  \n",
    "  - 다수의 독립적인 Traning model을 학습시켜 결과를 합치는 emsemble을 사용한다.\n",
    "  \n",
    "  - Network 구성을 다양한 방법으로 시도해 본다.\n",
    "\n",
    "## 1. Better activation function\n",
    "\n",
    "### 1.1. Activation function\n",
    "- Neural network에서 여러개의 network이 연결되어 구성된다. 일반적으로 input layer, hidden layer, output layer로 나눈다.\n",
    "- 이때 한 network의 hypothesis는 sigmoid와 같은 함수가 있어서 일정 값 이상이 될때 activation 시킨다. 그래서 Neural network에서는 activation function 이라고 부른다.\n",
    "\n",
    "- Neural network을 블럭다이어 그램으로 그려보면 activation function은 아래 그림과 같이 한 network의 출력단에 나타내수 있다.\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/20090158/node-output-300x195.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "### 1.2. Vanishing gradient problem\n",
    "- Network layer가 깊을수록 학습이 비례하여 좋아지지 않는다. output layer로부터 3단 이후에는 학습이 되지 않는다.\n",
    "\n",
    "- Chap 5에서 sigmoid 함수는 0.0 ~ 1.0 사이의 값을 출력하는 activation function 이라고 했다. 그리고 Chap 9에서 backpropagation은 종단부터 미분을 해나가야 학습이 된다고 했다. 아래 그림은 어떤 neural network 이 있다고 할때 backpropagation 과정을 나타낸 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                   x\n",
    "-(S)-(Net N-2)-(S)-----(Net N-1)--------\\\n",
    "                  ∂f/∂x = ∂f/∂g * ∂g/∂x  \\     \n",
    "                        = ∂f/∂g * y       \\  g                       a\n",
    "                        = ∂f/∂g * 0.01    (*)---(S)-----(Net N)-(S)------\\\n",
    "                                          / ∂f/∂g                   ∂f/∂a \\\n",
    "                                         /                                 \\\n",
    "                    y = 0.01            /                                  (+)--- f\n",
    "-(S)--(Net N-2)-(S)--------------------/                              b    /\n",
    "                   ∂f/∂y = ∂f/∂g * ∂g/∂y                          --------/\n",
    "                                                                    ∂f/∂b \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 network에서 Net N-2의 출력이 -10이었다면 Simoid curve에 따라 출력 y는 0.01과 같이 0에 가까운 값이 된다.\n",
    "\n",
    "- 그러므로 chain rule에 따라 ∂f/∂x = ∂g/∂x ＊ 0.01이 된다. 이 값은 이전 network과 또다시 곱의 연산을 거쳐서 미분의 과정을 반복하게 되는데, 마찬가지로 Net N-3의 sigmoid curve에 따라 y에 해당하는 값은 0에 가까워 질수 있고, x에 해당하는 값은 network이 깊어질수록 미분을 거치면서 \"0.01 ＊ 0.02 ＊ ... ＊ 0.001\" 로 누적되어 거의 0에 가까워진다.\n",
    "\n",
    "- 결론적으로 network이 깊어질수록 input layer의 입력 x가 output layer의 출력 f에 미치는 영향은 0에 가까워 지기 때문에 학습이 network 깊이에 비례하여 일어나지 않게 된다. 보통 network의 깊이가 4단 이상이 되면 학습이 일어나지 않는다. \n",
    "\n",
    "- 이와 같이 경사가 사라지는 현상을 Vanishing gradient 라고 한다.\n",
    "\n",
    "<img src=\"http://www.birc.co.kr/wp-content/uploads/2018/01/vanishing_gradient-1024x386.png\" alt=\"\" title=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "- Sigmoid activation function이 가진 vanishing gradient 문제점을 해결하기 위해 Rectifed linear unit이 만들어 졌으며 아래 파란색 그래프와 같이 0이하의 값은 사용하지 않는다는 원리이다.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/495px-Rectifier_and_softplus_functions.svg.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- Deep neural network에서 output layer에서는 0.0 ~ 1.0 사이의 출력을 받아 classifier 해야하므로 sigmoid activation을 사용해야 하고, 나뭐지 input layer 및 hidden layer에는 relu activation을 사용하면 학습이 잘된다.\n",
    "\n",
    "- Tenorflow에서는 아래와 같이 sigmoid 함수를 relu로 바꿔서 사용하면 된다.\n",
    "  - L1 = tf.relu(tf.matmul(X, W1) + b1)\n",
    "  - L2 = tf.relu(tf.matmul(L1, W2) + b2)\n",
    "  - ...\n",
    "  - L10 = tf.relu(tf.matmul(L9, W10) + b10)\n",
    "  - hypothesis = tf.sigmoid(tf.matmul(L10, W11) + b11)\n",
    "  \n",
    "## 1.4. Activation functions\n",
    "- Sigmoid, ReLU이외에도 아래와 같이 다양한 activation fuction이 존재한다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*DRKBmIlr7JowhSbqL6wngg.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- Activation function을 비교한 결과는 아래와 같다.\n",
    "<img src=\"https://camo.qiitausercontent.com/f0c0b6b782f620311b05441ddda4228cc9b43f90/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3130303532332f62613931616231302d393238642d306464612d616438622d6338326534396562343564622e706e67\" alt=\"\" title=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Better weights initialize\n",
    "- 이전 chapter 까지는 아래와 같이 weight 값을 random으로 생성하여 사용하였다. 이에 따라 학습 과정의 cost 값들이 조금씩 달랐다. 이에 따라 가장 좋은 초기화 방법을 사용할 필요가 있다.\n",
    " \n",
    " W1 = tf.Variable(tf.random_normal([2, 10], name='weight1'))\n",
    " \n",
    "\n",
    "### 2.1. Set all initial weights to zero\n",
    "- 아래와 같은 network에서 weight값을 0으로 초기화 했다면 편미분시 x가 0이 되고, x를 전달했던 이전 network의 미분 결과는 모두 0이 되어 학습이 일어나지 않는다. 따라서 절대로 0으로 초기화 하면 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0        0         x\n",
    "-(S)-(Net N-2)-(S)-----(Net N-1)--------\\\n",
    "                  ∂f/∂x = ∂f/∂g * ∂g/∂x  \\     \n",
    "                        = ∂f/∂g * w       \\  g                       a\n",
    "                        = ∂f/∂g * 0.00    (*)---(S)-----(Net N)-(S)------\\\n",
    "                                          / ∂f/∂g                   ∂f/∂a \\\n",
    "                                         /                                 \\\n",
    "                    w = 0.0             /                                  (+)--- f\n",
    "                   --------------------/                              b    /\n",
    "                                                                  --------/\n",
    "                                                                    ∂f/∂b \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Xavier initialization\n",
    "- Makes sure the weights are 'just right', not too small, not too big\n",
    "\n",
    "- Using number of input (fan_in) and output(fan_out)\n",
    "\n",
    "- 구현 방법은 아래와 같이 입력의 개수와 출력의 개수이용하여 식에 대입하면 된다. 놀랍도록 결과가 좋지만 더 놀라운 것은 왜 이렇게 되는지는 설명은 불가능 하다고 한다.\n",
    "\n",
    "- 실제 적용하여 테스트해보면 학습의 초기 과정부터 cost가 금방 떨어지는것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Xavier initialization\n",
    "W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow 에서는 아래와 같이 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Restricted Boatman Machine(RBM) 초기화를 비롯하여 LSUV, OrthonNorm, Xavier, MSRA와 같은 초기화 방법들이 개발되었으나, 2016년 현재에도 어떻게 초기화 하는 방법이 제일 좋은지 밝혀지지 않았다.\n",
    "\n",
    "- 현업에서는 xavier, MSRA 초기화를 많이 사용하는 편인데, 개발자가 직접 변경해가며 좋은 결과를 찾아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropout\n",
    "\n",
    "### 3.1. Overfitting\n",
    "- Network의 깊이가 깊을수록 overfitting 될 가능성이 높다.\n",
    "\n",
    "- Overfitting 된 경우 traning data로 학습후 traning data로 테스트시 accuracy가 0.99가 나오더라도 아래 그래프와 같이 test data로 accuracy 측정시 에러가 떨어질수 있다.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/rpqa6.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "### 3.2. Solution for overfitting\n",
    "  - More training data\n",
    "  - Regularization (chap7. machine learning tips 참조)\n",
    "  - Dropout\n",
    "  - Model ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Dropout\n",
    "- Randomly set some neurons to zero in the forward pass\n",
    "\n",
    "- 아래 그림과 같이 forward 방향으로 랜덤하게 network을 끈어 버리는 것을 의미한다.\n",
    "\n",
    "- 반드시 학습 과정에만 사용해야 한다. 평가시에는 절대로 적용하지 않아야 한다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "- Tensorflow 에서는 아래와 같이 사용한다.\n",
    "  - 한 network layer에서 weight 설계시 activation 출력을 dropout 함수의 입력받아서 다음 network layer의 입력으로 내보내도록 한다.\n",
    "  - Training 시 dropout rate를 설정한다. 일반적으로 0.5 ~ 0.7을 사용하는데 0.7이라면  70%를 통과 시키고 30%는 끈어버리겠다는 의미이다.\n",
    "  - Evaluation 시에는 dropout rate를 1.0으로 사용한다. 즉 dropout을 적용하지 않겠다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# dropout (keep_prob) rate 0.7 on training, but should be 1 for testing!!\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and bias for NN layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels, keep_prob: 1}))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model ensemble\n",
    "- 아래 그림과 같이 다수의 독립적인 Traning set과 Learning model을 학습시켜 결과를 합치는 emsemble 이라 한다.\n",
    "\n",
    "- 실제 적용시 약 2% ~ 5%까지 accuracy가 향상된다.\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/273153116/figure/fig1/AS:267380941127694@1440759992754/BsN-Score-ensemble-neural-network-SF-using-boosting-approach.png\" alt=\"\" title=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Various Neural Network\n",
    "- Neural network의 각 network layer를 다양한 형태로 라우팅하여 Accuracy를 끌어올릴수 있다.\n",
    "\n",
    "- 다양한 아이디어로 network을 구성해보고 잘되면 된다!\n",
    "\n",
    "### 5.1. Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(X)───(L1)───(L2)───(L3)───(L4)───(L6)───(L7)───(Y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Fast forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(X)────(L1)──●──(L2)────(L3)──⊙──(L4)──●──(L6)───(L7)──⊙──(L8)───(Y)\n",
    "             │                ▲        │               ▲\n",
    "             └────────────────┘        └───────────────┘\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Split and merge neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "            ┌──(L2)───(L3)──┐\n",
    "            │               ▼\n",
    "(X)───(L1)──●               ⊙────(L6)───(L7)───(Y)\n",
    "            │               ▲\n",
    "            └──(L4)───(L5)──┘\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(X1)───(L1)───(L2)──┐\n",
    "                    ▼\n",
    "(X2)───(L3)───(L4)──⊙──(L7)───(L8)───(Y)\n",
    "                    ▲\n",
    "(X3)───(L5)───(L6)──┘\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(X1) ────(L1)──●──(L2)──●──(L3)──●──(L4)──┐\n",
    "               │        │        │        │\n",
    "               ▼        ▼        ▼        ▼\n",
    "(X2) ────(L5)──⊙──(L6)──⊙──(L7)──⊙──(L8)──⊙──(Y)\n",
    "               ▲        ▲        ▲        ▲\n",
    "               │        │        │        │\n",
    "(X3) ────(L9)──●──(L10)─●──(L11)─●──(L12)─┘\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lab for Improve Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Lab 1: Softmax classifier for MNIST (Accuracy 0.9023)\n",
    "- Chapter 7의 Lab4: MNIST introduction과 다른점은 아래와 같다.\n",
    "  - cost 함수를 직접 구현하였으나, softmax_cross_entropy_with_logits_v2로 변경하였다.\n",
    "  - GradientDescentOptimizer 대신 AdamOptimizer로 변경하였다.\n",
    "  - Accuracy 결과는 비슷하다.\n",
    "\n",
    "- Weight과 Bias는 랜덤값을 사용한다.\n",
    "\n",
    "- Network 간단하게 Input layer, Hidden layer, Output layer로 구성되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 \tCost: 4.715295316\n",
      "Epoch: 0002 \tCost: 1.608079906\n",
      "Epoch: 0003 \tCost: 1.069499107\n",
      "Epoch: 0004 \tCost: 0.848682950\n",
      "Epoch: 0005 \tCost: 0.726699845\n",
      "Epoch: 0006 \tCost: 0.649306182\n",
      "Epoch: 0007 \tCost: 0.593791151\n",
      "Epoch: 0008 \tCost: 0.552169523\n",
      "Epoch: 0009 \tCost: 0.519451609\n",
      "Epoch: 0010 \tCost: 0.493092402\n",
      "Epoch: 0011 \tCost: 0.470662844\n",
      "Epoch: 0012 \tCost: 0.451730263\n",
      "Epoch: 0013 \tCost: 0.435721565\n",
      "Epoch: 0014 \tCost: 0.421236033\n",
      "Epoch: 0015 \tCost: 0.408534826\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.8974\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [4]\n",
      "Prediction:\t [4]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab10-1 : minist by softmax cross entropy\n",
    "#           accuracy : 0.9023\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10\n",
    "\n",
    "# Input placeholders, MNIST data image of shape 28 * 28 = 784 pixel\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "# 0 ~ 9 digits recognition = 10 classed\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "# Weights and bias for NN layers\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis using softmax\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"\\tCost:\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(hypothesis, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Lab 2: Neural network for MNIST (Accuracy 0.9423)\n",
    "- 위 Lab1과의 차이점은 아래와 같다.\n",
    "  - 3단의 Neural network으로 구성되었다.\n",
    "  - Hidden layer의 activation function은 ReLU를 적용하였다.\n",
    "  - Accuracy 결과는 0.9423%로 약 4% 상승 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 \tCost: 168.082875335\n",
      "Epoch: 0002 \tCost: 41.620471441\n",
      "Epoch: 0003 \tCost: 26.003158995\n",
      "Epoch: 0004 \tCost: 18.060220715\n",
      "Epoch: 0005 \tCost: 12.970530490\n",
      "Epoch: 0006 \tCost: 9.597306450\n",
      "Epoch: 0007 \tCost: 7.096123880\n",
      "Epoch: 0008 \tCost: 5.451318178\n",
      "Epoch: 0009 \tCost: 3.946004150\n",
      "Epoch: 0010 \tCost: 3.001856147\n",
      "Epoch: 0011 \tCost: 2.205120762\n",
      "Epoch: 0012 \tCost: 1.806419487\n",
      "Epoch: 0013 \tCost: 1.242452548\n",
      "Epoch: 0014 \tCost: 1.038211533\n",
      "Epoch: 0015 \tCost: 0.754097386\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9448\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [1]\n",
      "Prediction:\t [1]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab10-2 : minist by nerual network\n",
    "#           accuracy : 0.9423\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10\n",
    "\n",
    "# Input placeholders, MNIST data image of shape 28 * 28 = 784 pixel\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "# 0 ~ 9 digits recognition = 10 classed\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "# Weights and bias for NN layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, nb_classes]))\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis using softmax\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"\\tCost:\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(hypothesis, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Lab 3: Xavier init neural network for MNIST (Accuracy 0.9730)\n",
    "- 위 Lab2와의 차이점은 아래와 같다.\n",
    "  - Weight의 초기화에 랜덤을 사용하지 않고, Xavier 초기화를 적용하였다.\n",
    "  - Accuracy 결과는 0.9730%로 약 3% 상승 되었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 \tCost: 0.294609584\n",
      "Epoch: 0002 \tCost: 0.113068595\n",
      "Epoch: 0003 \tCost: 0.074517656\n",
      "Epoch: 0004 \tCost: 0.052327855\n",
      "Epoch: 0005 \tCost: 0.038574041\n",
      "Epoch: 0006 \tCost: 0.030784411\n",
      "Epoch: 0007 \tCost: 0.025149835\n",
      "Epoch: 0008 \tCost: 0.019272849\n",
      "Epoch: 0009 \tCost: 0.013632683\n",
      "Epoch: 0010 \tCost: 0.014041129\n",
      "Epoch: 0011 \tCost: 0.014991121\n",
      "Epoch: 0012 \tCost: 0.012427811\n",
      "Epoch: 0013 \tCost: 0.011833470\n",
      "Epoch: 0014 \tCost: 0.007531284\n",
      "Epoch: 0015 \tCost: 0.008567470\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9744\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [9]\n",
      "Prediction:\t [9]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab10-3 : minist by nerual network with xavier\n",
    "#           accuracy : 0.9780\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10\n",
    "\n",
    "# Input placeholders, MNIST data image of shape 28 * 28 = 784 pixel\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "# 0 ~ 9 digits recognition = 10 classed\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "# Weights and bias for NN layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, nb_classes],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis using softmax\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"\\tCost:\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(hypothesis, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Lab 4: Deep neural network for MNIST (Accuracy 0.9785)\n",
    "- 위 Lab3과의 차이점은 아래와 같다.\n",
    "  - 5단의 neural network으로 좀더 deep 하게 구성하였다.\n",
    "  - shape의 출력은 256개에서 512개로 좀더 wide하게 구성하였다.\n",
    "  - Accuracy 결과는 0.978%로 약 0.5% 상승 되었다. 때에 따라서 accuracy가 줄기도 한다. 아마도 overfitting 된것으로 보인다. 이에 따라 lab 5에서 dropout 으로 network을 설계할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 \tCost: 0.301736783\n",
      "Epoch: 0002 \tCost: 0.108016018\n",
      "Epoch: 0003 \tCost: 0.071312178\n",
      "Epoch: 0004 \tCost: 0.053806748\n",
      "Epoch: 0005 \tCost: 0.039286335\n",
      "Epoch: 0006 \tCost: 0.034092403\n",
      "Epoch: 0007 \tCost: 0.030653502\n",
      "Epoch: 0008 \tCost: 0.027925185\n",
      "Epoch: 0009 \tCost: 0.020962704\n",
      "Epoch: 0010 \tCost: 0.020842491\n",
      "Epoch: 0011 \tCost: 0.017203622\n",
      "Epoch: 0012 \tCost: 0.018255131\n",
      "Epoch: 0013 \tCost: 0.016747021\n",
      "Epoch: 0014 \tCost: 0.015837919\n",
      "Epoch: 0015 \tCost: 0.014658532\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9746\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [3]\n",
      "Prediction:\t [3]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab10-4 : minist by nerual network with deep learning\n",
    "#           accuracy : 0.9785\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10\n",
    "\n",
    "# Input placeholders, MNIST data image of shape 28 * 28 = 784 pixel\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "# 0 ~ 9 digits recognition = 10 classed\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "# Weights and bias for NN layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, nb_classes],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis using softmax\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"\\tCost:\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(hypothesis, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Lab 5: Dropout neural network for MNIST (Accuracy 0.9848)\n",
    "- 위 Lab4와의 차이점은 아래와 같다.\n",
    "  - 5단의 neural network에 deropout을 적용하였다.\n",
    "  - Accuracy 결과는 0.9848%로 약 1% 상승 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 \tCost: 0.458700201\n",
      "Epoch: 0002 \tCost: 0.172372331\n",
      "Epoch: 0003 \tCost: 0.129912548\n",
      "Epoch: 0004 \tCost: 0.108884575\n",
      "Epoch: 0005 \tCost: 0.095104513\n",
      "Epoch: 0006 \tCost: 0.080963188\n",
      "Epoch: 0007 \tCost: 0.078037094\n",
      "Epoch: 0008 \tCost: 0.068476953\n",
      "Epoch: 0009 \tCost: 0.065428976\n",
      "Epoch: 0010 \tCost: 0.061137256\n",
      "Epoch: 0011 \tCost: 0.057880940\n",
      "Epoch: 0012 \tCost: 0.054597637\n",
      "Epoch: 0013 \tCost: 0.050098970\n",
      "Epoch: 0014 \tCost: 0.051898345\n",
      "Epoch: 0015 \tCost: 0.045763725\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9801\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [6]\n",
      "Prediction:\t [6]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab10-5 : minist by nerual network with dropout\n",
    "#           accuracy : 0.9848\n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10\n",
    "\n",
    "# dropout (keep_prob) rate 0.7 on training, but should be 1 for testing!!\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Input placeholders, MNIST data image of shape 28 * 28 = 784 pixel\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "# 0 ~ 9 digits recognition = 10 classed\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "# Weights and bias for NN layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, nb_classes],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "# Hypothesis using softmax\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"\\tCost:\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(hypothesis, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Lab 6: High level tensorflow API for MNIST (Accuracy 0.9850)\n",
    "- 위 Lab5와의 차이점은 아래와 같다.\n",
    "  - High level의 tensorflow API를 적용하였고, 소스코드의 기능은 같다.\n",
    "  - Accuracy 결과는 0.9850%로 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "[Epoch:    1]\tCost: 0.38708553\n",
      "[Epoch:    2]\tCost: 0.330159667\n",
      "[Epoch:    3]\tCost: 0.321581603\n",
      "[Epoch:    4]\tCost: 0.316077446\n",
      "[Epoch:    5]\tCost: 0.311775169\n",
      "[Epoch:    6]\tCost: 0.310022716\n",
      "[Epoch:    7]\tCost: 0.307601213\n",
      "[Epoch:    8]\tCost: 0.307955301\n",
      "[Epoch:    9]\tCost: 0.305440181\n",
      "[Epoch:   10]\tCost: 0.303470927\n",
      "[Epoch:   11]\tCost: 0.302448962\n",
      "[Epoch:   12]\tCost: 0.302503021\n",
      "[Epoch:   13]\tCost: 0.302841304\n",
      "[Epoch:   14]\tCost: 0.30146164\n",
      "[Epoch:   15]\tCost: 0.302058428\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9832\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [9]\n",
      "Prediction:\t [9]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab10-6 : minist by nerual network with high level tensorflow api\n",
    "#           accuracy : 0.9850\n",
    "################################################################################\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01 # we can use large learning rate using Batch Normalization\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "nb_classes = 10 # 0 ~ 9 digits recognition = 10 classed\n",
    "\n",
    "# Input placeholders \n",
    "X = tf.placeholder(tf.float32, shape=[None, 784]) # imgage = 28*28 = 784 pixel\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "train_mode = tf.placeholder(tf.bool, name='train_mode')\n",
    "\n",
    "# Layer output size\n",
    "hidden_output_size = 512\n",
    "final_output_size = nb_classes\n",
    "\n",
    "# Weight initializer: xavier\n",
    "xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# Normalizer: batch normalization    \n",
    "bn_params = {\n",
    "    'is_training': train_mode,\n",
    "    'decay': 0.9,\n",
    "    'updates_collections': None\n",
    "}\n",
    "\n",
    "# We can build short code using 'arg_scope' to avoid duplicate code\n",
    "# same function with different arguments\n",
    "with arg_scope([fully_connected],\n",
    "        activation_fn = tf.nn.relu,\n",
    "        weights_initializer = xavier_init,\n",
    "        biases_initializer = None,\n",
    "        normalizer_fn = batch_norm,\n",
    "        normalizer_params = bn_params\n",
    "        ):\n",
    "    hidden_layer1 = fully_connected(X, hidden_output_size, scope=\"h1\")\n",
    "    h1_drop = dropout(hidden_layer1, keep_prob, is_training=train_mode)\n",
    "    hidden_layer2 = fully_connected(h1_drop, hidden_output_size, scope=\"h2\")\n",
    "    h2_drop = dropout(hidden_layer2, keep_prob, is_training=train_mode)\n",
    "    hidden_layer3 = fully_connected(h2_drop, hidden_output_size, scope=\"h3\")\n",
    "    h3_drop = dropout(hidden_layer3, keep_prob, is_training=train_mode)\n",
    "    hidden_layer4 = fully_connected(h3_drop, hidden_output_size, scope=\"h4\")\n",
    "    h4_drop = dropout(hidden_layer4, keep_prob, is_training=train_mode)\n",
    "    hypothesis = fully_connected(h4_drop, final_output_size,\n",
    "            activation_fn=None, scope=\"hypothesis\")\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict_train = {X: batch_xs, Y: batch_ys, train_mode: True}\n",
    "        feed_dict_cost = {X: batch_xs, Y: batch_ys, train_mode: False}\n",
    "        opt = sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "        c = sess.run(cost, feed_dict=feed_dict_cost)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}]\\tCost: {:>.9}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels, train_mode: False}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(hypothesis, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1], train_mode: False}))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
