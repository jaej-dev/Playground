{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 6. Multinomial logistic regression\n",
    "\n",
    "## 1. Understaing multi class classification\n",
    "\n",
    "<img src=\"https://utkuufuk.github.io/2018/06/03/one-vs-all-classification/one-vs-all.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- binary classification은 상단 왼쪽 그림과 같이 두 카테고리를 구분하는 선을 찾는 것이라면, multi class classification은 상단 우측 그림과 같이 여러개의 카테고리를 구분하는 선을 찾는 것이다.\n",
    "\n",
    "<img src=\"https://mrmint.fr/wp-content/uploads/2017/09/one-vs-all-classification.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- multi class classification은 아래 그림과 같이 binary classification을 여러번 거쳐서 구현할 수 있다.\n",
    "  - class 1 or not = Ha(x)\n",
    "  - class 2 or not = Hb(x)\n",
    "  - class 3 or not = Hc(x)\n",
    "  \n",
    "### 2. Multinomial classification  \n",
    "- 3개의 독립된 binary classification으로 구현할수도 있으나, 아래와 같이 행렬을 이용하여 하나의 벡터로 처리할수 있다.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "w_{a1} & w_{a2} & w_{a3} \\\\\n",
    "w_{b1} & w_{a2} & w_{a3} \\\\\n",
    "w_{c1} & w_{a2} & w_{a3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "w_{a1}x_1 + w_{a2}x_2 + w_{a3}x_3 \\\\\n",
    "w_{b1}x_1 + w_{b2}x_2 + w_{b3}x_3 \\\\\n",
    "w_{c1}x_1 + w_{c2}x_2 + w_{c3}x_3 \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{Y_a} & =Ha(x) \\\\\n",
    "\\hat{Y_b} & =Hb(x) \\\\\n",
    "\\hat{Y_c} & =Hc(x) \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "- 각각의 y hat은 실수를 갖는다. 그러므로 0과 1.0 사이의 값으로 처리하기 위해 sigmoid 함수도 각각 필요하다. 이것을 한번에 효율적으로 처리하기 위해 만들어진것이 softmax 함수이다.\n",
    "\n",
    "### 2.1. Hypothesis (Softmax)\n",
    "- softmax 함수는 N개의 입력을 softmax에 넣으면 각 입력에 대한 확률을 출력해준다.\n",
    "- 출력의 확률의 총합은 1.0이다. 즉 각 입력에 대한 sigmoid 함수를 한번에 처리해준다.\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/lvNdl7yg4Pg/maxresdefault.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "- softmax로 출력된 확률을 가지고 One-Hot Encoding(tf.argmax API)을 통하여 하나의 카테고리를 선택하도록 할 수 있다.\n",
    "\n",
    "### 2.2. Cost function (Cross Entropy)\n",
    "\n",
    "- 아래 그림에서 S(Y)는 예측값으로 Y hat을 의미하며, L은 label로 Y 정답을 의미한다.\n",
    "- cost function은 예측값 Y hat(S)과 Y(L)의 차이며, cost 값을 구하기위해 cross entropy를 사용한다.\n",
    "\n",
    "<img src=\"https://irenelizihui.files.wordpress.com/2016/01/mn3.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "  - 아래 함수가 cross enropy 함수이다. D는 Distance, S는 Softmax, L는 Label을 의미한다.\n",
    "\n",
    "\\begin{equation*}\n",
    "D(S, L) = -\\sum_{i} L_i \\log(S_i)\n",
    "\\end{equation*}\n",
    "\n",
    "  - 이 함수가 왜 적합한 cost 함수인지 증명하면 아래 설명과 같다.\n",
    "  - cross entropy 함수는 아래와 같이 다시 표현할수 있다.\n",
    "\n",
    "\\begin{equation*}\n",
    "D(S, L) = -\\sum_{i} L_i \\log(S_i) = \\sum_{i}(Li) \\text{◎} (-\\log(\\hat{Y_i}))\n",
    "\\end{equation*}\n",
    "\n",
    "  - Y hat은 softmax의 출력값으로 0 ~ 1.0의 값을 갖는다.\n",
    "  - -log 함수는 아래와 같은 그래프와 갖는다. Y hat의 값이 1.0에 가까우면 -log(Y hat)은 0에 수렴하고, Y hat의 값이 0에 가까우면 -log(Y hat)은 무한대에 수렴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "z = np.arange(0.01, 1.1, 0.01)\n",
    "gz = -np.log(z)\n",
    "plt.plot(z, gz)\n",
    "\n",
    "plt.xlabel('Y hat')\n",
    "plt.ylabel('-log(Y hat)')\n",
    "plt.title('-log(x) graph')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 설명을 위해 카테고리가 A, B 두가지 label이 있다고 가정하고\n",
    "  - 첫째, Label(실측값)이 B라면, Hypothesis는 A혹은 B라고 예측을 할것이다. 이때의 Cost를 각각 살펴보면...\n",
    "\n",
    "\\begin{equation*}\n",
    "Y = L = \n",
    "\\begin{bmatrix}\n",
    "0 & (A)\\\\\n",
    "1 & (B)\\\\\n",
    "\\end{bmatrix} = B\n",
    "\\end{equation*}\n",
    "\n",
    "  - 예측이 참일 경우 아래와 같이 cost가 0에 수렴한다. \"◎\"은 element wise multiplication (행렬의 요소별 곱셈)을 의미한다.\n",
    "    \n",
    "\\begin{equation*}\n",
    "\\hat Y = \n",
    "\\begin{bmatrix}\n",
    "0 & (A)\\\\\n",
    "1 & (B)\\\\\n",
    "\\end{bmatrix} = B(TRUE),\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix} \\text{◎} - \\log\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}  \\text{◎}\n",
    "\\begin{bmatrix}\n",
    "\\infty\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} => 0\n",
    "\\end{equation*}\n",
    "    \n",
    "  - 예측이 거짓일 경우 아래와 같이 cost는 무한대에 수렴한다.\n",
    "    \n",
    "\\begin{equation*}\n",
    "\\hat Y = \n",
    "\\begin{bmatrix}\n",
    "1 & (A)\\\\\n",
    "0 & (B)\\\\\n",
    "\\end{bmatrix} = A(FALSE),\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix} \\text{◎} - \\log\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}  \\text{◎}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "\\infty\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "\\infty\\\\\n",
    "\\end{bmatrix} => \\infty\n",
    "\\end{equation*}\n",
    "\n",
    "  - 둘째, Label(실측값)이 A라면, Hypothesis는 A혹은 B라고 예측을 할것이다. 이때의 Cost를 각각 살펴보면...\n",
    "\n",
    "\\begin{equation*}\n",
    "Y = L = \n",
    "\\begin{bmatrix}\n",
    "1 & (A)\\\\\n",
    "0 & (B)\\\\\n",
    "\\end{bmatrix} = A\n",
    "\\end{equation*}\n",
    "\n",
    "  - 예측이 참일 경우 아래와 같이 cost가 0에 수렴한다.\n",
    "    \n",
    "\\begin{equation*}\n",
    "\\hat Y = \n",
    "\\begin{bmatrix}\n",
    "1 & (A)\\\\\n",
    "0 & (B)\\\\\n",
    "\\end{bmatrix} = A(TRUE),\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \\text{◎} - \\log\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}  \\text{◎}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "\\infty\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} => 0\n",
    "\\end{equation*}\n",
    "    \n",
    "  - 예측이 거짓일 경우 아래와 같이 cost는 무한대에 수렴한다.\n",
    "    \n",
    "\\begin{equation*}\n",
    "\\hat Y = \n",
    "\\begin{bmatrix}\n",
    "0 & (A)\\\\\n",
    "1 & (B)\\\\\n",
    "\\end{bmatrix} = B(FALSE),\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \\text{◎} - \\log\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}  \\text{◎}\n",
    "\\begin{bmatrix}\n",
    "\\infty\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} = \n",
    "\\sum_{i}\n",
    "\\begin{bmatrix}\n",
    "\\infty\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} => \\infty\n",
    "\\end{equation*}\n",
    "  \n",
    "  - 결과적으로 cost function이 잘 동작함을 확인할수 있다.\n",
    "\n",
    "- Traning set이 있을 경우의 Multinomial regression의 Cost function을 정의하면 아래와 같다.\n",
    "\n",
    "\\begin{equation*}\n",
    "loss =\\frac{1}{m}\\sum_{i}^{m}D(S(WX_i + b), L_i)\n",
    "\\end{equation*}\n",
    "\n",
    "  - loss는 cost\n",
    "  - D는 Distance\n",
    "  - S는 Softmax\n",
    "  - WX+b는 Hypothesis(예측값)\n",
    "  - L은 Label (결과값)\n",
    "\n",
    "\n",
    "### 2.3. Logistic cost VS Cross entropy\n",
    "- Logistic regression의 cost 함수는 아래와 같고\n",
    "\n",
    "\\begin{equation*}\n",
    "c(H(x),y)=-ylog(H(x)) - (1-y)log(1-H(x))\n",
    "\\end{equation*}\n",
    "\n",
    "- Multinomial logistic regression의 cost 함수 (cross entropy)는 아래와 같다.\n",
    "\n",
    "\\begin{equation*}\n",
    "D(S, L) = -\\sum_{i} L_i \\log(S_i)\n",
    "\\end{equation*}\n",
    "\n",
    "- 두 cost 함수는 사실 같은 의미이다. Logistic regression 에서는 두개의 카테고리로 분류를 하기 때문에 cost 함수의 수식이 위와같이 나왔고, Multinomial logistic regression 에서는 여러개의 카테고리를 분류를 하기때문에 위와 같은식이 나왔다. 위 2.2 설명에서 A, B 두개의 카테고리를 예로 증명을 하였기 때문에 사실상 Multinomial logistic regression의 cost 함수를 logistic regression에 적용해도 된다.\n",
    "\n",
    "- Logistic(binary) classification을 행렬로 묶으면 Multinomial classification이 된다.\n",
    "\n",
    "### 2.4. Minimize cost (Gradient descent algorithm)\n",
    "\n",
    "- 최소화를 위한 Cost 함수인 cross entropy의 미분식은 생략되었으며 아래 그림으로 설명을 대신한다.\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/hMLUgM6kTp8/maxresdefault.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "## 3. Lab1: Softmax classifier example\n",
    "- x_data는 4개의 feature와 8개의 instance로 구성된 행렬이다.\n",
    "- y_data는 3개의 label과 8개의 instance로 구성된 행렬인데 one hot encoding을 해야하기 떄문에 세개중 한개의 label에 해당하는 index값에 1로 표기되었다. (byte mask라고 이해하면됨)\n",
    "- Weight은 X가 8개의 instance, 4개의 feature를 가지므로 [8, 4], Y가 8개의 instance, 3개의 label을 가지므로 [8, 3]이 되므로 4강의 행렬 곱셈 설명에 따라 [4, 3]이 된다.\n",
    "  - X ＊ W = Y 에서 [8, 4] ＊ [?, ?] = [8, 3]\n",
    "- logits, hypothesis, cost는 위 2.2. cross entropy에서 설명된 그림대로 tensorflow API를 사용하여 코딩 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8781685\n",
      "5000 0.07611559\n",
      "10000 0.039919432\n",
      "------------------------------------------------------------------\n",
      "case a: [[1.3999233e-06 9.9999857e-01 3.4802923e-08]] \tOne hot label: [1]\n",
      "------------------------------------------------------------------\n",
      "case b:  [[9.9512935e-01 4.7354838e-03 1.3512206e-04]] \tOne hot label: [0]\n",
      "------------------------------------------------------------------\n",
      "case c:  [[8.359102e-16 5.875436e-07 9.999994e-01]] \tOne hot label: [2]\n",
      "------------------------------------------------------------------\n",
      "case tot:  [[1.3999233e-06 9.9999857e-01 3.4802923e-08]\n",
      " [9.9512935e-01 4.7354838e-03 1.3512206e-04]\n",
      " [8.3591019e-16 5.8754358e-07 9.9999940e-01]] \tOne hot label: [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab6-1 : Softmax Classifier\n",
    "# \n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "# [x1, x2, x3, x4], 4 feature x 8 instance matrix\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "\n",
    "# [label0, label1, label2], One-Hot encoding, 3 label x 8 instance matrix\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]          \n",
    "\n",
    "# Placeholder for a tensor that will be always fed\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4]) # 4 features\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3]) # 3 labels\n",
    "nb_classes = 3 # 3 labels, 3 classes\n",
    "\n",
    "# Model parameters\n",
    "# Calculate weight: X*W=Y -> [8, 4] * [?,?] = [8, 3] -> [4, 3]\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss function\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "    # Testing & One-hot encoding\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(\"case a:\", a, \"\\tOne hot label:\", sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(\"case b: \", b, \"\\tOne hot label:\", sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(\"case c: \", c, \"\\tOne hot label:\", sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    tot = sess.run(hypothesis, feed_dict={\n",
    "        X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(\"case tot: \", tot, \"\\tOne hot label:\", sess.run(tf.argmax(tot, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습을 통하여 cost가 0에 수렴되었으므로 학습이 잘된것을 판단할수 있따.\n",
    "\n",
    "- 학습후 학습이 완료된 hypothesis에 임의의 X 데이터를 feed 시키면 softmax를 통과한 확률을 리턴해준다. (왜냐하면 당연히 hypothesis에 softmax함수를 사용했으므로)이를 출력해보면 3 label중 한가지만이 1.0에 가깝고 나뭐지는 0에 가까운 확률값을 볼수 있다. 또한 argmax 함수를 통하여 1.0에 가까운 label의 index 값을 리턴받을수 있다.\n",
    "\n",
    "- 동시에 여러개의 X 값을 feed로 학습된 hypothesis를 기반으로 세션을 실행하여 y hat, 즉 예측 값을 확인해볼수 있다.\n",
    "\n",
    "\n",
    "## 4. Lab2: Fancy softmax classifier example\n",
    "- Lab1에서 cost 함수를 cross entropy를 아래와 같이 조금은 복잡한 연산의 3줄로 구현하였다.\n",
    "  - logits = tf.matmul(X, W) + b\n",
    "  - hypothesis = tf.nn.softmax(logits)\n",
    "  - cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "- 위 cost 함수는 아래 tensorflow API로 fancy하게(?) 대체할 수 있다.\n",
    "  - logits = tf.matmul(X, W) + b\n",
    "  - cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "    labels=Y_one_hot)\n",
    "  - cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "- data-04-zoo.cvs는 동물의 16가지 특징에 따라 동물을 7종으로 분류한 데이터이다.\n",
    "\n",
    "- 7종에 해당하는 Y 데이터는 0~6 값을 갖는데 이것을 가지고 cross entropy 함수에 입력을 시키기 위해서는 one hot encoding 데이터로 변경해야 한다. 그래서 Y_one_hot은 tf.one_hot(Y, nc_blasses)함수를 통하여 생성되었다.\n",
    "\n",
    "- 그러나 tf.one_hot 함수를 통하여 반환된 shape은 rank(차원)이 1차원 늘어난다. 이에 따라 reshape을 하여 차원을 수정해야한다. 그래서 tf.reshape(Y_one_hot, [-1, nb_classes])를 사용하여 처리한것이다. 첫번째 파라미터 Y_one_hot은 입력 데이터이고, 두번째 파라미터는 reshape을 할 출력 데이의 shape을 넣어주어야 한다. [-1, nb_classes]에서 -1은 모든 인스턴스를 의미하고, nb_classes 즉 종의 수를 의미한다.\n",
    "\n",
    "- prediction은 학습된 hypothesis를 통해 예측된 확률을 argmax를 통해 one hot encoding된 동물의 종에 해당하는 index 값을 받는다. tf.argmax(hypothesis, 1)에서 1은 hypothesis에서 1개의 최고값을 고르라는 의미이다.\n",
    "\n",
    "- correct_prediction은 예측된 동물의 종에 해당하는 index값과 Y_one_hot의 argmax 값인 실제 동물의 종의 index 값을 비교하여 참과 거짓 값을 받는다.\n",
    "\n",
    "- accuracy는 correct_prediction 값을 float32로 캐스팅하여 0~1.0을 받고, 평균을 내어 저장한다. 즉 학습의 진행 정도에 따른 정확도를 확인하기 위해 사용되었다.\n",
    "\n",
    "- y_data.flatten()은 [[1], [0]]과 같은 데이터를 [1, 0]으로 shpae을 납작하게 변환시킨다는 의미이다.\n",
    "\n",
    "- for p, y in zip(pred, y_data.flatten())은 pred 리스트와 y_data.flatten() 두가지 리스트를 지퍼처럼 하나로 묶듯이 두 리스트의 요소를 각각 p, y로 반환시키라는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot:\n",
      " Tensor(\"one_hot_6:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape one_hot:\n",
      " Tensor(\"Reshape_6:0\", shape=(?, 7), dtype=float32)\n",
      "Step:     0\tLoss: 3.396\tAcc: 16.83%\n",
      "Step:  1000\tLoss: 0.111\tAcc: 99.01%\n",
      "Step:  2000\tLoss: 0.058\tAcc: 100.00%\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 4 \tOrigin Y: 4\n",
      "[True] \tPrediction: 4 \tOrigin Y: 4\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 4 \tOrigin Y: 4\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 2 \tOrigin Y: 2\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 2 \tOrigin Y: 2\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 2 \tOrigin Y: 2\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 4 \tOrigin Y: 4\n",
      "[True] \tPrediction: 2 \tOrigin Y: 2\n",
      "[True] \tPrediction: 2 \tOrigin Y: 2\n",
      "[True] \tPrediction: 3 \tOrigin Y: 3\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 5 \tOrigin Y: 5\n",
      "[True] \tPrediction: 0 \tOrigin Y: 0\n",
      "[True] \tPrediction: 6 \tOrigin Y: 6\n",
      "[True] \tPrediction: 1 \tOrigin Y: 1\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab6-2 : Fancy Softmax Classifier\n",
    "#          cross_entropy, one_hot, reshape\n",
    "################################################################################\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "X = tf.placeholder(tf.float32, shape=[None, 16])\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])  # 0 ~ 6\n",
    "\n",
    "# One hot\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "print(\"one_hot:\\n\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape one_hot:\\n\", Y_one_hot)\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "    labels=Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Prediction    \n",
    "prediction = tf.argmax(hypothesis, 1) # probabilities to labels, 0 ~ 6\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                X: x_data, Y: y_data})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] \\tPrediction: {} \\tOrigin Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one_shot을 통해 생성된 3차원의 데이터가 reshpae 후 2차원의 데이터로 변경된것을 확인할수 있다.\n",
    "\n",
    "- 학습을 거듭할수록 Cost가 0에 수렴하며, Accuracy가 100% 까지 상승되는 것을 확인할 수 있다.\n",
    "\n",
    "- 학습 종료후 예측을 잘하는지 확인을 하기위해 prediction에 x_data를 주고 세션을 실행시킨 결과 학습시 Accuracy가 100%로 나온것과 같이 모든 x_data에 대해 예측을 정확하게 하는것을 확인할수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
