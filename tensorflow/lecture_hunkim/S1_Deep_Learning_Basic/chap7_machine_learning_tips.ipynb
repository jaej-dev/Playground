{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 7. Machine learning tips\n",
    "\n",
    "## 1. Learning reate\n",
    "\n",
    "- 아래와 같은 Gradient descent algorithm의 수식에서 alpha 값이 learning rate 이다.\n",
    "\n",
    "\\begin{equation*}\n",
    "W := W - \\alpha \\frac{\\partial}{\\partial W}cost(W)\n",
    "\\end{equation*}\n",
    "\n",
    "- Learining rate은 아래와 같은 convex curve에서 임의의 W 점에서 다음 스텝의 W를 찾을때 스텝의 크기를 의미한다.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_learning_rate.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "### 1.1. Large learning rate\n",
    "- Learning rate가 너무 크면 Convex curve에서 상단 왼쪽의 그림과 같이 W가 밖으로 튕겨나가는 overshooting이 발생한다.\n",
    "- 이경우 cost를 출력하면 숫자가 아닌 값이(nan) 출력될 것이다. \n",
    "\n",
    "### 1.2. Small learning rate\n",
    "- Larning rate가 너무 작으면 Convex curve에서 상단 오른쪽의 그림과 같이 W가 global minimum(optimum)을 찾지 못하고 local minimum에서 학습을 종료할수 있다.\n",
    "\n",
    "### 1.3. Try several learning rate\n",
    "- Observe the cost function\n",
    "- Check it goes down in a reasonalble rate\n",
    "- 0.1 ~ 0.001 값을 시작으로 적당한 값을 찾아야 한다.\n",
    "\n",
    "\n",
    "## 2. Data (X) preprocessing\n",
    "- Feature data X의 어떤 데이터가 너무 크거나 너무 작으면 learning rate가 적당하다고 판단되어도 cost 함수가 overshooting 하거나, 학습이 정확하게 일어나지 않을수 있다. 이런 경우 X 데이터를 선처리할 필요가 있다.\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn2/prepro1.jpeg\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "### 2.1. Standardization\n",
    "- 표준정규분포표준화라고 하며 평균을 기준으로 얼마나 떨어져 있는지를 나타내는 값으로 2개 이상의 대상이 단위가 다를때 대상 데이터를 같은 기준으로 보게한다. 표준화를 적용하면 간극이 줄이는 결과를 얻을수 있다.\n",
    "\n",
    "- 아래 식과 같이 요소값 x를 x값들의 평균값(mu)으로 뺀후 x값들의 표준편차(sigma)로 나눠준다.\n",
    "\n",
    "\\begin{equation*}\n",
    "x'_j=\\frac{x_j - \\mu_j}{\\sigma_j}\n",
    "\\end{equation*}\n",
    "\n",
    "- python으로 구현하면 아래와 같다.\n",
    "  - X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\n",
    "\n",
    "\n",
    "### 2.2. Normalization\n",
    "- 정규화는 요소의 전체를 0~100으로 설정한다는 것을 의미한다.\n",
    "\n",
    "- 아래 식과 같이 요소값 x를 x값들의 최소값에서 뺀후 x값들의 최대값과 최소값을 뺀 값으로 나눈다.\n",
    "\n",
    "\\begin{equation*}\n",
    "x'_j=\\frac{x_j - x_{min}}{x_{max} - x_{min}}\n",
    "\\end{equation*}\n",
    "\n",
    "- python으로 구현하면 아래와 같다.\n",
    "  - X_nor[:,0] = (X[:,0] - np.min(X[:,0])) / (np.max(X[:,0]) - np.min(X[:,0]))\n",
    "\n",
    "## 3. Overfitting\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/dBLZg-RqoLg/maxresdefault.jpg\" alt=\"\" title=\"\" />\n",
    "\n",
    "- Our model is very good with training data set (with memorization)\n",
    "- Not good at test dataset or in real use\n",
    "- 학습 데이터에 너무 잘 맞게 학습이 되어 테스트 데이터로 테스트시 예측을 못하는 경우를 overfitting 이라고 한다.\n",
    "\n",
    "### 3.1. Solutions for overfitting\n",
    "- More training data!! \n",
    "- Reduce the number of features\n",
    "- Regularization\n",
    "\n",
    "### 3.2. Regularization\n",
    "- Let's not have too big numbers in the weight\n",
    "- 일반화라고 하며 아래 cost 함수 끝에 lambda로 시작하는 텀을 추가로 더한 것이다.\n",
    "- 각 W 엘리먼트에 대해 제곱한 값을 모두 더한후 lambda라는 regularization strengh라는 상수를 곱하여 cost 함수에 더하게 된다.\n",
    "\n",
    "\\begin{equation*}\n",
    "loss =\\frac{1}{m}\\sum_{i}^{m}D(S(WX_i + b), L_i) + \\lambda \\sum_ \\text{ } W^2\n",
    "\\end{equation*}\n",
    "\n",
    "- Regulaization을 tensorflow로 구현하면 아래와 같다.\n",
    "  - reg_strength = 0.001\n",
    "  - l2reg = reg_strength * tf.reduce_sum(tf.square(W))\n",
    "  - cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "    labels=Y_one_hot)\n",
    "  - cost = tf.reduce_mean(cost_i) + l2reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lab1: Tensor Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Array and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 1.],\n",
      "       [2., 3.],\n",
      "       [4., 5.],\n",
      "       [6., 7.]])\n",
      "t.ndim = 2\n",
      "t.shape = (4, 2)\n",
      "t[0], t[1], t[-1] = [0. 1.] [2. 3.] [6. 7.]\n",
      "t[:3] = [[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]]\n",
      "t[1:-1] = [[2. 3.]\n",
      " [4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# Array and slicing\n",
    "t = np.array([[0., 1.], [2., 3.], [4., 5.], [6., 7.]])\n",
    "\n",
    "pp.pprint(t)\n",
    "print(\"t.ndim =\", t.ndim) # rank\n",
    "print(\"t.shape =\", t.shape) # shape\n",
    "print(\"t[0], t[1], t[-1] =\", t[0], t[1], t[-1])\n",
    "print(\"t[:3] =\", t[:3])\n",
    "print(\"t[1:-1] =\", t[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Shape, Rank, Axis\n",
    "- Rank : [ 의 개수이다.\n",
    "- Shape : [?1, ?2, ?3]로 나타내며, ?3은 가장 안쪽의 [ ] 안에 있는 엘리먼트의 개수 ?2는 [[ ]] 안에 있는 엘리먼트의 개수, ?3은 [[[ ]]]안에 있는 엘리먼트의 개수이다. ?의 개수는 rank의 수와 같다.\n",
    "- Axis는 rank의 수와 같은만큼 존재하는데 가장 바깥쪽의 [가 axis 0이며 안쪽으로 들어갈수록 1씩 증가한다.\n",
    "- 가장 안쪽의 axis는 -1로 표기하는데 가장 많이 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank(count of brackets) = 1, element = 4, Shape = [4]\n",
    "t = tf.constant([1, 2, 3, 4])\n",
    "tf.shape(t).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank = 2, Shape = [2, 4]\n",
    "t = tf.constant([[1, 2, 3, 4],\n",
    "                 [5, 6, 7, 8]])\n",
    "tf.shape(t).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank = 4, Shape = [1, 2, 3, 4]\n",
    "t = tf.constant([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "tf.shape(t).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ # axis = 0 (1st brakets)\n",
    "    [ # axis = 1 (2nd brackets)\n",
    "        [ # axis = 2 (3th brackets)\n",
    "            [1,2,3,4],  # axis = 3 or -1 (4th brackets)\n",
    "            [5,6,7,8],\n",
    "            [9,10,11,12]\n",
    "        ],\n",
    "        [\n",
    "            [13,14,15,16],\n",
    "            [17,18,19,20], \n",
    "            [21,22,23,24]\n",
    "        ]\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Matmul vs Multiply\n",
    "- [x,y][y,z]  = [x,z]\n",
    "- 두 행렬의 shpae을 확인한다.\n",
    "- 행렬의 곱셈이 일어나려면 y 값이 같이야 하며, 행렬의 곱셈이 완료되면 [x,z]의 shape이 된다.\n",
    "- 행렬의 곱셈은 tf.matmul 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrix 1 shape = (2, 2)\n",
      "Metrix 2 shape = (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.],\n",
       "       [11.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 = tf.constant([[1., 2.], [3., 4.]])\n",
    "matrix2 = tf.constant([[1.], [2.]])\n",
    "print(\"Metrix 1 shape =\", matrix1.shape)\n",
    "print(\"Metrix 2 shape =\", matrix2.shape)\n",
    "tf.matmul(matrix1, matrix2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [6., 8.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning: martix multiplication vs general multiplication\n",
    "(matrix1 * matrix2).eval() # See below broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Watch out broadcasting\n",
    "- Shape이 맞지 않는 경우 수학적으로 연산이 안되는데, tensorflow에서는 broadcasting을 시켜 shape을 연산이 되도록 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same shape\n",
    "matrix1 = tf.constant([[3., 3.]]) # Rank = 2, Shape = [1, 2]\n",
    "matrix2 = tf.constant([[2., 2.]]) # Rank = 2, Shape = [1, 2]\n",
    "(matrix1 + matrix2).eval() # Rank = 2, Shape = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different shape case 1\n",
    "matrix1 = tf.constant([[1., 2.]]) # Rank = 2, Shape = [1, 2]\n",
    "matrix2 = tf.constant([3.]) # Rank = 1, Shape = [1]\n",
    "# Broadcasting output = [[3., 3.]]\n",
    "(matrix1 + matrix2).eval() # Rank = 2, Shape [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different shape case 2\n",
    "matrix1 = tf.constant([[1., 2.]]) # Rank = 2, Shape = [1, 2]\n",
    "            # Broadcasting output = [1., 2.]\n",
    "matrix2 = tf.constant([3., 4.]) # Rank = 1, Shape = [2]\n",
    "            # Broadcasting output = [3., 4.]\n",
    "(matrix1 + matrix2).eval() # Rank = 2, Shape = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5.],\n",
       "       [5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different shape case 3\n",
    "matrix1 = tf.constant([[1., 2.]]) # Rank = 2, Shape = [1, 2]\n",
    "# Broadcasting output = [1., 2.], \n",
    "#                       [1., 2.]]\n",
    "matrix2 = tf.constant([[3.], [4.]]) # Rank = 2, Shape = [2, 1]\n",
    "# Broadcasting output = [[3., 3.], \n",
    "#                        [4., 4.]]\n",
    "(matrix1 + matrix2).eval() # Rank = 2, Shape = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Reduce Mean and Reduce Sum\n",
    "- reduce_mean은 axis값에 따라 행렬의 평균을 계산하는 함수이다.\n",
    "- redume_sum은 axis값에 따라 행렬의 합을 계산하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 1\n",
    "tf.reduce_mean([1, 2], axis = 0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 2\n",
    "tf.reduce_mean([1., 2.], axis = 0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 3\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_mean(x, axis = 0).eval() # (1+3)/2, (2+4)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 4\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_mean(x, axis = 1).eval() # (1+2)/2, (3+4)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 5\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_mean(x, axis = -1).eval() # same case 4 (max axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 6\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_mean(x).eval() # ( ((1+2)/2) + (3+4)/2 ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 6.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 7\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_sum(x, axis = 0).eval() # (1+3), (2+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 7.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 8\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_sum(x, axis = -1).eval() # (1+2), (3+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 9\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_sum(x).eval() # (1+2) + (3 + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 10\n",
    "x = [[1., 2.],\n",
    "     [3., 4.]]\n",
    "tf.reduce_mean(tf.reduce_sum(x, axis = -1)).eval() # case 8 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Argmax\n",
    "- axis에 따라 행렬에서 가장 큰수에 해당하는 index 값을 찾아주는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 1\n",
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "tf.argmax(x, axis = 0).eval() # return array index, not return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 2\n",
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "tf.argmax(x, axis = 1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 3\n",
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "tf.argmax(x, axis = -1).eval() # same axis 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Reshape\n",
    "- 일반적으로 machine learning에서 traning data는 수정을 하지 않으므로, 가장 안쪽의 엘리먼트는 수정을 하지 않는것이 일반적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Origin\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape 1\n",
    "tf.reshape(t, shape=[-1, 3]).eval() # Do not reshape last element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2]],\n",
       "\n",
       "       [[ 3,  4,  5]],\n",
       "\n",
       "       [[ 6,  7,  8]],\n",
       "\n",
       "       [[ 9, 10, 11]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape 2\n",
    "tf.reshape(t, shape=[-1, 1, 3]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2],\n",
       "        [ 3,  4,  5]],\n",
       "\n",
       "       [[ 6,  7,  8],\n",
       "        [ 9, 10, 11]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape 3\n",
    "tf.reshape(t, shape=[-1, 2, 3]).eval() # same shape[2, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Reshape (Squeeze and Expand)\n",
    "- squeeze는 다차원의 행렬을 1차원으로 변경해준다.\n",
    "- expand_dims는 주어진 행렬의 차원을 다차원의 행렬로 변경해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squeeze\n",
    "tf.squeeze([[0], [1], [2]]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand 1\n",
    "tf.expand_dims([0, 1, 2], 0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2]], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand 2\n",
    "tf.expand_dims([0, 1, 2], 1).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9. One hot\n",
    "- one_hot은 각 엘리먼트의 수에 해당하는 자리만 hot하게 1로 만들어 준다. depth는 엘리먼트의 종류의 수이다. 그래야 자리로 구분할수 있기 때문이다.\n",
    "- case 1에서 엘리먼트는 0, 1, 2, 3 즉 4종류의 수로 이루어진 행렬이므로 depth는 4가 되며, 0에 해당하는 byte masking은 [[1.0, 0, 0, 0]]이 된다.\n",
    "- one_hot 함수의 결과는 항상 1차원이 늘어나기 때문에 필요시 reshape을 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 1\n",
    "tf.one_hot([[0], [1], [2], [3]], depth=4).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 2\n",
    "t = tf.one_hot([[0], [3], [1], [2]], depth=4).eval()\n",
    "tf.reshape(t, shape=[-1, 4]).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10. Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 1\n",
    "tf.cast([1.1, 2.2, 3.3, 4.9], tf.int32).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 2\n",
    "tf.cast([True, False, 1 == 1, 0 == 1], tf.int32).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11. Stack\n",
    "- axis에 따라 행렬을 쌓을수 있게 해주는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 1\n",
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "tf.stack([x, y, z]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 2\n",
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "tf.stack([x, y, z], axis = 0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 7],\n",
       "       [2, 5, 8],\n",
       "       [3, 6, 9]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 3\n",
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "tf.stack([x, y, z], axis = 1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 7],\n",
       "       [2, 5, 8],\n",
       "       [3, 6, 9]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 4\n",
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "tf.stack([x, y, z], axis = -1).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12. Ones and Zeros like\n",
    "- 주어진 행렬(tensor)의 모든 엘리먼트를 0또는 1로 채울수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 1\n",
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "tf.ones_like(x).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case 2\n",
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "tf.zeros_like(x).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.13. Zip\n",
    "- 두대 이상의 행렬에서 엘리먼트를 순서에 대로 묶에서 리턴해줄때 사용하는 함수이다.\n",
    "- 일반적으로 for loop에서 많이 사용한다.\n",
    "\n",
    "<img src=\"http://blog.londasfiles.com/wp-content/uploads/2017/10/zipper.jpg\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1 \ty = 4\n",
      "x = 2 \ty = 5\n",
      "x = 3 \ty = 6\n"
     ]
    }
   ],
   "source": [
    "# case 1\n",
    "for x, y in zip([1, 2, 3], [4, 5, 6]):\n",
    "    print(\"x =\", x, \"\\ty =\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1 \ty = 4 \tz = 7\n",
      "x = 2 \ty = 5 \tz = 8\n",
      "x = 3 \ty = 6 \tz = 9\n"
     ]
    }
   ],
   "source": [
    "# case 2\n",
    "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
    "    print(\"x =\", x, \"\\ty =\", y, \"\\tz =\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lab2: Learning rate and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.6709914 [[ 0.87485987  1.5441668  -0.2560822 ]\n",
      " [ 0.75910157  0.58953494 -0.02900083]\n",
      " [ 0.1535153   0.86252123 -0.5329474 ]]\n",
      "500 0.46780825 [[-1.3670235   1.0338025   2.4961648 ]\n",
      " [ 0.59331566  0.4376106   0.28871036]\n",
      " [ 0.9197705   0.3353759  -0.7720563 ]]\n",
      "1000 0.3669179 [[-2.5297198   1.2339215   3.4587445 ]\n",
      " [ 0.58751327  0.46741167  0.26471278]\n",
      " [ 1.3657718   0.26661664 -1.1492964 ]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab7-1 : Learning rate and evalutation\n",
    "#          \n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Predicting animal type based on various features\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax cmputes softmax activatins\n",
    "# softmax = exp(logits) / reduce_mean(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "#    learning_rate = 1e-10).minimize(cost)   # case1 1e-10\n",
    "#    learning_rate = 10.0).minimize(cost)   # case2 10.0\n",
    "    learning_rate = 0.1).minimize(cost)   # case3 0.1\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1001):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={x: x_data, y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "\n",
    "    # Pridict\n",
    "    print(\"Prediction: \", sess.run(prediction, feed_dict={x: x_test}))\n",
    "\n",
    "    # Calcuate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={x: x_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning_rate = 1e-10).minimize(cost) 일 경우의 결과는 아래와 같다.\n",
    "  - Prediction:  [1 1 1]\n",
    "  - Accuracy:  0.0\n",
    "- learning_rate = 10.0).minimize(cost) 일 경우의 결과는 아래와 같다.\n",
    "  - Prediction:  [0 0 0]\n",
    "  - Accuracy:  0.0\n",
    "- learning_rate = 0.1).minimize(cost) 일 경우의 결과는 아래와 같이 정상적인 학습이 일어난다.\n",
    "  - Prediction:  [2 2 2]\n",
    "  - Accuracy:  1.0\n",
    "  \n",
    "  \n",
    "## 6. Lab3: Data preprocessing (without minmax scale)\n",
    "- x data에 굉장히 큰값이 포함되었고, 이에대한 전처리 함수 MinMaxScaler를 호출하지 않았을 경우의 예를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  1125396600000.0 \n",
      "Prediction:\n",
      " [[ -747507.7 ]\n",
      " [-1505578.2 ]\n",
      " [-1184227.  ]\n",
      " [ -829917.75]\n",
      " [ -978230.9 ]\n",
      " [ -986472.44]\n",
      " [ -904079.8 ]\n",
      " [-1151281.1 ]]\n",
      "2000 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab7-2 : Linear regression without min max sclae\n",
    "#          \n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# MinMaxScaler function\n",
    "'''\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # Noiser term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "'''\n",
    "\n",
    "# Traning data\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "    \n",
    "# Very important, it does not work without it.\n",
    "'''\n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)\n",
    "'''\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Placeholders for tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1], name='weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 2000 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost 값이 nan이 되어, convex curve 외부로 W가 튕긴것을 알수 있다. 그래서 예측 또한 이루어지지 않았다.\n",
    "\n",
    "\n",
    "## 7. Lab4: Data preprocessing (with minmax scale)\n",
    "- x data에 굉장히 큰값이 포함되었고, 이에대한 전처리 함수 MinMaxScaler를 호출하였을 경우의 예를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  0.11489186 \n",
      "Prediction:\n",
      " [[ 1.5295815 ]\n",
      " [ 1.3669645 ]\n",
      " [ 0.962455  ]\n",
      " [ 0.4370768 ]\n",
      " [ 0.8395956 ]\n",
      " [ 0.77908486]\n",
      " [ 0.11631776]\n",
      " [-0.09806752]]\n",
      "2000 Cost:  0.10301289 \n",
      "Prediction:\n",
      " [[ 1.5001452 ]\n",
      " [ 1.3393453 ]\n",
      " [ 0.93978035]\n",
      " [ 0.419984  ]\n",
      " [ 0.81888986]\n",
      " [ 0.7591939 ]\n",
      " [ 0.10408911]\n",
      " [-0.1098053 ]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab7-3 : Linear regression with min max sclae\n",
    "#          \n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# MinMaxScaler function\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # Noiser term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "# Traning data\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "    \n",
    "# Very important, it dones not work without it.\n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Placeholders for tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1], name='weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 2000 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost 값이 0에 수렴하며 예측이 잘되고 있음을 확인할 수 있다.\n",
    "\n",
    "\n",
    "## 8. Lab5: MNIST introduction\n",
    "- MNIST dataset은 0~9의 숫자를 사람의 손으로 쓴 데이터로서 training set과 test set으로 구성되어 있다. \n",
    "\n",
    "- http://yann.lecun.com/exdb/mnist/ 에서 데이터를 받을수 있다.\n",
    "\n",
    "- tensorflow에는 input_data.py 파일에서 MNIST 파일을 웹에서 다운로드하여 처리하고 있다.\n",
    "\n",
    "- 한개의 숫자는 28 x 28 x 1(256 gray color) = 784 byte 크기의 행렬 데이터이다.\n",
    "\n",
    "- nb_classes는 0~9의 숫자이므로 10이 된다.\n",
    "\n",
    "- epoch 은 전체 데이터를 학습했을 때 1 epoch 이라 한다.\n",
    "\n",
    "- batch size 는 전체 데이터가 너무 방대할때 한번에 몇개의 데이터를 읽을 것인지를 나타내는 수이다.\n",
    "\n",
    "- iteration 은 전체 데이터에서 batch size를 나눈 값이다.\n",
    "\n",
    "- 만약 전체 데이터가 1000, epoch이 10, batch_size가 500이라고 가정하면, iterations은 2가 된다. 정리하면 한번 학습에 500개의 데이터를 가지고 2회 반복 학습하여 1epoch의 학습 완료하고, epochs가 10회 였으므로 전체 데이터를 10번을 반복 학습한다는 의미이다.\n",
    "\n",
    "- 아래 예제에서 num_examples은 55,000, epoch이 15, batch size가 100이므로 iteration에 해당하는 total_batch는 55,000/100 = 550이 된다.\n",
    "  - 안쪽 반복문은 100개의 데이터를 550번 읽어서 전체 데이터를 1회(epoch) 학습한다.\n",
    "  - 바깥쪽 반복문은 15회(epochs) 학습한다.\n",
    "\n",
    "- accuracy.eval(session=sess, ...) 소스코드는 세션을 실행시키는 또다른 방법이다. 이전에는  sess.run()을 사용하였으나, tensor 이름에 .eval()을 붙여서 사용할수 있다.\n",
    "\n",
    "- 학습 완료후 예측하는 부분의 소스코드가 있는데 이것은 전체 test 데이터의 갯수 중에서 하나를 난수로 선택해서 image와 label을 1개만 가져온다. [r:r+1]은 r에서부터 r+1 이전까지의 범위를 나타낸다는 의미인데 결국 r번째의 image와 label 하나를 가르키게 된다. 그리고 나서 matplotlib 라이브러리를 통하여 r번째 이미지와 Label 그리고 예측값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 \tCost =  3.069530771\n",
      "Epoch: 0002 \tCost =  1.117204865\n",
      "Epoch: 0003 \tCost =  0.885250891\n",
      "Epoch: 0004 \tCost =  0.772756116\n",
      "Epoch: 0005 \tCost =  0.701217290\n",
      "Epoch: 0006 \tCost =  0.650942425\n",
      "Epoch: 0007 \tCost =  0.613162382\n",
      "Epoch: 0008 \tCost =  0.583311871\n",
      "Epoch: 0009 \tCost =  0.558597704\n",
      "Epoch: 0010 \tCost =  0.538168136\n",
      "Epoch: 0011 \tCost =  0.520633296\n",
      "Epoch: 0012 \tCost =  0.505402260\n",
      "Epoch: 0013 \tCost =  0.491845261\n",
      "Epoch: 0014 \tCost =  0.480359333\n",
      "Epoch: 0015 \tCost =  0.469307867\n",
      "Learning finished...\n",
      "------------------------------------------------------------------\n",
      "Accuracy: 0.8874\n",
      "------------------------------------------------------------------\n",
      "Label: [3]\n",
      "Prediction: [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab7-4 : minist introduction\n",
    "#          \n",
    "################################################################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10 # 0 ~ 9\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784 pixel\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "# 0 ~ 9 digits recognition = 10 classed\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis using softmax\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# Graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0;\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print(\"Epoch:\", \"%04d\" % (epoch + 1),\n",
    "              \"\\tCost = \", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished...\")\n",
    "\n",
    "    # Test the model using test data sets\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"Accuracy:\", accuracy.eval(session=sess, feed_dict={\n",
    "            X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"Label:\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction:\", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    # Show image\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 epoch에 따라 cost가 0에 수렴하는 것을 확인할 수 있다.\n",
    "- 매우 간단한 모델임에도 정확도가 88%가 되는 것을 알수 있다.\n",
    "- 최종적으로 test image를 랜덤하게 하나 골라서 예측을 해본결과 3이라고 정확하게 예측한것을 확인할수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
