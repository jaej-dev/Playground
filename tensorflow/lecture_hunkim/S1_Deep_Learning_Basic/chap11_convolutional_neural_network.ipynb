{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 11. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture\n",
    "- 아래 그림은 Convolutaional neural network 의 동작 매커니즘 전체를 나타낸 것이다.\n",
    "\n",
    "- Hidden layers가 아래 그림과 동일한 network으로 구성될 필요는 없다. 일반적인 예인다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*NQQiyYqJJj4PSYAeWvxutg.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "### 1.1. Convolution + ReLU Layer \n",
    "\n",
    "- 처음 자동차 이미지 벡터가 32x32x3(RGB color) 이라면, Convolution은 일정 크기로 잘라서 필터(혹은 커널이라 부른다) 처리를 한다.\n",
    "\n",
    "- 예를들어 5x5x3 filter로 정의하여 한개의 값을 생성한다. one number = Wx + b\n",
    "\n",
    "- 한개의 값은 다시 ReLU activation layer에서 처리를 하게된다. one number = ReLU(Wx + b)\n",
    "\n",
    "- 위에서 정의한 동일한 필터를 아래 그림과 같이 전체 이미지벡터에 대해 stride의 크기 만큼 이동 시키면서 one number를 생성해 나가게 된다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif\" alt=\"\" title=\"\" />\n",
    "\n",
    "- N x N의 이미지 벡터가 있고, 이때 F x F 필터를 정의 했다면 one number의 output size는 아래와 같이 될것이다. output size는 반드시 자연수가 되도록 정의 해야한다.\n",
    "\n",
    "\\begin{equation*}\n",
    "Output size = \\frac{(N - F)}{Stride} + 1\n",
    "\\end{equation*}\n",
    "\n",
    "- 그런데 위 과정을 거치게 되면 원본 이미지에 대한 정보를 점점 잃어버리는 현상이 발생한다. 이에 따라 원본 이미지를 잃어버리지 않게 하기 위해 zero padding을 시켜주게 된다. 또한 zero padding은 원본 이미지 벡터의 모서리에 대한 정보도 알수있게 해준다.\n",
    "  - 예를 들어 7x7 원본 이미지에, 3x3 필터, stride가 1, zero padding을 1 만큼 했다면\n",
    "  - N은 7x7에서 zero padding에 따라 9x9가 되며 output size는 아래와 같이 7이 되어 7x7의 출력이 나온다.\n",
    "  - 결국 하기 그림과 같이 원본 이미지의 크기 7x7과 동일한 7x7의 결과를 얻게되어 원본 이미지의 정보를 잃지 않게 된다.\n",
    "  \n",
    "  - zero padding을 하게되면 결론적으로 stride로 나눈만큼의 출력 shape을 얻게된다. \n",
    "\n",
    "\\begin{equation*}\n",
    "Output size = \\frac{(N - F)}{Stride}+1 = \\frac{(9 - 3)}{1} + 1 = \\frac{6}{2} + 1 = 7\n",
    "\\end{equation*}\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/0*iqNdZWyNeCr5tCkc.\" alt=\"\" title=\"\" />\n",
    "\n",
    "- 위 까지의 설명은 1개의 필터를 통해서 one feature를 얻었는데, 필터를 여러개를 사용하게 되면 아래 그림과 같이 all feature maps 얻을수 있으며 depth는 필터의 수와 같다.\n",
    "\n",
    "<img src=\"https://ds055uzetaobb.cloudfront.net/image_optimizer/3097f525bea665618d2bdf1f11f5971d67d5490a.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- 이런 all feature maps를 여러개를 생성할수 있는데 convolution layer에서는 이런 과정을 수행한다.\n",
    "\n",
    "\n",
    "### 1.2. Pooling layer\n",
    "- Pooling layer는 아래 그림과 같이 Convolution layer에서 feature map 한개씩 pooling(또는 sampling)을 하여 resizing을 하여 다시 모으는 역할을 한다.\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0e39e519471cc41b232381bd529542e2c02f21fa/1-Figure1-1.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- 아래 그림은 Max pooling의 동작 원리를 보여주고 있는데 filter를 2x2, stride를 2로 설정 했다면 filter 영역 내에서 가장 큰 값을 찾아 2x2 크기로 출력하게된다.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*vbfPq-HvBCkAcZhiSTZybg.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "- Max pooing을 가장 많이 사용하지만 그밖에 평균값 또는 최소값 등을 pooling layer에 사용할수 있다.\n",
    "\n",
    "\n",
    "### 1.3. Fully connected layer\n",
    "- Hidden layer의 최종 출력이 Fully connected layer의 입력 X가 된다.\n",
    "- Fully connected layer는 그동안에 다루었던 Hypothesis, Cost, Minimize를 통해 학습 과정을 실행하게된다.\n",
    "\n",
    "\n",
    "## 2. Convolutional neural network case study\n",
    "\n",
    "### 2.1. LeNet-5 (LeCun et al., 1998)\n",
    "- Architecture : CONV1 - POOL1 - CONV2 - POOL2 - FC\n",
    "- CONV Filter : 5x5, Stride: 1\n",
    "- POOL Filter : 2x2, Stride: 2\n",
    "\n",
    "<img src=\"https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/lenet-5-825x285.png?x31195\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "### 2.2. AlexNet (Krizhevsky et al. 2012)\n",
    "- Architecture : CONV1 - MAXPOOL1 - NORM1 - CONV2 - MAXPOOL2 - NORM2 - CONV3 - CONV4 - CONV5 - MAXPOOL3 - FC6 - FC7 - FC8 (with 7 CNN ensemble)\n",
    "- CONV Filter : 11x11x3 Stride: 4\n",
    "- POOL Filter : 3x3, Stride: 2\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/314283258/figure/fig1/AS:469483564343296@1488945012114/AlexNet-convolutional-neural-network-25.ppm\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "### 2.3. GoogLeNet (Szegedy et al., 2014)\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/08/08131838/temp9.png\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "### 2.4. ResNet (He et al., 2015)\n",
    "- Layer depth : 152 layers\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-681b59a575398a29837549ba8d89d381\" alt=\"\" title=\"\" />\n",
    "\n",
    "\n",
    "### 2.5. CNN for Sentence Classification (Yoon Kim, 2014)\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7529838/34460821-5e3542f4-ee5d-11e7-93d4-f8ce81984b89.png\" alt=\"\" title=\"\" />\n",
    "\n",
    " \n",
    "### 2.6. AlphaGo (2017)\n",
    "- Input image size = 19x19x48\n",
    "- CONV1 : 192 Filter 5x5 , stride 1, pad 2 = 19x19x192\n",
    "- CONV2~CONV12 : 192 Filter 3x3, stride 1, padd 1 = 19x19x192\n",
    "- CONV : 1 Filter 1x1, stride 1, pad 0 = 19x19\n",
    "\n",
    "<img src=\"https://cdn.thenewstack.io/media/2016/01/alphago-2.png\" alt=\"\" title=\"\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lab for Improve Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Lab1: Simple Convolution layer\n",
    "\n",
    "#### 3.1.1. Make an image 3 x 3 x 1(1byte, 256 gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f28ba7e6ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                  [[4],[5],[6]],\n",
    "                  [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "\n",
    "# Visualization\n",
    "plt.imshow(image.reshape(3,3), cmap='Greys')\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- image shape (1,3,3,1) 에서 앞의 1은 n개 즉 1개의 이미지를 의미한다.\n",
    "- 상단 백색부터 이미지 백터 1,2,3,... 의 값을 시각화 한것이다.\n",
    "\n",
    "#### 3.1.2. Convolution layer without zero padding\n",
    "\n",
    "- output size = ((N-F)/stride)+1 = ((3-2)/1)+1 = 2가 되어 2x2의 출력이 만들어 진다.\n",
    "\n",
    "- logits은 WX+b의 형태를 가진다. 필터의 weight이 모두 1이고, stride가 1이므로 1+2+4+5 = 12, 2+3+5+6=16, 4+5+7+8=24, 5+6+8+9=28 순으로 출력이 만들어 진다.\n",
    "\n",
    "- Convolution layer는 tf.nn.conv2d 함수하나로 모두 처리된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (1, 3, 3, 1)\n",
      "weight.shape: (2, 2, 1, 1)\n",
      "conv2d_img.shape: (1, 2, 2, 1)\n",
      "[[12. 16.]\n",
      " [24. 28.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"image.shape:\", image.shape)\n",
    "weight = tf.constant([[[[1.]], [[1.]]],\n",
    "                      [[[1.]], [[1.]]]])\n",
    "print(\"weight.shape:\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding='VALID')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape:\", conv2d_img.shape)\n",
    "\n",
    "# Visualization\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(2, 2))\n",
    "    plt.subplot(1, 2, i + 1), plt.imshow(one_img.reshape(2, 2), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Convolution layer with zero padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv2d 함수에서 padding 을 'SAME'으로 설정할 경우 원본 이미지와 같이 크기의 출력을 만들도록 zero padding 처리를 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (1, 3, 3, 1)\n",
      "weight.shape: (2, 2, 1, 1)\n",
      "conv2d_img.shape: (1, 3, 3, 1)\n",
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"image.shape:\", image.shape)\n",
    "weight = tf.constant([[[[1.]], [[1.]]],\n",
    "                      [[[1.]], [[1.]]]])\n",
    "print(\"weight.shape:\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding='SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape:\", conv2d_img.shape)\n",
    "\n",
    "# Visuzlization\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3, 3))\n",
    "    plt.subplot(1, 2, i + 1), plt.imshow(one_img.reshape(3, 3), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4. Three filters (2x2x1x3)\n",
    "- 3가지 필터를 사용하고 싶을 경우, 필터의 weight만 변경해주면 된다.\n",
    "\n",
    "- output size는 padding이 SAME이고 stride가 1이므로 원본 이미지와 동일한 3x3의 출력이 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (1, 3, 3, 1)\n",
      "weight.shape: (2, 2, 1, 3)\n",
      "conv2d_img.shape: (1, 3, 3, 3)\n",
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n",
      "[[120. 160.  90.]\n",
      " [240. 280. 150.]\n",
      " [150. 170.  90.]]\n",
      "[[-12. -16.  -9.]\n",
      " [-24. -28. -15.]\n",
      " [-15. -17.  -9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.,10.,-1.]],[[1.,10.,-1.]]],\n",
    "                      [[[1.,10.,-1.]],[[1.,10.,-1.]]]])\n",
    "print(\"weight.shape:\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape:\", conv2d_img.shape)\n",
    "\n",
    "# Visualization\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5. Max Pooling\n",
    "- Pooling layer는 convolution layer의 출력을 받아 resizing을 하는 과정이다.\n",
    "\n",
    "- tf.nn.max_pool 함수를 사용하여 처리 한다. 커널사이즈를 만큼 stride 시키며 최대값을 찾아  resizing을 실현한다.\n",
    "\n",
    "- 아래 소스에서는 2x2 이미지를 커널 사이즈 2x2, stride 1, zero padding을 SAME으로 처리하였기 때문에 원본 이미지와 같은 출력이 나오게된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape =  (1, 2, 2, 1)\n",
      "pool.shape =  (1, 2, 2, 1)\n",
      "pool.eval = \n",
      " [[[[4.]\n",
      "   [3.]]\n",
      "\n",
      "  [[2.]\n",
      "   [1.]]]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[[[4],[3]],\n",
    "                    [[2],[1]]]], dtype=np.float32)\n",
    "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 1, 1, 1], padding='SAME')\n",
    "print(\"image.shape = \", image.shape)\n",
    "print(\"pool.shape = \", pool.shape)\n",
    "print(\"pool.eval = \\n\", pool.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5. MNIST image loading\n",
    "\n",
    "- MNIST 첫번째 데이터를 불러와서 출력을 해보면 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4d6c28bf28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data                      \n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Display image\n",
    "img = mnist.train.images[0].reshape(28,28)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.6. MNIST Convolution layer with 5 filters\n",
    "- 위 이미지를 3x3x1x5 필터를 통해 convolution layer를 만들어 이를 시각화하면 아래와 같다.\n",
    "\n",
    "- 3x3은 필터 사이즈, 1은 256 gray color, 5는 필터의 종류를 의미한다.\n",
    "\n",
    "- stride 는 2x2로 하였고, padding 은 SAME으로 원본 이미지와 크기를 같게 하였으므로 출력의 shape은 14x14가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(1, 14, 14, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "img = img.reshape(-1,28,28,1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 5], stddev=0.01))\n",
    "conv2d = tf.nn.conv2d(img, W1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "print(conv2d)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d_img = conv2d.eval()\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(14,14), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.7. MNIST Max pooling\n",
    "- 커널 사이즈를 2x2, stride를 2x2,로 padding은 SAME으로 하였으므로 resizing 되었다.\n",
    "\n",
    "- Convolution layer의 shape이 14x14였으나, stride가 2기 때문에 출력의 shpae은 7x7이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(1, 7, 7, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pool = tf.nn.max_pool(conv2d, ksize=[1, 2, 2, 1], strides=[\n",
    "                        1, 2, 2, 1], padding='SAME')\n",
    "print(pool)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pool_img = pool.eval()\n",
    "pool_img = np.swapaxes(pool_img, 0, 3)\n",
    "for i, one_img in enumerate(pool_img):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(7, 7), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Lab2: CNN basic for MNIST (Accuracy : 0.9877)\n",
    "- NN : Input layer - CONV1 - POOL1 - CONV2 - POOL2 - FC - Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime...\n",
      "[Epoch:    1]\tCost: 0.379597128\n",
      "[Epoch:    2]\tCost: 0.101766317\n",
      "[Epoch:    3]\tCost: 0.0761249596\n",
      "[Epoch:    4]\tCost: 0.062085432\n",
      "[Epoch:    5]\tCost: 0.0518382422\n",
      "[Epoch:    6]\tCost: 0.044835228\n",
      "[Epoch:    7]\tCost: 0.0414421321\n",
      "[Epoch:    8]\tCost: 0.0364384824\n",
      "[Epoch:    9]\tCost: 0.0314655367\n",
      "[Epoch:   10]\tCost: 0.0288160969\n",
      "[Epoch:   11]\tCost: 0.0244606332\n",
      "[Epoch:   12]\tCost: 0.0233854826\n",
      "[Epoch:   13]\tCost: 0.020476038\n",
      "[Epoch:   14]\tCost: 0.0180840186\n",
      "[Epoch:   15]\tCost: 0.0162545285\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.987\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [1]\n",
      "Prediction:\t [1]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab11-2 : minist by convolution nerual network\n",
    "#           accuracy : 0.9877\n",
    "################################################################################\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # we can use large learning rate using Batch Normalization\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10 # 0 ~ 9 digits recognition = 10 classed\n",
    "\n",
    "# Input placeholders \n",
    "X = tf.placeholder(tf.float32, shape=[None, 784]) # imgage = 28x28 = 784 pixel\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1]) # all images x 28x28 x 1(256 gray)\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "# L1 image input shape -> (?, 28, 28, 1)\n",
    "# Filter1 shape design -> (3x3 x 1(256gray) x 32EA filters)\n",
    "# Convolution shape    -> (?, 28, 28, 32) because padding set to 'SAME'\n",
    "# Poolig shape         -> (?, 14, 14, 32) because pooling kernel size set to 2x2\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "# L2 image input shape -> (?, 14, 14, 32)\n",
    "# Filter2 shape design -> (3x3 x 32(32channel) x 64EA filters)\n",
    "# Convolution shape    -> (?, 14, 14, 64) because padding set to 'SAME'\n",
    "# Poolig shape         -> (?, 7, 7, 64) because pooling kernel size set to 7x7\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "        padding='SAME')\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64]) # for fully connected layer!!\n",
    "\n",
    "# Final FC 7x7x64 inputs to 10(nb_classes)\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, nb_classes],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "print(\"Learning started. It takes sometime...\")\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}]\\tCost: {:>.9}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(logits, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Lab3: CNN deep for MNIST (Accuracy: 0.9931)\n",
    "- NN : Input layer - CONV1 - POOL1 - CONV2 - POOL2 - CONV3 - POOL3 - FC1 - FC2 - Output layer\n",
    "- CONV1 부터 FC1 까지는 Dropout을 적용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime...\n",
      "[Epoch:    1]\tCost: 0.416150304\n",
      "[Epoch:    2]\tCost: 0.0926853217\n",
      "[Epoch:    3]\tCost: 0.0682261674\n",
      "[Epoch:    4]\tCost: 0.0537698725\n",
      "[Epoch:    5]\tCost: 0.0506835632\n",
      "[Epoch:    6]\tCost: 0.0441732411\n",
      "[Epoch:    7]\tCost: 0.040319178\n",
      "[Epoch:    8]\tCost: 0.0370661633\n",
      "[Epoch:    9]\tCost: 0.0338365095\n",
      "[Epoch:   10]\tCost: 0.0351766979\n",
      "[Epoch:   11]\tCost: 0.0298522976\n",
      "[Epoch:   12]\tCost: 0.027705881\n",
      "[Epoch:   13]\tCost: 0.02742508\n",
      "[Epoch:   14]\tCost: 0.0257915536\n",
      "[Epoch:   15]\tCost: 0.0252825001\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9937\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [3]\n",
      "Prediction:\t [3]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab11-3 : minist by deep learning convolution nerual network\n",
    "#           accuracy : 0.9931\n",
    "################################################################################\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # we can use large learning rate using Batch Normalization\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10 # 0 ~ 9 digits recognition = 10 classed\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Input placeholders \n",
    "X = tf.placeholder(tf.float32, shape=[None, 784]) # imgage = 28x28 = 784 pixel\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1]) # all images x 28x28 x 1(256 gray)\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "# L1 image input shape -> (?, 28, 28, 1)\n",
    "# Filter1 shape design -> (3x3 x 1(256gray) x 32EA filters)\n",
    "# Convolution shape    -> (?, 28, 28, 32) because padding set to 'SAME'\n",
    "# Poolig shape         -> (?, 14, 14, 32) because pooling kernel size set to 2x2\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "        padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "# L2 image input shape -> (?, 14, 14, 32)\n",
    "# Filter2 shape design -> (3x3 x 32(channel) x 64EA filters)\n",
    "# Convolution shape    -> (?, 14, 14, 64) because padding set to 'SAME'\n",
    "# Poolig shape         -> (?, 7, 7, 64) because pooling kernel size set to 7x7\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "        padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "    \n",
    "# L3 image input shape -> (?, 7, 7, 64)\n",
    "# Filter2 shape design -> (3x3 x 64(channel) x 128EA filters)\n",
    "# Convolution shape    -> (?, 7, 7, 128) because padding set to 'SAME'\n",
    "# Poolig shape         -> (?, 4, 4, 128) because pooling kernel size set to 4x4\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "        padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 4 * 4 * 128]) # for fully connected layer!!\n",
    "\n",
    "# FC 1st 4x4x128 inputs to 625 outputs\n",
    "W4 = tf.get_variable(\"W4\", shape=[4 * 4 * 128, 625],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "# FC 2nd 625 inputs to 10(nb_classes)   \n",
    "W5 = tf.get_variable(\"W5\", shape=[625, nb_classes],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# Define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "print(\"Learning started. It takes sometime...\")\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}]\\tCost: {:>.9}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"\\nAccuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "    Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nTest one label and prediction...\")\n",
    "print(\"Label:     \\t\", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction:\\t\", sess.run(tf.argmax(logits, 1), feed_dict={\n",
    "    X: mnist.test.images[r:r + 1], keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Lab4: CNN deep for MNIST class version (Accuracy: 0.9931)\n",
    "- Lab2와 동일하나 python class 로 refactorying 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime...\n",
      "[Epoch:    1]\tCost: 0.416150304\n",
      "[Epoch:    2]\tCost: 0.0926853217\n",
      "[Epoch:    3]\tCost: 0.0682261674\n",
      "[Epoch:    4]\tCost: 0.0537698725\n",
      "[Epoch:    5]\tCost: 0.0506835632\n",
      "[Epoch:    6]\tCost: 0.0441732411\n",
      "[Epoch:    7]\tCost: 0.040319178\n",
      "[Epoch:    8]\tCost: 0.0370661633\n",
      "[Epoch:    9]\tCost: 0.0338365095\n",
      "[Epoch:   10]\tCost: 0.0351766979\n",
      "[Epoch:   11]\tCost: 0.0298522976\n",
      "[Epoch:   12]\tCost: 0.027705881\n",
      "[Epoch:   13]\tCost: 0.02742508\n",
      "[Epoch:   14]\tCost: 0.0257915536\n",
      "[Epoch:   15]\tCost: 0.0252825001\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9937\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [1]\n",
      "Prediction:\t [1]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab11-4 : minist by deep learning convolution nerual network class version\n",
    "#           accuracy : 0.9931\n",
    "################################################################################\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # we can use large learning rate using Batch Normalization\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10 # 0 ~ 9 digits recognition = 10 classed\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate 0.7~0.5 on training,\n",
    "            # build should be 1 for testing\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # Input placeholders \n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, 784]) # imgage = 28x28 = 784 pixel\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1]) # all images x 28x28 x 1(256 gray)\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "            # L1 image input shape -> (?, 28, 28, 1)\n",
    "            # Filter1 shape design -> (3x3 x 1(256gray) x 32EA filters)\n",
    "            # Convolution shape    -> (?, 28, 28, 32) because padding set to 'SAME'\n",
    "            # Poolig shape         -> (?, 14, 14, 32) because pooling kernel size set to 2x2\n",
    "            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            L1 = tf.nn.max_pool(L1, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                    padding='SAME')\n",
    "            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n",
    "\n",
    "            # L2 image input shape -> (?, 14, 14, 32)\n",
    "            # Filter2 shape design -> (3x3 x 32(channel) x 64EA filters)\n",
    "            # Convolution shape    -> (?, 14, 14, 64) because padding set to 'SAME'\n",
    "            # Poolig shape         -> (?, 7, 7, 64) because pooling kernel size set to 7x7\n",
    "            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L2 = tf.nn.relu(L2)\n",
    "            L2 = tf.nn.max_pool(L2, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                    padding='SAME')\n",
    "            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n",
    "\n",
    "    \n",
    "            # L3 image input shape -> (?, 7, 7, 64)\n",
    "            # Filter2 shape design -> (3x3 x 64(channel) x 128EA filters)\n",
    "            # Convolution shape    -> (?, 7, 7, 128) because padding set to 'SAME'\n",
    "            # Poolig shape         -> (?, 4, 4, 128) because pooling kernel size set to 4x4\n",
    "            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L3 = tf.nn.relu(L3)\n",
    "            L3 = tf.nn.max_pool(L3, ksize=[1 ,2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                    padding='SAME')\n",
    "            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n",
    "            L3_flat = tf.reshape(L3, [-1, 4 * 4 * 128]) # for fully connected layer!!\n",
    "\n",
    "            # FC 1st 4x4x128 inputs to 625 outputs\n",
    "            W4 = tf.get_variable(\"W4\", shape=[4 * 4 * 128, 625],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4 = tf.Variable(tf.random_normal([625]))\n",
    "            L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n",
    "\n",
    "            # FC 2nd 625 inputs to 10(nb_classes)   \n",
    "            W5 = tf.get_variable(\"W5\", shape=[625, nb_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "            self.logits = tf.matmul(L4, W5) + b5\n",
    "\n",
    "        # Define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                        logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "                            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        # Accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(\n",
    "                                self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, keep_prop=1.0):\n",
    "        return self.sess.run(self.logits, feed_dict={self.X: x_test,\n",
    "                self.keep_prob: keep_prop})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, keep_prop=1.0):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test,\n",
    "                self.Y: y_test, self.keep_prob: keep_prop})\n",
    "\n",
    "    def train(self, x_data, y_data, keep_prop=0.7):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "                self.X: x_data, self.Y: y_data, self.keep_prob: keep_prop})\n",
    "\n",
    "    def prediction_self_test(self, keep_prop=1.0):\n",
    "        # Get one and predict\n",
    "        r = random.randint(0, mnist.test.num_examples - 1)\n",
    "        print(\"\\nTest one label and prediction...\")\n",
    "        print(\"Label:     \\t\",\n",
    "            self.sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "        print(\"Prediction:\\t\",\n",
    "            self.sess.run(tf.argmax(self.logits, 1),\n",
    "            feed_dict={self.X: mnist.test.images[r:r + 1],\n",
    "               self.keep_prob: keep_prop}))\n",
    "    \n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "m1 = Model(sess, \"m1\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "print(\"Learning started. It takes sometime...\")\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = m1.train(batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}]\\tCost: {:>.9}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "print(\"\\nAccuracy:\", m1.get_accuracy(mnist.test.images, mnist.test.labels))\n",
    "\n",
    "# Get one and predict\n",
    "m1.prediction_self_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Lab5: CNN deep for MNIST tf layer version (Accuracy: 0.9939)\n",
    "- Lab3와 동일하나 tensorflow high level API를 적용하여 가독성을 높였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime...\n",
      "[Epoch:    1]\tCost: 0.289628289\n",
      "[Epoch:    2]\tCost: 0.085612601\n",
      "[Epoch:    3]\tCost: 0.066528947\n",
      "[Epoch:    4]\tCost: 0.0535344537\n",
      "[Epoch:    5]\tCost: 0.0495403229\n",
      "[Epoch:    6]\tCost: 0.0437016882\n",
      "[Epoch:    7]\tCost: 0.0395926459\n",
      "[Epoch:    8]\tCost: 0.038719447\n",
      "[Epoch:    9]\tCost: 0.0330028621\n",
      "[Epoch:   10]\tCost: 0.034108294\n",
      "[Epoch:   11]\tCost: 0.0311745752\n",
      "[Epoch:   12]\tCost: 0.0303084652\n",
      "[Epoch:   13]\tCost: 0.0281676022\n",
      "[Epoch:   14]\tCost: 0.027779782\n",
      "[Epoch:   15]\tCost: 0.0262370907\n",
      "Learning finished!!\n",
      "\n",
      "Accuracy: 0.9938\n",
      "\n",
      "Test one label and prediction...\n",
      "Label:     \t [0]\n",
      "Prediction:\t [0]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab11-5 : minist by deep learning convolution nerual network with tf.layer\n",
    "#           accuracy : 0.9939\n",
    "################################################################################\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # we can use large learning rate using Batch Normalization\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "nb_classes = 10 # 0 ~ 9 digits recognition = 10 classed\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate 0.7~0.5 on training,\n",
    "            # build should be 1 for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # Input placeholders \n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, 784]) # imgage = 28x28 = 784 pixel\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1]) # all images x 28x28 x 1(256 gray)\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "            # Convolutional layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, \n",
    "                    kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                    padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1, rate=0.3, \n",
    "                    training=self.training)\n",
    "\n",
    "            # Convolutional layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, \n",
    "                    kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling layer #2\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                    padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2, rate=0.3, \n",
    "                    training=self.training)\n",
    "\n",
    "            # Convolutional layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128,\n",
    "                    kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling layer #3\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                    padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3, rate=0.3, \n",
    "                    training=self.training)\n",
    "\n",
    "            # Dense layer with RELU\n",
    "            flat = tf.reshape(dropout3, [-1, 4 * 4* 128])\n",
    "            dense4 = tf.layers.dense(inputs=flat, units=625,\n",
    "                    activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(dense4, rate=0.5,\n",
    "                    training=self.training)\n",
    "\n",
    "            # Logits (no activation) layer : L5 final FC 625 inputs to 10(nb_classes)\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=nb_classes)\n",
    "\n",
    "        # Define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                        logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "                        learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        # Accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(\n",
    "                                self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits, feed_dict={self.X: x_test,\n",
    "                self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test,\n",
    "                self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "                self.X: x_data, self.Y: y_data, self.training: training})\n",
    "\n",
    "    def prediction_self_test(self, training=False):\n",
    "        # Get one and predict\n",
    "        r = random.randint(0, mnist.test.num_examples - 1)\n",
    "        print(\"\\nTest one label and prediction...\")\n",
    "        print(\"Label:     \\t\",\n",
    "            self.sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "        print(\"Prediction:\\t\",\n",
    "            self.sess.run(tf.argmax(self.logits, 1),\n",
    "            feed_dict={self.X: mnist.test.images[r:r + 1],\n",
    "               self.training: training}))\n",
    "    \n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "m1 = Model(sess, \"m1\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "print(\"Learning started. It takes sometime...\")\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = m1.train(batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}]\\tCost: {:>.9}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "print(\"\\nAccuracy:\", m1.get_accuracy(mnist.test.images, mnist.test.labels))\n",
    "\n",
    "# Get one and predict\n",
    "m1.prediction_self_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Lab6: CNN ensemble for MNIST (Accuracy: 0.9946)\n",
    "- Lab4와 동일하나 ensemble을 적용하였다.\n",
    "- 독립된 3개의 모델을 학습시킨 후 각 prediction을 모두 더한후 가장 높은 값으로 분류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime...\n",
      "[Epoch: 0001 \tCost: [0.28962829 0.28869031 0.28473182]\n",
      "[Epoch: 0002 \tCost: [0.0856126  0.09220979 0.08904249]\n",
      "[Epoch: 0003 \tCost: [0.06652895 0.06821074 0.06888896]\n",
      "[Epoch: 0004 \tCost: [0.05353445 0.0574173  0.05881537]\n",
      "[Epoch: 0005 \tCost: [0.04954032 0.04863594 0.0490442 ]\n",
      "[Epoch: 0006 \tCost: [0.04370169 0.04839998 0.04546389]\n",
      "[Epoch: 0007 \tCost: [0.03959265 0.04065218 0.04149157]\n",
      "[Epoch: 0008 \tCost: [0.03871945 0.03738105 0.03928586]\n",
      "[Epoch: 0009 \tCost: [0.03300286 0.03560205 0.03581892]\n",
      "[Epoch: 0010 \tCost: [0.03410829 0.03188517 0.03399692]\n",
      "[Epoch: 0011 \tCost: [0.03117458 0.03069735 0.03093477]\n",
      "[Epoch: 0012 \tCost: [0.03030847 0.02916503 0.03105707]\n",
      "[Epoch: 0013 \tCost: [0.0281676  0.02935034 0.03023285]\n",
      "[Epoch: 0014 \tCost: [0.02777978 0.02772595 0.02549779]\n",
      "[Epoch: 0015 \tCost: [0.02623709 0.02799495 0.02674515]\n",
      "[Epoch: 0016 \tCost: [0.02503783 0.02706008 0.02598841]\n",
      "[Epoch: 0017 \tCost: [0.02491645 0.02416915 0.02492279]\n",
      "[Epoch: 0018 \tCost: [0.02316422 0.02633178 0.02283863]\n",
      "[Epoch: 0019 \tCost: [0.02194312 0.02304286 0.02242828]\n",
      "[Epoch: 0020 \tCost: [0.02139535 0.02258107 0.02278083]\n",
      "Learning finished!!\n",
      "\n",
      "Model index: (0, <__main__.Model object at 0x7f7be5f2aa90>) \tAccuracy: 0.9936\n",
      "\n",
      "Model index: (1, <__main__.Model object at 0x7f7bda464f28>) \tAccuracy: 0.9936\n",
      "\n",
      "Model index: (2, <__main__.Model object at 0x7f7bda0e5f60>) \tAccuracy: 0.9936\n",
      "Ensemble accuracy: 0.9936\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# lab11-6 : minist by convolution nerual network with ensemble layers\n",
    "#           accuracy : 0.9946\n",
    "################################################################################\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for reproducibility\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "# Import MNIST data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../mnist_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # we can use large learning rate using Batch Normalization\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "nb_classes = 10 # 0 ~ 9 digits recognition = 10 classed\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate 0.7~0.5 on training,\n",
    "            # build should be 1 for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # Input placeholders \n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, 784]) # imgage = 28x28 = 784 pixel\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1]) # all images x 28x28 x 1(256 gray)\n",
    "            self.Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "            # Convolutional layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, \n",
    "                    kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                    padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1, rate=0.3, \n",
    "                    training=self.training)\n",
    "\n",
    "            # Convolutional layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, \n",
    "                    kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling layer #2\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                    padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2, rate=0.3, \n",
    "                    training=self.training)\n",
    "\n",
    "            # Convolutional layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128,\n",
    "                    kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling layer #3\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                    padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3, rate=0.3, \n",
    "                    training=self.training)\n",
    "\n",
    "            # Dense layer with RELU\n",
    "            flat = tf.reshape(dropout3, [-1, 4 * 4* 128])\n",
    "            dense4 = tf.layers.dense(inputs=flat, units=625,\n",
    "                    activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(dense4, rate=0.5,\n",
    "                    training=self.training)\n",
    "\n",
    "            # Logits (no activation) layer : L5 final FC 625 inputs to 10(nb_classes)\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=nb_classes)\n",
    "\n",
    "        # Define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                        logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "                        learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        # Accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(\n",
    "                                self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits, feed_dict={self.X: x_test,\n",
    "                self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test,\n",
    "                self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "                self.X: x_data, self.Y: y_data, self.training: training})\n",
    "\n",
    "    def prediction_self_test(self, training=False):\n",
    "        # Get one and predict\n",
    "        r = random.randint(0, mnist.test.num_examples - 1)\n",
    "        print(\"\\nTest one label and prediction...\")\n",
    "        print(\"Label:     \\t\",\n",
    "            self.sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "        print(\"Prediction:\\t\",\n",
    "            self.sess.run(tf.argmax(self.logits, 1),\n",
    "            feed_dict={self.X: mnist.test.images[r:r + 1],\n",
    "               self.training: training}))\n",
    "    \n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "\n",
    "models = []\n",
    "num_models = 3\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train model\n",
    "print(\"Learning started. It takes sometime...\")\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "\n",
    "    print(\"[Epoch:\", \"%04d\" % (epoch + 1), \"\\tCost:\", avg_cost_list)\n",
    "\n",
    "print(\"Learning finished!!\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros([test_size, 10])\n",
    "for m_idx in enumerate(models):\n",
    "    print(\"\\nModel index:\", m_idx, \"\\tAccuracy:\", m.get_accuracy(\n",
    "                mnist.test.images, mnist.test.labels))\n",
    "    p = m.predict(mnist.test.images)\n",
    "    predictions += p\n",
    "\n",
    "ensemble_correct_prediction = tf.equal(\n",
    "        tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(\n",
    "        tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print(\"Ensemble accuracy:\", sess.run(ensemble_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
